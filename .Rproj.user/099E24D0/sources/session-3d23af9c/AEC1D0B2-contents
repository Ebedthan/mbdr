# Regression linéaire

Au chapitre précédent, nous avons utilisé divers outils graphiques (diagrammes en pointillé de Cleveland, diagrammes en boîte, histogrammes) pour explorer la forme de nos données (normalité), rechercher la présence de valeurs aberrantes et évaluer la nécessité de transformer les données.

Nous avons également abordé des méthodes plus complexes (coplot, scatterplot, pairplots, boxplots, histogrammes) qui ont permis de voir les relations entre une seule variable de réponse et plus d'une variable explicative. Il s'agit de la première étape essentielle de toute analyse qui permet au chercheur de se faire une idée des données avant de passer à des outils statistiques formels tels que la régression linéaire.

Tous les ensembles de données ne se prêtent pas à la régression linéaire. Pour les données de comptage ou les données de présences-absences, des méthodes générales de régression linéaire peuvent être utilisées.

Pour les données de comptage ou de présences-absences, la modélisation linéaire généralisée (GLM) est plus appropriée. Et lorsque les modèles paramétriques utilisés par la régression linéaire et la GLM donnent de mauvais résultats, techniques non paramétriques telles que la modélisation additive et la modélisation additive généralisée (GAM) sont susceptibles de donner de meilleurs résultats. Dans cet ouvrage, nous examinons une série d'outils adaptés à l'analyse des données univariées que l'on trouve couramment, y compris la régression linéaire, la régression linéaire partielle, le GLM, la modélisation additive, le GAM.

Les techniques telles que le GLM et le GAM sont plus difficiles à comprendre et à mettre en œuvre. C'est pourquoi nous commencerons donc par résumer brièvement les principes sous-jacents de la modélisation linéaire.

## Regression linéaire bivariée

Le jeu de données `RIKZ` présente l'abondance d'environ 75 espèces d'invertébrés provenant de 45 sites et qui a été mesurée sur différentes plages le long de la côte néerlandaise. Dans cette étude, la variable «NAP» mesure la hauteur du site d'échantillonnage par rapport au niveau moyen de la mer et indique le temps pendant lequel un site est sous l'eau. Un site ayant une faible valeur NAP passera plus de temps sous l'eau qu'un site ayant une valeur NAP élevée, et les sites ayant des valeurs NAP élevées se trouvent normalement plus haut sur la plage. Les marées créent un environnement difficile pour les animaux qui y vivent, et il est raisonnable de supposer que des espèces et des abondances d'espèces différentes seront trouvées sur des plages ayant des valeurs de PNA différentes.

Un point de départ simple consiste donc à comparer la diversité des espèces (richesse des espèces) avec les valeurs de NAP dans différentes zones de la plage. Bien que les connaissances écologiques suggèrent que la relation entre la richesse en espèces et les valeurs de NAP n'est probablement pas linéaire, nous commençons par un modèle de régression linéaire bivarié utilisant une seule variable explicative. Il est toujours préférable de commencer par un modèle simple et de ne passer à des modèles plus avancés que lorsque cette approche s'avère inadéquate.

Le modèle de régression linéaire bivarié (c'est-à-dire à deux variables) est donné par la formule suivante:

$$ Y_i = \alpha + X_i\beta + \epsilon_i$$

où α est l'ordonnée à l'origine, β est la pente et ε est le résidu, ou l'information qui n'est pas expliquée par le modèle. Ce modèle est basé sur l'ensemble de la population, mais comme expliqué ci-dessus, nous ne disposons que d'un échantillon de la population, et nous devons d'une manière ou d'une autre utiliser les données de cet échantillon pour estimer les valeurs de α et de β pour l'ensemble de la population. Pour ce faire, nous devons faire quatre hypothèses sur nos données qui permettront à une procédure mathématique de produire des valeurs estimées pour α et β. Ces estimateurs, appelés α et β, basés sur les données de l'échantillon agissent alors comme des estimateurs pour leurs paramètres de population équivalents, α et β respectivement. Les quatre hypothèses qui permettent d'utiliser les données de l'échantillon pour estimer les données de la population sont (i) la normalité, (ii) l'homogénéité, (iii) l'indépendance et (iv) la fixité de X.

### Normalité

L'hypothèse de normalité signifie que si nous répétons l'échantillonnage plusieurs fois dans les mêmes conditions environnementales, les observations seront normalement distribuées pour chaque valeur de X. Plusieurs auteurs affirment que la violation de la normalité n'est pas un problème grave en raison de la théorie de la limite centrale . Certains auteurs affirment même que l'hypothèse de normalité n'est pas du tout nécessaire si la taille de l'échantillon est suffisamment grande (Fitzmaurice et al., 2004).
La normalité à chaque valeur $X$ doit être vérifiée en réalisant un histogramme de toutes les observations à cette valeur $X$ particulière. Très souvent, nous ne disposons pas de plusieurs observations (sous-échantillons) pour chaque valeur $X$. Dans ce cas, le mieux que nous puissions faire est de regrouper tous les résidus et de réaliser un histogramme des résidus regroupés ; la normalité des résidus regroupés est rassurante, mais elle n'implique pas la normalité des données de la population.

Nous expliquons également comment ne pas vérifier la normalité, car le concept sous-jacent de normalité est très mal compris par de nombreux chercheurs. Le modèle de régression linéaire exige la normalité des données, et donc des résidus pour _chaque_ valeur $X$. Les résidus représentent l'information qui reste après avoir éliminé l'effet des variables explicatives. Les résidus représentent l'information qui reste après avoir éliminé l'effet des variables explicatives. Cependant, les données brutes $Y$ ($Y$ représente la variable réponse) contiennent les effets des variables explicatives. Pour évaluer la normalité des données $Y$, il est donc trompeur de fonder son jugement uniquement sur un histogramme de toutes les données $Y$. La situation est différente si vous disposez d'un grand nombre de réplicats pour chaque valeur $X$. En résumé, à moins que vous ne disposiez d'observations répétées pour chaque valeur $X$, vous ne devez pas fonder votre jugement de normalité sur un histogramme des données brutes. Appliquez plutôt un modèle et inspectez les résidus.
