# Regression linéaire

## Introduction
Au chapitre précédent, nous avons utilisé divers outils graphiques (diagrammes en pointillé de Cleveland, diagrammes en boîte, histogrammes) pour explorer la forme de nos données (normalité), rechercher la présence de valeurs aberrantes et évaluer la nécessité de transformer les données.

Nous avons également abordé des méthodes plus complexes (coplot, scatterplot, pairplots, boxplots, histogrammes) qui ont permis de voir les relations entre une seule variable de réponse et plus d'une variable explicative. Il s'agit de la première étape essentielle de toute analyse qui permet de se faire une idée sur les données avant de passer à des outils statistiques formels tels que la régression linéaire.

Tous les ensembles de données ne se prêtent pas à la régression linéaire. Pour les données de comptage ou de présences-absences, la modélisation linéaire généralisée (GLM) est plus appropriée. Et lorsque les modèles paramétriques utilisés par la régression linéaire et la GLM donnent de mauvais résultats, les techniques non paramétriques telles que la modélisation additive et la modélisation additive généralisée (GAM) sont susceptibles de donner de meilleurs résultats.

Les techniques telles que le GLM et le GAM sont plus difficiles à comprendre et à mettre en œuvre. C'est pourquoi nous commencerons donc par résumer brièvement les principes sous-jacents de la modélisation linéaire.

Avant de taper avec enthousiasme le code R pour la régression linéaire et de l'exécuter, nous devrions d'abord réfléchir à ce que nous voulons faire. Le but de l'analyse est de trouver une relation entre les densités d'oiseaux (`abundance`) et les six variables explicatives. Mais il se pourrait bien que les oiseaux perçoivent l'effet `area` différemment selon que les valeurs de `graze` sont faibles ou élevées. Si c'est le cas, nous devons inclure un terme d'interaction entre `graze` et `larea`. Le problème est qu'il existe un grand nombre d'interactions bidirectionnelles potentielles. Les oiseaux peuvent réagir différemment à area si les valeurs de `graze` sont faibles en combinaison avec de faibles valeurs d'altitude (`altitude`). Plus l'ensemble de données est petit (56 observations, c'est petit), plus il est difficile d'inclure de multiples interactions.

Il peut arriver que vous ne disposiez pas d'un nombre suffisant d'observations par combinaison. Dans ce cas, les observations individuelles peuvent devenir particulièrement influentes. De nombreux groupes de discussion statistiques ont de longs fils de discussion sur le sujet de l'interaction, voici les points que je vous recommande:

1. Commencer par un modèle sans interactions. Appliquer le modèle, la sélection du modèle et la validation du modèle. Si la validation montre qu'il existe des motifs dans les résidus, cherchez-en la raison. L'ajout d'interactions peut être une option pour améliorer le modèle. 
2. Utiliser les connaissances biologiques pour décider quelles sont les interactions qu'il est judicieux d'ajouter, le cas échéant. 
3. Appliquer une bonne exploration des données pour voir quelles interactions peuvent être importantes. 

## Modélisation

Nous débutons l'analyse avec un modèle sans interactions.


```{r}
loyn <- read.table("Loyn.txt", header=T)
loyn$larea <- log(loyn$area)
loyn$ldist <- log(loyn$distance)
loyn$lldist <- log(loyn$ldistance)
loyn$fgraze <- factor(loyn$graze)
model1 <- lm(abundance ~ larea + ldist + lldist + year + altitude + fgraze, data = loyn)
```

La question qui se pose maintenant est la suivante : Devons-nous examiner d'abord la sortie numérique ou la sortie graphique ? Il est inutile d'appliquer une validation détaillée du modèle si rien n'est significatif. D'un autre côté, pourquoi examiner les résultats numériques si toutes les hypothèses ne sont pas respectées ? Il est peut-être préférable de commencer par les résultats numériques, car cela prend moins de temps et c'est plus facile. Il existe plusieurs façons d'obtenir le résultat numérique de notre modèle de régression linéaire.

### Visualisation des résultats

La commande `summary` donne le résultat suivant:

```{r}
summary(model1)
```

La première partie de la sortie indique le modèle appliqué et quelques informations de base sur les résidus. 

La partie située sous « Coefficients » donne les paramètres de régression estimés, les erreurs standard, les $t$-values et les $p$-values. La seule partie déroutante de ce résultat est peut-être l'absence de graze niveau 1. Il est utilisé comme base de référence. Ainsi, une parcelle ayant le niveau 2 de graze a 0,52 oiseaux (densité) de plus qu'une parcelle ayant le niveau 1, et une parcelle ayant le niveau 5 de graze a 12,4 oiseaux de moins qu'une parcelle ayant le niveau 1. Les $p$-values correspondantes indiquent si une parcelle est significativement différente du niveau 1. Notez que vous ne devez pas évaluer l'importance d'un facteur en fonction des $p$-values individuelles. Nous proposerons une meilleure méthode à cet effet dans un instant. Vous ne devez pas laisser de côté les niveaux individuels d'une variable nominale. Ils sont tous pris en compte ou vous laissez tomber la variable entière. La dernière partie du code donne le $R^2$ et le $R^2$ ajusté (pour la sélection du modèle). Le reste de la sortie devrait, je l'espère, vous être familier.

La fonction `drop1` fais exactement ce que son nom indique, elle elimine une variable, chacune à son tour et réevalue le modèle:

```{r}
drop1(model1, test="F")
```

Le modèle complet a une somme des carrés de 1714,43. Chaque fois, $un$ terme est supprimé à tour de rôle et la somme des carrés résiduelle est calculée à chaque fois. Celles-ci sont ensuite utilisées pour calculer une statistique $F$ et une $p$-value correspondante. Par exemple, pour obtenir le résultat de la première ligne, R ajuste deux modèles. Le premier modèle contient toutes les variables explicatives et le second modèle toutes, sauf $larea$. Il utilise ensuite les sommes des carrés résiduels de chaque modèle dans la statistique $F$ suivante:
$$ F=\frac{(SCR_1 - SCR_2)/(p-q)}{SCR_2/(n-p)} $$

Les termes $SCR_1$ et $SCR_2$ sont les sommes des carrés résiduels du modèle `model1` et du modèle $model2$ , respectivement, et $n$ est le nombre d'observations. Le nombre de paramètres dans les modèles 2 et 1 est respectivement $p$ et $q (p > q)$. 
Les modèles sont imbriqués dans le sens où un modèle est obtenu à partir d'un autre en fixant certains paramètres à 0. L'hypothèse nulle sous-jacente à cette statistique est que les paramètres omis sont égaux à 0 : $H_0: \beta = 0$.
Plus la valeur de la statistique $F$ est élevée, plus il y a de preuves pour rejeter cette hypothèse. En fait, la statistique $F$ suit une distribution $F$, en supposant l'homogénéité, la normalité, l'indépendance et l'absence de modèles résiduels. Dans ce cas, nous pouvons rejeter l'hypothèse nulle.

Dans la régression linéaire, les $p$-values de la fonction `drop1` sont les mêmes que celles obtenues par la statistique $t$ de la commande `summary`, mais pour les GLM non gaussiens, ce n'est pas nécessairement le cas. L'hypothèse nulle sous-jacente à la statistique $F$ est que le paramètre de régression du terme qui a été abandonné est égal à 0. Fondamentalement, nous comparons un modèle complet et (à plusieurs reprises) un modèle imbriqué.

Si le modèle comporte plusieurs variables nominales, la fonction `drop1` donne une $p$-value pour chaque variable, ce qui est pratique.

La commande `anova` donne le résultat suivant.

```{r}
anova(model1)
```