# Regression linéaire

## Introduction
Au chapitre précédent, nous avons utilisé divers outils graphiques (diagrammes en pointillé de Cleveland, diagrammes en boîte, histogrammes) pour explorer la forme de nos données (normalité), rechercher la présence de valeurs aberrantes et évaluer la nécessité de transformer les données.

Nous avons également abordé des méthodes plus complexes (coplot, scatterplot, pairplots, boxplots, histogrammes) qui ont permis de voir les relations entre une seule variable de réponse et plus d'une variable explicative. Il s'agit de la première étape essentielle de toute analyse qui permet de se faire une idée sur les données avant de passer à des outils statistiques formels tels que la régression linéaire.

Tous les ensembles de données ne se prêtent pas à la régression linéaire. Pour les données de comptage ou de présences-absences, la modélisation linéaire généralisée (GLM) est plus appropriée. Et lorsque les modèles paramétriques utilisés par la régression linéaire et la GLM donnent de mauvais résultats, les techniques non paramétriques telles que la modélisation additive et la modélisation additive généralisée (GAM) sont susceptibles de donner de meilleurs résultats.

Les techniques telles que le GLM et le GAM sont plus difficiles à comprendre et à mettre en œuvre. C'est pourquoi nous commencerons donc par résumer brièvement les principes sous-jacents de la modélisation linéaire.

Avant de taper avec enthousiasme le code R pour la régression linéaire et de l'exécuter, nous devrions d'abord réfléchir à ce que nous voulons faire. Le but de l'analyse est de trouver une relation entre les densités d'oiseaux (`abundance`) et les six variables explicatives. Mais il se pourrait bien que les oiseaux perçoivent l'effet `area` différemment selon que les valeurs de `graze` sont faibles ou élevées. Si c'est le cas, nous devons inclure un terme d'interaction entre `graze` et `larea`. Le problème est qu'il existe un grand nombre d'interactions bidirectionnelles potentielles. Les oiseaux peuvent réagir différemment à area si les valeurs de `graze` sont faibles en combinaison avec de faibles valeurs d'altitude (`altitude`). Plus l'ensemble de données est petit (56 observations, c'est petit), plus il est difficile d'inclure de multiples interactions.

Il peut arriver que vous ne disposiez pas d'un nombre suffisant d'observations par combinaison. Dans ce cas, les observations individuelles peuvent devenir particulièrement influentes. De nombreux groupes de discussion statistiques ont de longs fils de discussion sur le sujet de l'interaction, voici les points que je vous recommande:

1. Commencer par un modèle sans interactions. Appliquer le modèle, la sélection du modèle et la validation du modèle. Si la validation montre qu'il existe des motifs dans les résidus, cherchez-en la raison. L'ajout d'interactions peut être une option pour améliorer le modèle. 
2. Utiliser les connaissances biologiques pour décider quelles sont les interactions qu'il est judicieux d'ajouter, le cas échéant. 
3. Appliquer une bonne exploration des données pour voir quelles interactions peuvent être importantes. 

## Modélisation

Nous débutons l'analyse avec un modèle sans interactions.


```{r}
loyn <- read.table("Loyn.txt", header=T)
loyn$larea <- log(loyn$area)
loyn$ldist <- log(loyn$distance)
loyn$lldist <- log(loyn$ldistance)
loyn$fgraze <- factor(loyn$graze)
model1 <- lm(abundance ~ larea + ldist + lldist + year + altitude + fgraze, data = loyn)
```

La question qui se pose maintenant est la suivante : Devons-nous examiner d'abord la sortie numérique ou la sortie graphique ? Il est inutile d'appliquer une validation détaillée du modèle si rien n'est significatif. D'un autre côté, pourquoi examiner les résultats numériques si toutes les hypothèses ne sont pas respectées ? Il est peut-être préférable de commencer par les résultats numériques, car cela prend moins de temps et c'est plus facile. Il existe plusieurs façons d'obtenir le résultat numérique de notre modèle de régression linéaire.

### Visualisation des résultats

La commande `summary` donne le résultat suivant:

```{r}
summary(model1)
```

La première partie de la sortie indique le modèle appliqué et quelques informations de base sur les résidus. 

La partie située sous « Coefficients » donne les paramètres de régression estimés, les erreurs standard, les $t$-values et les $p$-values. La seule partie déroutante de ce résultat est peut-être l'absence de graze niveau 1. Il est utilisé comme base de référence. Ainsi, une parcelle ayant le niveau 2 de graze a 0,52 oiseaux (densité) de plus qu'une parcelle ayant le niveau 1, et une parcelle ayant le niveau 5 de graze a 12,4 oiseaux de moins qu'une parcelle ayant le niveau 1. Les $p$-values correspondantes indiquent si une parcelle est significativement différente du niveau 1. Notez que vous ne devez pas évaluer l'importance d'un facteur en fonction des $p$-values individuelles. Nous proposerons une meilleure méthode à cet effet dans un instant. Vous ne devez pas laisser de côté les niveaux individuels d'une variable nominale. Ils sont tous pris en compte ou vous laissez tomber la variable entière. La dernière partie du code donne le $R^2$ et le $R^2$ ajusté (pour la sélection du modèle). Le reste de la sortie devrait, je l'espère, vous être familier.

La fonction `drop1` fais exactement ce que son nom indique, elle elimine une variable, chacune à son tour et réevalue le modèle:

```{r}
drop1(model1, test="F")
```

Le modèle complet a une somme des carrés de 1714,43. Chaque fois, $un$ terme est supprimé à tour de rôle et la somme des carrés résiduelle est calculée à chaque fois. Celles-ci sont ensuite utilisées pour calculer une statistique $F$ et une $p$-value correspondante. Par exemple, pour obtenir le résultat de la première ligne, R ajuste deux modèles. Le premier modèle contient toutes les variables explicatives et le second modèle toutes, sauf $larea$. Il utilise ensuite les sommes des carrés résiduels de chaque modèle dans la statistique $F$ suivante:
$$ F=\frac{(SCR_1 - SCR_2)/(p-q)}{SCR_2/(n-p)} $$

Les termes $SCR_1$ et $SCR_2$ sont les sommes des carrés résiduels du modèle `model1` et du modèle $model2$ , respectivement, et $n$ est le nombre d'observations. Le nombre de paramètres dans les modèles 2 et 1 est respectivement $p$ et $q (p > q)$. 
Les modèles sont imbriqués dans le sens où un modèle est obtenu à partir d'un autre en fixant certains paramètres à 0. L'hypothèse nulle sous-jacente à cette statistique est que les paramètres omis sont égaux à 0 : $H_0: \beta = 0$.
Plus la valeur de la statistique $F$ est élevée, plus il y a de preuves pour rejeter cette hypothèse. En fait, la statistique $F$ suit une distribution $F$, en supposant l'homogénéité, la normalité, l'indépendance et l'absence de modèles résiduels. Dans ce cas, nous pouvons rejeter l'hypothèse nulle.

Dans la régression linéaire, les $p$-values de la fonction `drop1` sont les mêmes que celles obtenues par la statistique $t$ de la commande `summary`, mais pour les GLM non gaussiens, ce n'est pas nécessairement le cas. L'hypothèse nulle sous-jacente à la statistique $F$ est que le paramètre de régression du terme qui a été abandonné est égal à 0. Fondamentalement, nous comparons un modèle complet et (à plusieurs reprises) un modèle imbriqué.

Si le modèle comporte plusieurs variables nominales, la fonction `drop1` donne une $p$-value pour chaque variable, ce qui est pratique.

La commande `anova` donne le résultat suivant.

```{r}
anova(model1)
```

R utilise le carré moyen du modèle complet (37,3) et le carré moyen de chaque ligne dans un test F similaire au précédent. Ainsi, 93,13 est obtenu en divisant 3471,0 par 37,3, et 1,75 est égal à 65,5/37,3. Les carrés moyens sont calculés à partir de la somme des carrés divisée par les degrés de liberté. La somme des carrés sur la première ligne, 3471,0, est la somme des carrés de régression du modèle $abundance_i = \alpha + \beta \times larea + \epsilon_i$. Les 65,5 de la deuxième ligne représentent la diminution de la somme des carrés résiduels si $ldist$ est ajouté à ce modèle (pour le voir, ajustez un modèle avec seulement l'ordonnée à l'origine et $larea$, et un modèle avec l'ordonnée à l'origine, $larea$, et $ldist$ et comparez les deux sommes des carrés résiduels obtenus à partir des commandes anova ; la différence sera de 65,5).
L'avantage de cette approche est que la dernière ligne nous donne une valeur p pour la variable nominale GRAZE (puisqu'il s'agit de la dernière variable ajoutée), et nous en avons besoin pour déterminer si GRAZE est significatif. L'inconvénient de cette méthode de test est que les valeurs p dépendent de l'ordre des variables : Changez l'ordre et vous obtiendrez une conclusion différente. 
Notez que la dernière ligne de la commande anova et la ligne drop1 sont identiques. Cela s'explique par le fait que les mêmes modèles imbriqués sont comparés. 
La fonction `anova` peut également être utilisée pour comparer des modèles imbriqués. Supposons que nous ajustions un modèle linéaire avec toutes les variables explicatives, et un modèle avec toutes les variables explicatives, à l'exception de `graze`. Ces modèles sont imbriqués car le second modèle est un cas particulier du premier, en supposant que les quatre paramètres de régression pour les niveaux de `graze` sont égaux à zéro (voir ci-dessous).

```{r}
model2 <- lm(abundance ~ larea + ldist + lldist + year + altitude, data = loyn)
anova(model1, model2)
```

L'hypothèse nulle sous-jacente à la statistique $F$ est que les quatre paramètres de régression pour $graze$ (niveaux 2-5) sont égaux à 0, ce qui est rejeté au niveau de 5%. Notez que la $p$-value est identique à la $p$-value obtenue pour le même test avec la commande `anova(M1)`. Alors pourquoi faire la comparaison ? L'avantage de la commande `anova(M1, M2)` est que nous pouvons contrôler quels termes sont éliminés. Ceci est particulièrement utile dans le cas de termes d'interaction multiples.

### Sélection de modèle

Toutes les variables explicatives ne sont pas significativement différentes de 0, comme le montrent les $p$-values des statistiques $t$ (commande `summary`) ou les $p$-value de la statistique $F$ (commande `drop1`) présentées ci-dessus. Si l'objectif de l'analyse est de comprendre quelles variables explicatives déterminent l'abondance des oiseaux, nous pourrions alors décider d'abandonner les variables explicatives qui ne sont pas significatives. Il s'agit là encore d'un sujet sur lequel les statisticiens ne sont pas d'accord. Il existe essentiellement trois approches principales :

1. Abandonner les variables explicatives une à une sur la base de procédures de test d'hypothèses. 
2. Abandonner les variables explicatives une à une (et à chaque fois refaire le modèle) et utiliser un critère de sélection de modèle comme l'AIC ou le BIC pour décider du modèle optimal.
3. Spécifier les modèles choisis a priori et comparer ces modèles entre eux.

La première approche consiste à éliminer le terme le moins significatif, soit sur la base des valeurs $t$ et $p4 obtenues par la commande `summary`, soit par la commande `anova` pour comparer les modèles imbriqués s'il y a des variables nominales et/ou des interactions. Dans la deuxième approche, nous utilisons un critère de sélection comme le critère d'information d'Akaike (AIC). Il mesure la qualité de l'ajustement et la complexité du modèle. L'avantage de l'AIC est que R dispose d'outils permettant d'appliquer une sélection automatique vers l'avant ou vers l'arrière sur la base de l'AIC, ce qui facilite la vie ! L'inconvénient est que l'AIC peut être conservateur et qu'il peut être nécessaire de procéder à un réglage fin (en utilisant les résultats des tests d'hypothèse de la première approche) une fois que l'AIC a sélectionné un modèle optimal. Une sélection en arrière est appliquée par la commande `step(model1)`, et sa sortie est donnée comme suit:

```{r}
step(model1)
```

La première partie du code montre que le modèle contenant toutes les variables explicatives a un AIC de 211,6 avant l'élimination de chaque terme. Plus l'AIC est faible, meilleur est le modèle, selon l'AIC. Par conséquent, nous devrions abandonner `ldist`. La procédure se poursuit en éliminant `year`, `lldist` et `altitude`. À ce stade, aucun autre terme n'est supprimé; le modèle avec `larea` et `fgraze` a un AIC de 204,12, et la suppression de n'importe lequel de ces termes donne un AIC plus élevé. Cela signifie que le modèle optimal basé sur l'AIC contient `fgraze` et `larea`. Vous devriez réappliquer ce modèle et voir si les deux termes sont significatifs. Notez que la commande summary et la commande anova sont toutes deux nécessaires pour cela. Les deux termes sont significatifs au niveau de 5%. Vous pouvez également essayer de voir si l'ajout d'une interaction entre `larea` et `fgraze` améliore le modèle. Vous devriez pouvoir obtenir une $p$-value pour ce terme d'interaction.

### Validation de modèle

Une fois que le modèle optimal a été trouvé, il est temps d'appliquer une validation du modèle. Ce processus comprend (au minimum) les étapes suivantes:

- Tracer les résidus (standardisés) par rapport aux valeurs ajustées pour évaluer l'homogénéité.
- Faites un histogramme des résidus pour vérifier la normalité. Vous pouvez également utiliser un graphique QQ.
- Tracez les résidus en fonction de chaque variable explicative utilisée dans le modèle. Si vous observez une tendance, vous ne respectez pas l'hypothèse d'indépendance.
- Représentez les résidus par rapport à chaque variable explicative non utilisée dans le modèle. Si vous observez une tendance, incluez la variable explicative omise et réinitialisez le modèle. Si les modèles de résidus disparaissent, incluez le terme, même s'il n'est pas significatif.
- Évaluer le modèle en fonction des observations influentes. La fonction de distance de Cook est un outil utile.

Notez que la plupart des points sus-mentionnés utilisent des outils graphiques pour évaluer les hypothèses sous-jacentes d'homogénéité, de normalité et d'indépendance. Les statisticiens ont tendance à utiliser des graphiques, mais les non-statisticiens semblent préférer les tests. Il convient d'être prudent avec les tests, car certains tests utilisés pour évaluer l'homogénéité dépendent fortement de la normalité. 

```{r}
model3 <- lm(abundance ~ larea + fgraze, data = loyn)
op <- par(mfrow = c(2,2))
E <- rstandard(model3)
hist(E)
qqnorm(E)
plot(y = E, x = loyn$larea, xlab = "area", ylab="residus")
abline(0,0)
plot(E ~ loyn$fgraze, xlab="graze", ylab="residus")
abline(0,0)
```