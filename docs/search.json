[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Modélisation des données biologiques avec R",
    "section": "",
    "text": "Bienvenue\nCe livre est le manuel compagnon de la formation modélisation de données biologiques organisée par le service innovation et transfert de technologie des fablab agritech à destination des élèves ingénieurs agronomes ou biologiques de l’Institut National Polytechnique Félix Houphouët-Boigny.\nLa formation se déroule typiquement sur cinq jours et réuni autour de 10 étudiants.\nCe manuel est et sera toujours gratuit, sous licence CC BY-NC-ND 3.0."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "Ce que vous allez apprendre\nAu cours de cette formation nous aborderons les concepts suivants:\nCe livre est un manuel de formation à destination des ingénieurs en sciences agronomiques ou biologiques en fin de cycle (3e année).\nIl se veut pratique et facile à aborder tout en n’hésitant pas à revenir sur les concepts mathématiques avancés pour solidifier les fondements mathématiques des participants.\nCe manuel est utilisé lors de la formation organisée par le biais du service innovation et transfert de technologie des fablabs agritech de l’Institut National Polytechnique Félix Houphouët-Boigny de Côte d’Ivoire.\nCette formation n’est pas une formation à l’utilisation de R. Nous n’aborderons donc pas les notions basiques de son utilisation. Nous ferons cependant l’effort d’apporter de l’aide ou des informations pour chaque fonction utilisée. Nous n’aborderont pas aussi les thèmes des tests d’hypothèses, de la conception d’expérience, de la manipulation des données, de la visualisation des données ou de la programmation littérale avec R.\nCes thèmes sont abordés dans d’autres formations que nous organisons.\nNous avons choisi de faire cette formation en utilisant les fonction R basiques et non sous l’aspect de la philosophie tidyverse (pour laquelle nous consacrons une autre formation). Ce choix est fait pour garder l’attention des étudiants dirigées exclusivement sur la compréhension des concepts statistiques présentés et à leur mise en pratique avec R.\nNous avons fait quelques suppositions sur ce que vous savez déjà pour tirer le meilleur parti de cette formation. Vous devez avoir des connaissances générales en calcul et il est utile que vous ayez déjà une certaine expérience de la programmation de base. Si vous n’avez jamais programmé auparavant, Hands on Programming with R pourrait être un outil précieux.\nVous avez besoin de quatre éléments pour exécuter le code de ce livre : R, RStudio, un certains nombre de jeu de données. Les packages sont les unités fondamentales du code R reproductible. Ils comprennent des fonctions réutilisables, de la documentation décrivant comment les utiliser et des exemples de données."
  },
  {
    "objectID": "intro.html#r",
    "href": "intro.html#r",
    "title": "Introduction",
    "section": "R",
    "text": "R\nPour télécharger R, rendez-vous sur CRAN, le réseau complet d’archives R, https://cloud.r-project.org. Une nouvelle version majeure de R est publiée une fois par an, et il y a 2 à 3 versions mineures par an. C’est une bonne idée de faire des mises à jour régulièrement. La mise à jour peut être un peu fastidieuse, en particulier pour les versions majeures qui vous obligent à réinstaller tous vos paquets, mais la remettre à plus tard ne fait qu’empirer les choses. Nous recommandons R 4.2.0 ou une version ultérieure pour cette formation."
  },
  {
    "objectID": "intro.html#rstudio",
    "href": "intro.html#rstudio",
    "title": "Introduction",
    "section": "RStudio",
    "text": "RStudio\nRStudio est un environnement de développement intégré (IDE) pour la programmation R, que vous pouvez télécharger à partir de https://posit.co/download/rstudio-desktop/. RStudio est mis à jour plusieurs fois par an et vous informe automatiquement de la sortie d’une nouvelle version. C’est une bonne idée de faire des mises à jour régulières pour profiter des dernières et meilleures fonctionnalités. Pour ce livre, assurez-vous d’avoir au moins RStudio 2022.02.0.\nLorsque vous démarrez RStudio , vous voyez deux zones clés de l’interface : le volet de console et le volet de sortie. Pour une exécution du code ligne par ligne, il faut taper le code R dans le volet de la console et appuyer sur la touche Entrée pour l’exécuter. Cependant si l’on veut créer un fichier pour y saisir le code il est possible d’utiliser l’éditeur de texte incorporé."
  },
  {
    "objectID": "datasets.html#le-jeu-de-données-penguins",
    "href": "datasets.html#le-jeu-de-données-penguins",
    "title": "1  Présentation des jeux de données",
    "section": "1.1 Le jeu de données penguins",
    "text": "1.1 Le jeu de données penguins\n\n1.1.1 A propos\n\n\n\nFigure 1.1: Palmer penguins hex sticker (Artwork by allison_horst)\n\n\nLe jeu de données Penguins est un jeu de données collectées et mises à disposition par le Dr. Kristen Gorman et la station Palmer, Antarctica LTER, membre du Long Term Ecological Research Network (réseau de recherche écologique à long terme) et mise à disposition de la communauté R au travers du package palmerpenguins.\nLe jeu de données s’appelle penguins, mais fait références en français à des manchots et non à des pingouins. Pour rappel, il y a deux différences fondamentales entre les pingouins et les manchots: leur répartition géographique et leur (in)capacité à voler. Les pingouins vivent dans l’hémisphère nord et ils peuvent voler! Quant aux manchots, ils ne peuvent pas voler et ils vivent dans l’hémisphère sud. Cependant, lors de ce atelier nous allons faire reference a ce jeu de données en utilisant le terme penguins pour garder le nom original du jeu de données.\nLe jeu de données contient des données de 344 manchots. Il y a 3 espèces différentes de manchots dans ce jeu de données Figure 1.2, collectées sur 3 îles de l’archipel de Palmer, en Antarctique.\n\n\n\nFigure 1.2: Les espèces de manchots dans palmerpenguins\n\n\n\n\n1.1.2 Installation et description courte\nLe package est disponible sur le CRAN et peut être installé à partir de la console R en utilisant la commande ci-dessous:\n\ninstall.packages(\"palmerpenguins\")\n\nLe jeu de donnée est composé de 344 observations et de 8 variables:\n\nstr(penguins)\n\ntibble [344 × 8] (S3: tbl_df/tbl/data.frame)\n $ species          : Factor w/ 3 levels \"Adelie\",\"Chinstrap\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ island           : Factor w/ 3 levels \"Biscoe\",\"Dream\",..: 3 3 3 3 3 3 3 3 3 3 ...\n $ bill_length_mm   : num [1:344] 39.1 39.5 40.3 NA 36.7 39.3 38.9 39.2 34.1 42 ...\n $ bill_depth_mm    : num [1:344] 18.7 17.4 18 NA 19.3 20.6 17.8 19.6 18.1 20.2 ...\n $ flipper_length_mm: int [1:344] 181 186 195 NA 193 190 181 195 193 190 ...\n $ body_mass_g      : int [1:344] 3750 3800 3250 NA 3450 3650 3625 4675 3475 4250 ...\n $ sex              : Factor w/ 2 levels \"female\",\"male\": 2 1 1 NA 1 2 1 2 NA NA ...\n $ year             : int [1:344] 2007 2007 2007 2007 2007 2007 2007 2007 2007 2007 ...\n\n\nLes différentes variables sont l’espèce, l’île (lieu de collecte des données), la longueur du culmen (mm), la profondeur du culmen (mm), la longueur de la nageoire (mm), le poids (g), le sexe et l’année de l’étude. Le culmen est appelé bill dans le jeu de donnée. En zoologie, le culmen est l’arête dorsale de la mandibule supérieure des oiseaux Figure 1.3.\n\n\n\nFigure 1.3: Description du culmen des manchots\n\n\nPour rappel, la description complète du format du jeu de donnée est disponible directement dans R en utilisant la commande ?penguins."
  },
  {
    "objectID": "datasets.html#le-jeu-de-donnée-loyn",
    "href": "datasets.html#le-jeu-de-donnée-loyn",
    "title": "1  Présentation des jeux de données",
    "section": "1.2 Le jeu de donnée loyn",
    "text": "1.2 Le jeu de donnée loyn\n\n1.2.1 A propos\nLes densités d’oiseaux forestiers ont été mesurées dans 56 parcelles forestières du sud-est de l’État de Victoria, en Australie. L’objectif de l’étude était de relier les densités d’oiseaux à six variables d’habitat : taille de la parcelle forestière, distance par rapport à la parcelle la plus proche, distance par rapport à la parcelle plus grande la plus proche, altitude moyenne de la parcelle, année d’isolement par défrichement et indice de l’histoire du pâturage des animaux (1 = léger, 5 = intensif).\n\n\n1.2.2 Variables\n\n\n\n\n\n\n\n\nVariable\nDescription\nType\n\n\n\n\nabundance\nDensité d’oiseaux dans une parcelle de forêt\nVariable réponse continue\n\n\narea\nTaille de la parcelle forestière\nVariable explicative continue\n\n\ndistance\nDistance par rapport à l’îlot le plus proche\nVariable explicative continue\n\n\nldistance\nDistance par rapport à l’îlot plus grand le plus proche\nVariable explicative continue\n\n\naltitude\nAltitude moyenne de la parcelle\nVariable explicative continue\n\n\nyear\nAnnée d’isolement\nVariable explicative continue\n\n\ngraze\nIndice d’intensité du pâturage\nVariable nominale explicative avec 5 niveaux: 1 (faible) à 5 (intensif)\n\n\n\n\n\n\n\n\n\n\n1.2.3 Importer les données\n\nloyn &lt;- read.table(\"Loyn.txt\", header=TRUE)"
  },
  {
    "objectID": "organigram.html",
    "href": "organigram.html",
    "title": "2  Organigramme de relation entre les outils d’analyse",
    "section": "",
    "text": "Nous présentons ici un organigramme montrant comment la régression linéaire, la modélisation additive, la modélisation linéaire généralisée (utilisant la distribution de Poisson et la fonction log-link) et la modélisation additive généralisée (utilisant la distribution de Poisson) sont liées les unes aux autres.\n\n\n\nOrganigramme des outils d’analyse. Adapté de (Zuur et al. 2007)\n\n\nDans la régression linéaire, la violation de l’homogénéité signifie que le GLM (modèle linéaire généralisé) avec une distribution de Poisson peut être utilisé. La normalité mais les relations non linéaires (détectées par exemple par un graphique des résidus en fonction de chaque variable explicative) signifient que la modélisation additive peut être appliquée. Des relations non linéaires et une violation de l’hypothèse de normalité signifient qu’un GAM (modèles additif) avec une distribution de Poisson peut être utilisée. Le graphique changera si une autre fonction de liaison ou une autre distribution est utilisée.\n\n\n\n\nZuur, Alain F, Elena N Ieno, Graham M Smith, et al. 2007. Analysing Ecological Data. Vol. 680. Springer."
  },
  {
    "objectID": "exploration.html#outils-graphiques",
    "href": "exploration.html#outils-graphiques",
    "title": "3  Exploration des données",
    "section": "3.1 Outils graphiques",
    "text": "3.1 Outils graphiques\n\n3.1.1 Boîte à moustache\nUn diagramme en boîte, ou boîte à moustaches, permet de visualiser la moyenne et la dispersion d’une variable univariée. Normalement, le point central d’un diagramme en boîte est la médiane, mais il peut également s’agir de la moyenne. Les quartiles 25% et 75% (Q25 et Q75) définissent les charnières (extrémités des boîtes), et la différence entre les charnières est appelée l’écart. Des lignes (ou moustaches) sont tracées à partir de chaque charnière jusqu’à 1,5 fois l’écart ou jusqu’à la valeur la plus extrême de l’écart, la plus petite étant retenue. Tous les points situés en dehors de ces valeurs sont normalement identifiés comme des valeurs aberrantes Figure 3.1.\n\n\n\nFigure 3.1: Boîte à moustache\n\n\n\n\n\n\n\n\nLa syntaxe générale pour créer la boîte à moustache d’une variable en utilisant base R est: boxplot(variable).\nL’aide complète de la fonction boxplot() est obtenue par la commande ?boxplot().\n\n\n\nEtudions la variable body_mass_g en représentant sa boîte à moustache.\n\nboxplot(penguins$body_mass_g, xlab=\"Masse (g)\")\n\n\n\n\nOn remarque qu’aucune valeur aberrante n’apparait de façon visible. De plus on observe que la médiane du poids des manchots se situent autour de 4000 g (on peut facilement confirmer cela en calculant la médiane median(penguins$body_mass_g)).\nOn peut alors être amenée à continuer l’exploration en étudiant le poids, cette fois-ci en étudiant la variable par espèce de manchot.\n\nboxplot(body_mass_g ~ species, data=penguins, ylab=\"Masse (g)\", xlab=NULL)\n\n\n\n\nLes boîte à moustaches representées nous permettent de nous rendre compte que l’espèce Gentoo a sensiblement une masse plus élevée que les deux autres. On pourra confirmer si cette différence est significative avec une analyse de variance. De plus, on observe la présence de quelques valeurs aberrantes chez l’espèce Chinstrap.\n\n\n3.1.2 Diagramme de points de Cleveland\nLes diagrammes de points de Cleveland sont utiles pour identifier les valeurs aberrantes et l’homogénéité.\nL’homogénéité signifie que la variance des données ne change pas le long des gradients. La violation de cette condition est appelée hétérogénéité et l’homogénéité est une hypothèse cruciale pour de nombreuses méthodes statistiques.\nLa valeur est présentée sur l’axe horizontal et l’ordre des points (tel qu’il est organisé par le programme) est présenté sur l’axe vertical.\n\n\n\n\n\n\nLa syntaxe pour générer un diagramme de points de Cleveland avec base R est dotchart(variable). L’aide est disponible avec la commande ?dotchart().\n\n\n\nRepresentons le diagramme de point de Cleveland avec pour but d’identifier une possible violation de l’homogénéité ou la présence de valeurs aberrantes. Le diagrammes à points est réalisé en utilisant différents symboles conditionnels à une variable explicative nominale qui est ici l’espèce.\n\ndotchart(penguins$body_mass_g, main=\"Masse (g)\", pch=as.numeric(penguins$species))\n\n\n\n\nTout point isolé à droite ou à gauche indique des valeurs aberrantes, mais dans cet jeu de données, en considérant l’ensemble du graphique aucun point n’est considéré comme aberrant, ce qui confirme notre observation de la boîte à moustache. Cependant comme observé plus haut, en prenant les points par groupe (chaque symbole représentant une espèce différente) on observe bien un groupe présentant des valeurs aberrantes.\n\n\n3.1.3 Exercices\n\nExplorez la présence de valeurs aberrantes pour les variables bill_depth_mm, bill_length_mm et flipper_length_mm à l’aide de boîte à moustaches et de diagrammes en points de Cleveland.\n\n\n\n3.1.4 Histogramme\nUn histogramme montre le centre et la distribution des données et donne une indication de la normalité. Toutefois, l’application d’une transformation des données pour les faire correspondre à une distribution normale nécessite des précautions.\n\n# Subdivision du panel graphique\nlayout(matrix(c(1, 2, 1, 3), nrow = 2, byrow = TRUE))\n\n# Histrogramme de la masse pour l'ensemble des manchots\nhist(penguins$body_mass_g, main=\"Masse (g)\", xlab=NULL)\n\n# Histrogramme de la masse pour les males\nhist(penguins$body_mass_g[penguins$sex==\"male\"], main=\"Males\", xlab=NULL)\n# Histrogramme de la masse pour les femelles\nhist(penguins$body_mass_g[penguins$sex==\"female\"], main=\"Females\", xlab=NULL)\n\n\n\n\nLa forme de l’histogramme montre une certaine asymétrie et l’on pourrait être tenté d’appliquer une transformation. Cependant, un histogramme conditionnel donne une image assez différente. Dans un histogramme conditionnel, les données sont divisées en fonction d’une variable nominale et les histogrammes des sous-ensembles sont tracés côte-à-côte. A ce moment on obtient une figure tout autre montrant une bimodalité aussi bien chez les males que chez les femelles manchots. On a une différence claire du centre de la distribution et la pic initial de données observé sur le premier histogramme est grandement du aux femelles. Il faudrait donc explorer les effets du sexe sur le poids des manchots ainsi que les interactions avant d’envisager une transformation des données.\n\n\n3.1.5 QQ-plots\nUn graphique Quantile-Quantile (QQ-plots) est un outil graphique utilisé pour déterminer si les données suivent une distribution particulière. Le graphique QQ pour une distribution normale compare la distribution d’une variable donnée à la distribution gaussienne. Si les points obtenus se situent approximativement sur une ligne droite, on considère que la distribution des données est la même que celle d’une variable normalement distribuée.\nLe \\(p\\)-ème quantile \\(q\\) d’une variable aléatoire \\(y\\) est donnée par \\(F(q) = P(y \\leq q) = p\\). Si l’on souhaire savoir quelle valeur de \\(q\\) appartient à \\(p\\), on inverse la formule précédente pour obtenir \\(q = F^{-1}(p)\\). Supposons que nous avons cinq observations \\(Y_i\\) avec les valeurs 1, 2, 3, 4, 5. Par définition, le premier chiffre est le 0% percentile, le milieu est le 50% percentile et 5 est le 100% percentile. La différence entre un quantile et un percentile est un seulement le facteur 100. Les QQ-plots sont soit basés sur les percentiles ou typiquement sur les points quantiles de l’échantillon définis par \\((i-0.5)/n\\) où \\(i\\) varie de 1 à 5 et \\(n=5\\) dans notre example. Ainsi pour notre exemple, les points de quantiles de l’échantillon sont 0,1, 0,3, 0,5, 0,7 et 0,9. Ce sont les valeurs de \\(p\\) pour l’échantillon. Dans la séconde étape, ces quantiles de l’échantillon sont comparés à une distribution normale. Celà signifie que la fonction de densité \\(P(y\\leq q)\\) est désormais une fonction de densité normale et \\(F()\\) est désormais la fonction de répartition de la loi normale.\nLe QQ-plot est donc un graphique des valeurs de \\(Y_i\\) de l’échantillon comparés aux \\(q_i\\). On peut ajouter à ce graphique dans R, une ligne qui connecte les 25e et le 75e quartiles.\nNous appliquons dans le même temps une transformation des données pour visualiser laquelle produit le meilleur ajustement.\nIl est très souvent utilse de combiner les QQ-plots avec des transformations de puissance, qui est donnée par \\[ \\frac{Y^p - 1}{p}, \\forall p \\neq 0; log(Y) , p = 0\\]\nVeuillez noter que le \\(p\\) utilisé ici n’est pas le \\(p\\) utilisé pour décrire les quantiles. Il est aussi utile de comparer plusieurs QQ-plots pour différentes valeurs de \\(p\\).\n\n# Transformation racine carrée\nbmsq &lt;- sqrt(penguins$body_mass_g)\n# Transformation racine quatrième\nbmfq &lt;- penguins$body_mass_g^(0.25)\n# Transformation logarithmique\nbmlog &lt;- log(penguins$body_mass_g)\n\nDans le graphique ci-dessous, aucune transormation semble prendre le dessus sur l’autre.\n\nlayout(matrix(c(1, 2, 3, 4), nrow = 2, byrow = TRUE))\nqqnorm(penguins$body_mass_g, main=\"Aucune transformation (p=0)\")\nqqline(penguins$body_mass_g)\nqqnorm(bmsq, main=\"Racine carré (p=0.5)\")\nqqline(bmsq)\nqqnorm(bmfq, main=\"Racine quatrième (p=0.25)\")\nqqline(bmfq)\nqqnorm(bmlog, main=\"Logarithme (p=1)\")\nqqline(bmlog)\n\n\n\n\n\n\n3.1.6 Exercices\n\nExplorez la normalité des variables bill_depth_mm, bill_length_mm et flipper_length_mm à l’aide d’histogrammes et de diagramme en points de Cleveland.\n\n\n\n3.1.7 Nuage de points\nJusqu’à présent, l’accent a été mis sur la détection des valeurs aberrantes, la vérification de la normalité et l’exploration d’ensembles de données associés à des variables explicatives nominales uniques. Les techniques suivantes s’intéressent aux relations entre plusieurs variables. Un nuage de points est un outil permettant de trouver une relation entre deux variables. Il représente une variable sur l’axe horizontal et une seconde variable sur l’axe vertical. Pour aider à visualiser la relation entre les variables, une ligne droite ou une courbe de lissage est souvent ajoutée au graphique.\nLa figure suivante montre le nuage de points entre les variables flipper_length_mm (taille de la nageoire) et body_mass_g (masse du manchot).\n\nplot(penguins$flipper_length_mm, penguins$body_mass_g, xlab=\"Taille des nageoires\", ylab=\"Masse (g)\")\nm1 &lt;- lm(body_mass_g ~ flipper_length_mm, data=penguins)\nabline(m1)\n\n\n\n\n\n\n3.1.8 Pairplots\nSi vous avez plus de deux variables, vous pouvez produire une série de nuage de points : un pour chaque paire de variables. Cependant, le nombre de diagrammes augmente rapidement si vous avez plus de trois variables à explorer. Une meilleure approche, jusqu’à environ 10 variables explicatives, est le diagramme de paires encore appelée matrice de nuage de points. Ces diagrammes présentent plusieurs nuage de points par paire dans un seul graphique et peuvent être utilisés pour détecter les relations entre les variables et pour détecter la colinéarité.\nLa figure suivante montre un pairplot entre les variables body_mass_g, flipper_length_mm, bill_length_mm et bill_depth_mm. Chaque sous-graphique est un nuage de points entre deux variables, avec les etiquettes de chaque variable ajoutée dans la diagonale.\n\n# penguins[3:6] sélectionne les variables bill_length_mm\n# bill_depth_mm, flipper_length_mm, body_mass_g\npairs(penguins[,3:6])\n\n\n\n\nIl est aussi possible d’ajouter une droite ajustée à chaque graphique.\n\npairs(penguins[,3:6], panel=panel.smooth)\n\n\n\n\nOn parle de colinéarité lorsqu’il existe une forte corrélation entre deux (ou plus) variables.\nLa figure ci-dessous montre un autre pairplot du même jeu de donnée sur lequel nous avons, cette fois-ci ajouté les coefficients de corrélation entre les variables dans le triangle bas du pairplot.\nPour cela, nous utiliserons une fonction panel.cor définie comme suit:\n\npanel.cor &lt;- function(x, y, digits=1, prefix=\"\", cex.cor) {\n  usr &lt;- par(\"usr\")\n  on.exit(par(usr))\n  par(usr=c(0, 1, 0, 1))\n  r1 &lt;- cor(x, y, use=\"pairwise.complete.obs\")\n  r &lt;- abs(cor(x, y, use=\"pairwise.complete.obs\"))\n  \n  txt &lt;- format(c(r1, 0.123456789), digits=digits)[1]\n  txt &lt;- paste(prefix, txt, sep=\"\")\n  if (missing(cex.cor)) {\n    cex &lt;- 0.9/strwidth(txt)\n  }\n  text(0.5, 0.5, txt, cex = cex * r)\n}\n\nPar la suite le diagramme est construit en faisant:\n\npairs(penguins[,3:6], lower.panel=panel.cor)\n\n\n\n\nIl faut noter qu’il existe une forte colinéarité entre la taille de la nageoire et la masse du manchot.\nDes diagrammes en paires doivent être établis pour chaque analyse. Ils doivent comprendre (i) un diagramme par paire de toutes les variables de réponse (en supposant que plus d’une variable de réponse soit disponible) ; (ii) un diagramme par paire de toutes les variables explicatives ; et (iii) un diagramme par paire de toutes les variables de réponse et de toutes les variables explicatives.\nLe premier graphique (i) fournit des informations qui aideront à choisir les techniques multivariées les plus appropriées. On espère que les variables de réponse présenteront de fortes relations linéaires (certaines techniques telles que l’ACP dépendent de relations linéaires). Toutefois, si le graphique (ii) montre une relation linéaire claire entre les variables explicatives, indiquant une colinéarité, nous savons que nous avons un problème majeur à résoudre avant de poursuivre l’analyse.\nLe graphique (iii) permet de déterminer si les relations entre les variables de réponse et les variables explicatives sont linéaires. Si ce n’est pas le cas, plusieurs options s’offrent à nous. La plus simple consiste à appliquer une transformation aux variables de réponse et/ou explicatives afin de linéariser les relations.\nD’autres options seront explorée le long du séminaire.\n\n\n3.1.9 Coplot\nUn coplot est un nuage de points conditionnel montrant la relation entre y et x, pour différentes valeurs d’une troisième variable z, voire d’une quatrième variable w. Les variables conditionnelles peuvent être nominales ou continues.\nLa figure suivante présente un coplot entre la masse des manchots et la taille de leur nageoire conditionée par la variable nominale espèce.\nLes panneaux sont classés de la partie inférieure gauche à la partie supérieure droite. Cet ordre correspond à des valeurs croissantes de la variable explicative du conditionnement.\n\ncoplot(body_mass_g ~ flipper_length_mm | species, data = penguins)\n\n\n\n\n\n Missing rows: 4, 272 \n\n\n\n\n3.1.10 Exercices\n\nExplorez la présence de colinéarité entre les variables body_mass_g vs bill_depth_mm et body_mass_g vs bill_length_mm à l’aide de nauge de points et de coplot.\n\n\n\n3.1.11 Diagrammes de conception et d’interaction\nLes diagrammes de conception et d’interaction sont un autre outil précieux pour explorer les ensembles de données avec des variables nominales et sont particulièrement utiles à utiliser avant d’appliquer la régression, la GLM, la modélisation mixte ou l’analyse de variance.\nIls permettent de visualiser (i) les différences entre les valeurs moyennes de la variable réponse pour différents niveaux de variables nominales et (ii) les interactions entre les variables explicatives.\nLa figure suivante montre un diagramme de conception entre la masse des manchots et les trois variables nominale du jeu de données: species, sex et island. Il nous permet de comparer directement les moyennes (ou médianes) de chaque variable nominale en utilisant un seul graphique.\nLe graphique nous montre que la masse moyenne de l’espèce Gentoo est située autour de 5100g et est sensiblement plus élevée que celle de Chinstrap et Adélie. De même on peut aussi remarquer que la masse moyenne des mâles est plus grande que celle des femelles.\n\nplot.design(body_mass_g ~ species + sex + island, data = penguins)\n\n\n\n\nCependant, les diagrammes de conception ne nous permettent pas d’explorer les interactions entre les variables explicatives, d’où la nécessité d’utiliser les diagrammes d’interactions.\n\n\n\n\n\n\nInterprétation d’un diagramme d’interaction\n\nIdentifier les facteurs et les niveaux: observez l’abscisse pour identifier les niveaux du premier facteur et l’ordonnée pour identifier les niveaux du deuxième facteur.\nÉvaluer le parallélisme des lignes: si les lignes sont parallèles, cela signifie qu’il n’y a pas d’interaction entre les facteurs; l’effet d’un facteur est constant pour tous les niveaux de l’autre facteur. Cependant, si les lignes ne sont pas parallèles (c’est-à-dire qu’elles se croisent ou divergent), cela indique une interaction entre les facteurs. L’effet d’un facteur dépend alors du niveau de l’autre facteur.\nAmpleur de l’interaction: le degré de divergence ou de convergence des lignes peut donner une indication de la force de l’interaction. Une divergence ou une convergence plus importante indique un effet d’interaction plus fort.\nDirection des effets: observez si la variable de réponse augmente ou diminue avec les changements de niveau des facteurs. Cela peut aider à déterminer la nature de l’interaction et des effets principaux.\n\nPour résumer\n\nPas d’interaction: les lignes parallèles suggèrent que les deux facteurs n’interagissent pas. Les effets principaux de chaque facteur sont additifs.\nInteraction présente: les lignes non parallèles indiquent une interaction entre les facteurs, ce qui signifie que l’effet d’un facteur dépend du niveau de l’autre facteur.\nForce et direction: le degré de divergence ou de convergence et la direction des lignes donnent des indications sur la force et la nature de l’interaction.\n\n\n\n\nLe premier diagrame d’interaction ci-dessous montre l’interaction entre l’espèce et le sexe. On observe qu’il n’y a une interaction entre le sexe et la masse des manchots, ce qui signifie que la masse dépend du sexe et de l’espèce de manchot.\n\ninteraction.plot(penguins$species, penguins$sex, penguins$body_mass_g)\n\n\n\n\nLe second diagramme montre l’interaction entre l’île (l’habitat du manchot), le sexe et la masse. On observe qu’il existe aussi une interaction entre ces variables.\n\ninteraction.plot(penguins$sex, penguins$island, penguins$body_mass_g)\n\n\n\n\n\n\n3.1.12 Exercices\n\nQuels types d’interaction il existe entre sex, species et flipper_length_mm ? Explorez de même le type d’interaction entre sex, species et bill_length_mm."
  },
  {
    "objectID": "exploration.html#valeurs-aberrantes-transformations-et-standardisation",
    "href": "exploration.html#valeurs-aberrantes-transformations-et-standardisation",
    "title": "3  Exploration des données",
    "section": "3.2 Valeurs aberrantes, transformations et standardisation",
    "text": "3.2 Valeurs aberrantes, transformations et standardisation\n\n3.2.1 Valeurs aberrantes\nUne valeur aberrante est un point de données qui, en raison de sa valeur extrême par rapport au reste de l’ensemble de données, peut influencer incorrectement une analyse. La première question qui se pose est donc la suivante: “Comment identifier une valeur aberrante?” Une approche simple pourrait consister à quantifier tout ce qui est aberrant au-delà d’une certaine distance par rapport au centre des données. On pourrait par exemple considérer les points situés en dehors des charnières d’un diagramme en boîte comme des valeurs aberrantes. Cependant, si la quantité des données n’est pas suffisante, il pourrait être difficile de considérer ces valeurs comme abberantes.\nLes nuages de points peuvent aussi nous permettre d’identifier les valeurs aberrantes. Ainsi, même si une observation n’est pas considérée comme aberrante dans l’espace x ou dans l’espace y (alors possiblement identifiée par une boîte à moustache), elle peut l’être dans l’espace xy. La situation dans laquelle une observation est une valeur aberrante dans l’espace x, ainsi que dans l’espace y, mais pas dans l’espace xy, est également possible.\nPour résumer la détection des valeurs aberrantes peut rapidemment devenir difficile, cependant l’analyse pourrait permettre d’y voir plus clair.\n\n\n3.2.2 Transformation\n\n3.2.2.1 Quelques transformations usuelles\nIl existe de nombreuses raisons de transformer les données, mais c’est généralement parce que les données présentent des valeurs aberrantes et des distributions non normales. La transformation des données (sur les variables de réponse) sera également nécessaire lorsque vous prévoyez d’utiliser l’analyse discriminante et qu’il existe des preuves évidentes (par exemple, en utilisant un diagramme en pointillés de Cleveland) d’hétérogénéité.\nDe plus, le choix de la transformation est influencée par le choix de l’analyse de suivi. Pour certaines techniques, telles que les arbres de classification ou de régression, la transformation des variables explicatives ne change rien aux résultats. Cependant, la plupart des techniques peuvent nécessiter une certaine transformation des données brutes avant l’analyse.\nLe problème le plus facile à résoudre est celui où les observations extrêmes identifiées au cours de l’étape d’exploration des données s’avèrent être des erreurs de frappe. Nous supposerons toutefois que cette solution facile n’existe pas et que nous disposons d’un ensemble de données contenant de véritables observations extrêmes. Si ces observations extrêmes se trouvent dans les variables explicatives, une transformation des variables explicatives (continues) est certainement nécessaire, en particulier si l’on applique des techniques de régression, d’analyse de la covariance, de GLM, de GAM ou des techniques multivariées telles que l’analyse de la redondance et l’analyse canonique des correspondances.\nLorsque les observations extrêmes se trouvent dans la variable réponse, plusieurs approches sont possibles. Vous pouvez soit transformer les données, soit appliquer une technique légèrement plus performante pour traiter les valeurs extrêmes, comme un GLM ou un GAM avec une distribution de Poisson. Cette dernière méthode ne fonctionne que s’il y a une augmentation de la dispersion des données observées pour des valeurs plus élevées. Il est également possible d’utiliser des modèles de quasi-Poisson si les données sont trop dispersées.\n\n\n\n\n\n\nVous ne devez jamais appliquer une racine carrée ou une transformation logarithmique à la variable réponse, puis continuer avec un modèle GLM de Poisson, car cela applique la correction deux fois.\n\n\n\nUne solution plus radicale pour les observations extrêmes consiste à les omettre purement et simplement de l’analyse. Toutefois, si vous adoptez cette approche, vous devez toujours fournir les résultats de l’analyse avec et sans les observations extrêmes. Si les grandes valeurs proviennent toutes d’une région, d’un mois ou d’un sexe, il est possible d’utiliser différentes composantes de variance dans le modèle de régression linéaire, ce qui permet d’obtenir des moindres carrés généralisés (GLS).\nSupossons par exemple que nous souhaitons faire une regression linéaire. Le diagramme de points de Cleveland ou les boîtes à moustaches nous indiquent qu’il n’y a pas de valeurs aberrantes préoccupantes, mais le nuage de points d’une variable réponse et d’une variable explicative montre une relation non linéaire évidente. Dans ce cas, nous devrions envisager de transformer l’une des variables ou les deux. Mais quelle transformation utiliser ? L’éventail des transformations possibles pour les variables réponse et explicative peut être choisi parmi les suivantes \\[ y^{\\frac{1}{4}}, y^{\\frac{1}{3}}, y^{\\frac{1}{2}}, y, log(y), y^2, y^3, y^4, ..\\]\nCes transformations sont des cas particulier de la transformation de puissance de Box-Cox qui est une famille de transformation qui ne peuvent être appliquée que sur les données non-négatives.\nUne autre alternative est d’utiliser la transformation en rang (méthode utilisée par la plupart des méthodes non paramétrique) ou la transformation en données binaires. Par exemple, en supposant que l’on a la série statistique suivante 2, 7, 4, 9, 22, 40, la transformation en rang va donner 1, 3, 2, 4, 5, 6. Si l’on a la série statistique 0, 1, 3, 0, 4, 0, 100 alors la transformation en série binaire va donner 0, 1, 1, 0, 1, 0, 1.\n\n\n3.2.2.2 Stratégies pour le choix d’une transformation\nIl existe plusieurs stratégies pour le choix de la transformation la plus appropriée. Nous présentons ici deux d’entre elles: essai-erreur et la règle de Mosteller-Tukey.\nLa transformation par essai-erreur est le fait de tester différentes transformation et de visualiser les résultats à l’aide des outils graphiques présentés ci-dessus afin d’effectuer la sélection de la meilleure transformation. En utilisant cette méthode, il es important de reporter les résultats incluant aussi bien les transformations réussies que celles qui ont échouées.\nLorsque l’analyse qui suivra nécessite des relations linéaire entre les variables, la règle de Mosteller-Tukey Figure 3.2 qui appartient à la famille des transformations de Box-Cox.\n\n\n\nFigure 3.2: Mosteller-Tukey bulging rule\n\n\nCette approche est basée sur l’identification des motifs non-linéaires par l’inspection des nuages de points. Les transformations requises peuvent alors être inférée du graphique Figure 3.2.\nPar exemple, si la forme du nuage de points est similaire à la forme du quadrant en bas à gauche de la figure Figure 3.2, alors soit if faudra effectuer une transformation \\(x^2\\) ou \\(x^3\\) de la variable explicative ou effectuer une transformation \\(log(y)\\) ou \\(y^{0.5}\\) de la variable réponse.\n\n\n\n3.2.3 Standardisation\nSi les variables comparées proviennent d’échelles très différentes, comme la comparaison des taux de croissance de petites espèces de poissons avec ceux de grandes espèces de poissons, la standardisation (conversion de toutes les variables à la même échelle) peut être une option. Toutefois, cela dépend de la technique statistique utilisée. Il existe plusieurs méthodes pour convertir les données à la même échelle, et une option consiste à centrer toutes les variables autour de zéro par \\[ Y_i^{new} = Y_i - \\bar{Y} \\] où \\(\\bar{Y}\\) est la moyenne de l’échantillon et \\(Y_i\\) est la valeur de la i-ème observation. Cependant la standardisation la plus utilisée est donnée par \\[ Y_i^{new} = (Y_i - \\bar{Y}) / s_y \\] où \\(s_y\\) est l’écart type de l’échanitllon. Les valeurs obtenues sont alors centrées autour de zéro, avec une variance de 1, et sont sans unités. Cette transformation est aussi appelée normalisation.\nComme pour les autres transformations, la décision de standardiser vos données dépend de la technique statistique que vous envisagez d’utiliser. Par exemple, si vous souhaitez comparer des paramètres de régression, vous pouvez juger utile de normaliser les variables explicatives avant l’analyse, en particulier si elles sont exprimées dans des unités différentes ou si elles ont des intervalles différents. Certaines techniques, telles que l’analyse en composantes principales, normalisent ou centrent automatiquement les variables."
  },
  {
    "objectID": "exploration.html#conclusion",
    "href": "exploration.html#conclusion",
    "title": "3  Exploration des données",
    "section": "3.3 Conclusion",
    "text": "3.3 Conclusion\n\n3.3.1 Même si vous ne les voyez pas, elles peuvent être présentes\nMême si les nuages de points suggèrent l’absence de relation entre Y et X, cela ne signifie pas nécessairement qu’il n’en existe pas. Un nuage de points ne montre que la relation entre deux variables, et l’inclusion d’une troisième, d’une quatrième ou même d’une cinquième variable peut conduire à une conclusion différente.\n\n\n3.3.2 Prochaines étapes\nUne fois l’exploration des données terminée, l’étape suivante consiste à vérifier et à étudier les modèles et les relations que cette étape a permis d’identifier. Si le nuage de points indique une relation linéaire entre les variables, la régression linéaire est l’étape suivante évidente. Toutefois, si le nuage de points suggère un modèle non linéaire clair, une approche différente doit être adoptée, qui peut inclure (i) l’utilisation d’interactions et/ou de termes quadratiques dans le modèle de régression linéaire, (ii) la transformation des données, (iii) la poursuite avec un modèle de régression non linéaire, (iii) l’utilisation d’une modélisation linéaire généralisée, (iv) l’application de techniques de modélisation additive généralisée, ou (v) l’application de techniques de modélisation mixtes (additives). Toutes ces approches sont étudiées dans les chapitres suivants. La première option consiste à utiliser le modèle de régression linéaire, mais il faut s’assurer que toutes les hypothèses sont respectées. Pour choisir l’approche la plus appropriée, il faut connaître les hypothèses des méthodes sélectionnées et disposer d’outils permettant de détecter les violations (à l’aide des résidus). Tous ces éléments sont abordés dans les chapitres suivants, mais tout se résume à quelque chose de très fondamental : apprenez de vos erreurs."
  },
  {
    "objectID": "regression.html#introduction",
    "href": "regression.html#introduction",
    "title": "4  Regression linéaire",
    "section": "4.1 Introduction",
    "text": "4.1 Introduction\nAu chapitre précédent, nous avons utilisé divers outils graphiques (diagrammes en pointillé de Cleveland, diagrammes en boîte, histogrammes) pour explorer la forme de nos données (normalité), rechercher la présence de valeurs aberrantes et évaluer la nécessité de transformer les données.\nNous avons également abordé des méthodes plus complexes (coplot, scatterplot, pairplots, boxplots, histogrammes) qui ont permis de voir les relations entre une seule variable de réponse et plus d’une variable explicative. Il s’agit de la première étape essentielle de toute analyse qui permet de se faire une idée sur les données avant de passer à des outils statistiques formels tels que la régression linéaire.\nTous les ensembles de données ne se prêtent pas à la régression linéaire. Pour les données de comptage ou de présences-absences, la modélisation linéaire généralisée (GLM) est plus appropriée. Et lorsque les modèles paramétriques utilisés par la régression linéaire et la GLM donnent de mauvais résultats, les techniques non paramétriques telles que la modélisation additive et la modélisation additive généralisée (GAM) sont susceptibles de donner de meilleurs résultats.\nLes techniques telles que le GLM et le GAM sont plus difficiles à comprendre et à mettre en œuvre. C’est pourquoi nous commencerons donc par résumer brièvement les principes sous-jacents de la modélisation linéaire.\nAvant de taper avec enthousiasme le code R pour la régression linéaire et de l’exécuter, nous devrions d’abord réfléchir à ce que nous voulons faire. Le but de l’analyse est de trouver une relation entre les densités d’oiseaux (ABUND) et les six variables explicatives. Mais il se pourrait bien que les oiseaux perçoivent l’effet AREA différemment selon que les valeurs de GRAZE sont faibles ou élevées. Si c’est le cas, nous devons inclure un terme d’interaction entre GRAZE et L.AREA. Le problème est qu’il existe un grand nombre d’interactions bidirectionnelles potentielles. Les oiseaux peuvent réagir différemment à AREA si les valeurs de GRAZE sont faibles en combinaison avec de faibles valeurs d’altitude (ALT). Plus l’ensemble de données est petit (56 observations, c’est petit), plus il est difficile d’inclure de multiples interactions.\nIl peut arriver que vous ne disposiez pas d’un nombre suffisant d’observations par combinaison. Dans ce cas, les observations individuelles peuvent devenir particulièrement influentes. De nombreux groupes de discussion statistiques ont de longs fils de discussion sur le sujet de l’interaction, voici les points que je vous recommande:\n\nCommencer par un modèle sans interactions. Appliquer le modèle, la sélection du modèle et la validation du modèle. Si la validation montre qu’il existe des motifs dans les résidus, cherchez-en la raison. L’ajout d’interactions peut être une option pour améliorer le modèle.\nUtiliser les connaissances biologiques pour décider quelles sont les interactions qu’il est judicieux d’ajouter, le cas échéant.\nAppliquer une bonne exploration des données pour voir quelles interactions peuvent être importantes."
  },
  {
    "objectID": "regression.html#modélisation",
    "href": "regression.html#modélisation",
    "title": "4  Regression linéaire",
    "section": "4.2 Modélisation",
    "text": "4.2 Modélisation\nNous débutons l’analyse avec un modèle sans interactions.\n\nloyn &lt;- read.table(\"Loyn.txt\", header=T)\nloyn$larea &lt;- log(loyn$area)\nloyn$ldist &lt;- log(loyn$distance)\nloyn$lldist &lt;- log(loyn$ldistance)\nloyn$fgraze &lt;- factor(loyn$graze)\nmodel1 &lt;- lm(abundance ~ larea + ldist + lldist + year + altitude + fgraze, data = loyn)\n\nLa question qui se pose maintenant est la suivante : Devons-nous examiner d’abord la sortie numérique ou la sortie graphique ? Il est inutile d’appliquer une validation détaillée du modèle si rien n’est significatif. D’un autre côté, pourquoi examiner les résultats numériques si toutes les hypothèses ne sont pas respectées ? Il est peut-être préférable de commencer par les résultats numériques, car cela prend moins de temps et c’est plus facile. Il existe plusieurs façons d’obtenir le résultat numérique de notre modèle de régression linéaire.\n\n4.2.1 Visualisation des résultats\nLa commande summary donne le résultat suivant:\n\nsummary(model1)\n\n\nCall:\nlm(formula = abundance ~ larea + ldist + lldist + year + altitude + \n    fgraze, data = loyn)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.8992  -2.7245  -0.2772   2.7052  11.2811 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  36.68025  115.16348   0.319   0.7515    \nlarea         2.96755    0.65287   4.545 3.97e-05 ***\nldist         0.14456    1.19334   0.121   0.9041    \nlldist        0.34641    0.92835   0.373   0.7107    \nyear         -0.01277    0.05803  -0.220   0.8267    \naltitude      0.01070    0.02390   0.448   0.6565    \nfgraze2       0.52851    3.25221   0.163   0.8716    \nfgraze3       0.06601    2.95871   0.022   0.9823    \nfgraze4      -1.24877    3.19838  -0.390   0.6980    \nfgraze5     -12.47309    4.77827  -2.610   0.0122 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.105 on 46 degrees of freedom\nMultiple R-squared:  0.7295,    Adjusted R-squared:  0.6766 \nF-statistic: 13.78 on 9 and 46 DF,  p-value: 2.115e-10\n\n\nLa première partie de la sortie indique le modèle appliqué et quelques informations de base sur les résidus.\nLa partie située sous « Coefficients » donne les paramètres de régression estimés, les erreurs standard, les \\(t\\)-values et les \\(p\\)-values. La seule partie déroutante de ce résultat est peut-être l’absence de graze niveau 1. Il est utilisé comme base de référence. Ainsi, une parcelle ayant le niveau 2 de graze a 0,52 oiseaux (densité) de plus qu’une parcelle ayant le niveau 1, et une parcelle ayant le niveau 5 de graze a 12,4 oiseaux de moins qu’une parcelle ayant le niveau 1. Les \\(p\\)-values correspondantes indiquent si une parcelle est significativement différente du niveau 1. Notez que vous ne devez pas évaluer l’importance d’un facteur en fonction des \\(p\\)-values individuelles. Nous proposerons une meilleure méthode à cet effet dans un instant. Vous ne devez pas laisser de côté les niveaux individuels d’une variable nominale. Ils sont tous pris en compte ou vous laissez tomber la variable entière. La dernière partie du code donne le \\(R^2\\) et le \\(R^2\\) ajusté (pour la sélection du modèle). Le reste de la sortie devrait, je l’espère, vous être familier.\nLa fonction drop1 fais exactement ce que son nom indique, elle elimine une variable, chacune à son tour et réevalue le modèle:\n\ndrop1(model1, test=\"F\")\n\nSingle term deletions\n\nModel:\nabundance ~ larea + ldist + lldist + year + altitude + fgraze\n         Df Sum of Sq    RSS    AIC F value   Pr(&gt;F)    \n&lt;none&gt;                1714.4 211.60                     \nlarea     1    770.01 2484.4 230.38 20.6603 3.97e-05 ***\nldist     1      0.55 1715.0 209.62  0.0147  0.90411    \nlldist    1      5.19 1719.6 209.77  0.1392  0.71075    \nyear      1      1.81 1716.2 209.66  0.0485  0.82675    \naltitude  1      7.47 1721.9 209.85  0.2004  0.65650    \nfgraze    4    413.50 2127.9 215.70  2.7736  0.03799 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLe modèle complet a une somme des carrés de 1714,43. Chaque fois, \\(un\\) terme est supprimé à tour de rôle et la somme des carrés résiduelle est calculée à chaque fois. Celles-ci sont ensuite utilisées pour calculer une statistique \\(F\\) et une \\(p\\)-value correspondante. Par exemple, pour obtenir le résultat de la première ligne, R ajuste deux modèles. Le premier modèle contient toutes les variables explicatives et le second modèle toutes, sauf \\(larea\\). Il utilise ensuite les sommes des carrés résiduels de chaque modèle dans la statistique \\(F\\) suivante: \\[ F=\\frac{(SCR_1 - SCR_2)/(p-q)}{SCR_2/(n-p)} \\]\nLes termes \\(SCR_1\\) et \\(SCR_2\\) sont les sommes des carrés résiduels du modèle model1 et du modèle \\(model2\\) , respectivement, et \\(n\\) est le nombre d’observations. Le nombre de paramètres dans les modèles 2 et 1 est respectivement \\(p\\) et \\(q (p &gt; q)\\). Les modèles sont imbriqués dans le sens où un modèle est obtenu à partir d’un autre en fixant certains paramètres à 0. L’hypothèse nulle sous-jacente à cette statistique est que les paramètres omis sont égaux à 0 : \\(H_0: \\beta = 0\\)."
  },
  {
    "objectID": "glm.html#glm-poisson",
    "href": "glm.html#glm-poisson",
    "title": "5  Modèles linéaires généralisés",
    "section": "5.1 GLM Poisson",
    "text": "5.1 GLM Poisson"
  },
  {
    "objectID": "glm.html#glm-logistique",
    "href": "glm.html#glm-logistique",
    "title": "5  Modèles linéaires généralisés",
    "section": "5.2 GLM logistique",
    "text": "5.2 GLM logistique"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Zuur, Alain F, Elena N Ieno, Graham M Smith, et al. 2007. Analysing\nEcological Data. Vol. 680. Springer."
  }
]