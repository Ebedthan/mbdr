[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Modélisation des données biologiques avec R",
    "section": "",
    "text": "Bienvenue\nCe livre est le manuel compagnon de la formation modélisation de données biologiques organisée par le service innovation et transfert de technologie des fablab agritech à destination des élèves ingénieurs agronomes ou biologiques de l’Institut National Polytechnique Félix Houphouët-Boigny.\nLa formation se déroule typiquement sur cinq jours et réuni autour de 10 étudiants.\nCe manuel est et sera toujours gratuit, sous licence CC BY-NC-ND 3.0."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "Ce que vous allez apprendre\nAu cours de cette formation nous aborderons les concepts suivants:\nCe livre est un manuel de formation à destination des ingénieurs en sciences agronomiques ou biologiques en fin de cycle (3e année).\nIl se veut pratique et facile à aborder tout en n’hésitant pas à revenir sur les concepts mathématiques avancés pour solidifier les fondements mathématiques des participants.\nCe manuel est utilisé lors de la formation organisée par le biais du service innovation et transfert de technologie des fablabs agritech de l’Institut National Polytechnique Félix Houphouët-Boigny de Côte d’Ivoire.\nCette formation n’est pas une formation à l’utilisation de R. Nous n’aborderons donc pas les notions basiques de son utilisation. Nous ferons cependant l’effort d’apporter de l’aide ou des informations pour chaque fonction utilisée. Nous n’aborderont pas aussi les thèmes des tests d’hypothèses, de la conception d’expérience, de la manipulation des données, de la visualisation des données ou de la programmation littérale avec R.\nCes thèmes sont abordés dans d’autres formations que nous organisons.\nNous avons choisi de faire cette formation en utilisant les fonction R basiques et non sous l’aspect de la philosophie tidyverse (pour laquelle nous consacrons une autre formation). Ce choix est fait pour garder l’attention des étudiants dirigées exclusivement sur la compréhension des concepts statistiques présentés et à leur mise en pratique avec R.\nNous avons fait quelques suppositions sur ce que vous savez déjà pour tirer le meilleur parti de cette formation. Vous devez avoir des connaissances générales en calcul et il est utile que vous ayez déjà une certaine expérience de la programmation de base. Si vous n’avez jamais programmé auparavant, Hands on Programming with R pourrait être un outil précieux.\nVous avez besoin de quatre éléments pour exécuter le code de ce livre : R, RStudio, un certains nombre de jeu de données. Les packages sont les unités fondamentales du code R reproductible. Ils comprennent des fonctions réutilisables, de la documentation décrivant comment les utiliser et des exemples de données."
  },
  {
    "objectID": "intro.html#r",
    "href": "intro.html#r",
    "title": "Introduction",
    "section": "R",
    "text": "R\nPour télécharger R, rendez-vous sur CRAN, le réseau complet d’archives R, https://cloud.r-project.org. Une nouvelle version majeure de R est publiée une fois par an, et il y a 2 à 3 versions mineures par an. C’est une bonne idée de faire des mises à jour régulièrement. La mise à jour peut être un peu fastidieuse, en particulier pour les versions majeures qui vous obligent à réinstaller tous vos paquets, mais la remettre à plus tard ne fait qu’empirer les choses. Nous recommandons R 4.2.0 ou une version ultérieure pour cette formation."
  },
  {
    "objectID": "intro.html#rstudio",
    "href": "intro.html#rstudio",
    "title": "Introduction",
    "section": "RStudio",
    "text": "RStudio\nRStudio est un environnement de développement intégré (IDE) pour la programmation R, que vous pouvez télécharger à partir de https://posit.co/download/rstudio-desktop/. RStudio est mis à jour plusieurs fois par an et vous informe automatiquement de la sortie d’une nouvelle version. C’est une bonne idée de faire des mises à jour régulières pour profiter des dernières et meilleures fonctionnalités. Pour ce livre, assurez-vous d’avoir au moins RStudio 2022.02.0.\nLorsque vous démarrez RStudio , vous voyez deux zones clés de l’interface : le volet de console et le volet de sortie. Pour une exécution du code ligne par ligne, il faut taper le code R dans le volet de la console et appuyer sur la touche Entrée pour l’exécuter. Cependant si l’on veut créer un fichier pour y saisir le code il est possible d’utiliser l’éditeur de texte incorporé."
  },
  {
    "objectID": "datasets.html#le-jeu-de-données-penguins",
    "href": "datasets.html#le-jeu-de-données-penguins",
    "title": "1  Présentation des jeux de données",
    "section": "1.1 Le jeu de données penguins",
    "text": "1.1 Le jeu de données penguins\n\n1.1.1 A propos\n\n\n\nFigure 1.1: Palmer penguins hex sticker (Artwork by allison_horst)\n\n\nLe jeu de données Penguins est un jeu de données collectées et mises à disposition par le Dr. Kristen Gorman et la station Palmer, Antarctica LTER, membre du Long Term Ecological Research Network (réseau de recherche écologique à long terme) et mise à disposition de la communauté R au travers du package palmerpenguins.\nLe jeu de données s’appelle penguins, mais fait références en français à des manchots et non à des pingouins. Pour rappel, il y a deux différences fondamentales entre les pingouins et les manchots: leur répartition géographique et leur (in)capacité à voler. Les pingouins vivent dans l’hémisphère nord et ils peuvent voler! Quant aux manchots, ils ne peuvent pas voler et ils vivent dans l’hémisphère sud. Cependant, lors de ce atelier nous allons faire reference a ce jeu de données en utilisant le terme penguins pour garder le nom original du jeu de données.\nLe jeu de données contient des données de 344 manchots. Il y a 3 espèces différentes de manchots dans ce jeu de données Figure 1.2, collectées sur 3 îles de l’archipel de Palmer, en Antarctique.\n\n\n\nFigure 1.2: Les espèces de manchots dans palmerpenguins\n\n\n\n\n1.1.2 Installation et description courte\nLe package est disponible sur le CRAN et peut être installé à partir de la console R en utilisant la commande ci-dessous:\n\ninstall.packages(\"palmerpenguins\")\n\nLe jeu de donnée est composé de 344 observations et de 8 variables:\n\nstr(penguins)\n\ntibble [344 × 8] (S3: tbl_df/tbl/data.frame)\n $ species          : Factor w/ 3 levels \"Adelie\",\"Chinstrap\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ island           : Factor w/ 3 levels \"Biscoe\",\"Dream\",..: 3 3 3 3 3 3 3 3 3 3 ...\n $ bill_length_mm   : num [1:344] 39.1 39.5 40.3 NA 36.7 39.3 38.9 39.2 34.1 42 ...\n $ bill_depth_mm    : num [1:344] 18.7 17.4 18 NA 19.3 20.6 17.8 19.6 18.1 20.2 ...\n $ flipper_length_mm: int [1:344] 181 186 195 NA 193 190 181 195 193 190 ...\n $ body_mass_g      : int [1:344] 3750 3800 3250 NA 3450 3650 3625 4675 3475 4250 ...\n $ sex              : Factor w/ 2 levels \"female\",\"male\": 2 1 1 NA 1 2 1 2 NA NA ...\n $ year             : int [1:344] 2007 2007 2007 2007 2007 2007 2007 2007 2007 2007 ...\n\n\nLes différentes variables sont l’espèce, l’île (lieu de collecte des données), la longueur du culmen (mm), la profondeur du culmen (mm), la longueur de la nageoire (mm), le poids (g), le sexe et l’année de l’étude. Le culmen est appelé bill dans le jeu de donnée. En zoologie, le culmen est l’arête dorsale de la mandibule supérieure des oiseaux Figure 1.3.\n\n\n\nFigure 1.3: Description du culmen des manchots\n\n\nPour rappel, la description complète du format du jeu de donnée est disponible directement dans R en utilisant la commande ?penguins."
  },
  {
    "objectID": "datasets.html#le-jeu-de-donnée-loyn",
    "href": "datasets.html#le-jeu-de-donnée-loyn",
    "title": "1  Présentation des jeux de données",
    "section": "1.2 Le jeu de donnée loyn",
    "text": "1.2 Le jeu de donnée loyn\n\n1.2.1 A propos\nLes densités d’oiseaux forestiers ont été mesurées dans 56 parcelles forestières du sud-est de l’État de Victoria, en Australie. L’objectif de l’étude était de relier les densités d’oiseaux à six variables d’habitat : taille de la parcelle forestière, distance par rapport à la parcelle la plus proche, distance par rapport à la parcelle plus grande la plus proche, altitude moyenne de la parcelle, année d’isolement par défrichement et indice de l’histoire du pâturage des animaux (1 = léger, 5 = intensif).\n\n\n1.2.2 Variables\n\n\n\n\n\n\n\n\nVariable\nDescription\nType\n\n\n\n\nabundance\nDensité d’oiseaux dans une parcelle de forêt\nVariable réponse continue\n\n\narea\nTaille de la parcelle forestière\nVariable explicative continue\n\n\ndistance\nDistance par rapport à l’îlot le plus proche\nVariable explicative continue\n\n\nldistance\nDistance par rapport à l’îlot plus grand le plus proche\nVariable explicative continue\n\n\naltitude\nAltitude moyenne de la parcelle\nVariable explicative continue\n\n\nyear\nAnnée d’isolement\nVariable explicative continue\n\n\ngraze\nIndice d’intensité du pâturage\nVariable nominale explicative avec 5 niveaux: 1 (faible) à 5 (intensif)\n\n\n\n\n\n\n\n\n\n\n1.2.3 Importer les données\n\nloyn &lt;- read.table(\"Loyn.txt\", header=TRUE)"
  },
  {
    "objectID": "organigram.html",
    "href": "organigram.html",
    "title": "2  Organigramme de relation entre les outils d’analyse",
    "section": "",
    "text": "Nous présentons ici un organigramme montrant comment la régression linéaire, la modélisation additive, la modélisation linéaire généralisée (utilisant la distribution de Poisson et la fonction log-link) et la modélisation additive généralisée (utilisant la distribution de Poisson) sont liées les unes aux autres.\n\n\n\nOrganigramme des outils d’analyse. Adapté de (Zuur et al. 2007)\n\n\nDans la régression linéaire, la violation de l’homogénéité signifie que le GLM (modèle linéaire généralisé) avec une distribution de Poisson peut être utilisé. La normalité mais les relations non linéaires (détectées par exemple par un graphique des résidus en fonction de chaque variable explicative) signifient que la modélisation additive peut être appliquée. Des relations non linéaires et une violation de l’hypothèse de normalité signifient qu’un GAM (modèles additif) avec une distribution de Poisson peut être utilisée. Le graphique changera si une autre fonction de liaison ou une autre distribution est utilisée.\n\n\n\n\nZuur, Alain F, Elena N Ieno, Graham M Smith, et al. 2007. Analysing Ecological Data. Vol. 680. Springer."
  },
  {
    "objectID": "exploration.html#outils-graphiques",
    "href": "exploration.html#outils-graphiques",
    "title": "3  Exploration des données",
    "section": "3.1 Outils graphiques",
    "text": "3.1 Outils graphiques\n\n3.1.1 Boîte à moustache\nUn diagramme en boîte, ou boîte à moustaches, permet de visualiser la moyenne et la dispersion d’une variable univariée. Normalement, le point central d’un diagramme en boîte est la médiane, mais il peut également s’agir de la moyenne. Les quartiles 25% et 75% (Q25 et Q75) définissent les charnières (extrémités des boîtes), et la différence entre les charnières est appelée l’écart. Des lignes (ou moustaches) sont tracées à partir de chaque charnière jusqu’à 1,5 fois l’écart ou jusqu’à la valeur la plus extrême de l’écart, la plus petite étant retenue. Tous les points situés en dehors de ces valeurs sont normalement identifiés comme des valeurs aberrantes Figure 3.1.\n\n\n\nFigure 3.1: Boîte à moustache\n\n\n\n\n\n\n\n\nLa syntaxe générale pour créer la boîte à moustache d’une variable en utilisant base R est: boxplot(variable).\nL’aide complète de la fonction boxplot() est obtenue par la commande ?boxplot().\n\n\n\nEtudions la variable body_mass_g en représentant sa boîte à moustache.\n\nboxplot(penguins$body_mass_g, xlab=\"Masse (g)\")\n\n\n\n\nOn remarque qu’aucune valeur aberrante n’apparait de façon visible. De plus on observe que la médiane du poids des manchots se situent autour de 4000 g (on peut facilement confirmer cela en calculant la médiane median(penguins$body_mass_g)).\nOn peut alors être amenée à continuer l’exploration en étudiant le poids, cette fois-ci en étudiant la variable par espèce de manchot.\n\nboxplot(body_mass_g ~ species, data=penguins, ylab=\"Masse (g)\", xlab=NULL)\n\n\n\n\nLes boîte à moustaches representées nous permettent de nous rendre compte que l’espèce Gentoo a sensiblement une masse plus élevée que les deux autres. On pourra confirmer si cette différence est significative avec une analyse de variance. De plus, on observe la présence de quelques valeurs aberrantes chez l’espèce Chinstrap.\n\n\n3.1.2 Diagramme de points de Cleveland\nLes diagrammes de points de Cleveland sont utiles pour identifier les valeurs aberrantes et l’homogénéité.\nL’homogénéité signifie que la variance des données ne change pas le long des gradients. La violation de cette condition est appelée hétérogénéité et l’homogénéité est une hypothèse cruciale pour de nombreuses méthodes statistiques.\nLa valeur est présentée sur l’axe horizontal et l’ordre des points (tel qu’il est organisé par le programme) est présenté sur l’axe vertical.\n\n\n\n\n\n\nLa syntaxe pour générer un diagramme de points de Cleveland avec base R est dotchart(variable). L’aide est disponible avec la commande ?dotchart().\n\n\n\nRepresentons le diagramme de point de Cleveland avec pour but d’identifier une possible violation de l’homogénéité ou la présence de valeurs aberrantes. Le diagrammes à points est réalisé en utilisant différents symboles conditionnels à une variable explicative nominale qui est ici l’espèce.\n\ndotchart(penguins$body_mass_g, main=\"Masse (g)\", pch=as.numeric(penguins$species))\n\n\n\n\nTout point isolé à droite ou à gauche indique des valeurs aberrantes, mais dans cet jeu de données, en considérant l’ensemble du graphique aucun point n’est considéré comme aberrant, ce qui confirme notre observation de la boîte à moustache. Cependant comme observé plus haut, en prenant les points par groupe (chaque symbole représentant une espèce différente) on observe bien un groupe présentant des valeurs aberrantes.\n\n\n3.1.3 Exercices\n\nExplorez la présence de valeurs aberrantes pour les variables bill_depth_mm, bill_length_mm et flipper_length_mm à l’aide de boîte à moustaches et de diagrammes en points de Cleveland.\n\n\n\n3.1.4 Histogramme\nUn histogramme montre le centre et la distribution des données et donne une indication de la normalité. Toutefois, l’application d’une transformation des données pour les faire correspondre à une distribution normale nécessite des précautions.\n\n# Subdivision du panel graphique\nlayout(matrix(c(1, 2, 1, 3), nrow = 2, byrow = TRUE))\n\n# Histrogramme de la masse pour l'ensemble des manchots\nhist(penguins$body_mass_g, main=\"Masse (g)\", xlab=NULL)\n\n# Histrogramme de la masse pour les males\nhist(penguins$body_mass_g[penguins$sex==\"male\"], main=\"Males\", xlab=NULL)\n# Histrogramme de la masse pour les femelles\nhist(penguins$body_mass_g[penguins$sex==\"female\"], main=\"Females\", xlab=NULL)\n\n\n\n\nLa forme de l’histogramme montre une certaine asymétrie et l’on pourrait être tenté d’appliquer une transformation. Cependant, un histogramme conditionnel donne une image assez différente. Dans un histogramme conditionnel, les données sont divisées en fonction d’une variable nominale et les histogrammes des sous-ensembles sont tracés côte-à-côte. A ce moment on obtient une figure tout autre montrant une bimodalité aussi bien chez les males que chez les femelles manchots. On a une différence claire du centre de la distribution et la pic initial de données observé sur le premier histogramme est grandement du aux femelles. Il faudrait donc explorer les effets du sexe sur le poids des manchots ainsi que les interactions avant d’envisager une transformation des données.\n\n\n3.1.5 QQ-plots\nUn graphique Quantile-Quantile (QQ-plots) est un outil graphique utilisé pour déterminer si les données suivent une distribution particulière. Le graphique QQ pour une distribution normale compare la distribution d’une variable donnée à la distribution gaussienne. Si les points obtenus se situent approximativement sur une ligne droite, on considère que la distribution des données est la même que celle d’une variable normalement distribuée.\nLe \\(p\\)-ème quantile \\(q\\) d’une variable aléatoire \\(y\\) est donnée par \\(F(q) = P(y \\leq q) = p\\). Si l’on souhaire savoir quelle valeur de \\(q\\) appartient à \\(p\\), on inverse la formule précédente pour obtenir \\(q = F^{-1}(p)\\). Supposons que nous avons cinq observations \\(Y_i\\) avec les valeurs 1, 2, 3, 4, 5. Par définition, le premier chiffre est le 0% percentile, le milieu est le 50% percentile et 5 est le 100% percentile. La différence entre un quantile et un percentile est un seulement le facteur 100. Les QQ-plots sont soit basés sur les percentiles ou typiquement sur les points quantiles de l’échantillon définis par \\((i-0.5)/n\\) où \\(i\\) varie de 1 à 5 et \\(n=5\\) dans notre example. Ainsi pour notre exemple, les points de quantiles de l’échantillon sont 0,1, 0,3, 0,5, 0,7 et 0,9. Ce sont les valeurs de \\(p\\) pour l’échantillon. Dans la séconde étape, ces quantiles de l’échantillon sont comparés à une distribution normale. Celà signifie que la fonction de densité \\(P(y\\leq q)\\) est désormais une fonction de densité normale et \\(F()\\) est désormais la fonction de répartition de la loi normale.\nLe QQ-plot est donc un graphique des valeurs de \\(Y_i\\) de l’échantillon comparés aux \\(q_i\\). On peut ajouter à ce graphique dans R, une ligne qui connecte les 25e et le 75e quartiles.\nNous appliquons dans le même temps une transformation des données pour visualiser laquelle produit le meilleur ajustement.\nIl est très souvent utilse de combiner les QQ-plots avec des transformations de puissance, qui est donnée par \\[ \\frac{Y^p - 1}{p}, \\forall p \\neq 0; log(Y) , p = 0\\]\nVeuillez noter que le \\(p\\) utilisé ici n’est pas le \\(p\\) utilisé pour décrire les quantiles. Il est aussi utile de comparer plusieurs QQ-plots pour différentes valeurs de \\(p\\).\n\n# Transformation racine carrée\nbmsq &lt;- sqrt(penguins$body_mass_g)\n# Transformation racine quatrième\nbmfq &lt;- penguins$body_mass_g^(0.25)\n# Transformation logarithmique\nbmlog &lt;- log(penguins$body_mass_g)\n\nDans le graphique ci-dessous, aucune transormation semble prendre le dessus sur l’autre.\n\nlayout(matrix(c(1, 2, 3, 4), nrow = 2, byrow = TRUE))\nqqnorm(penguins$body_mass_g, main=\"Aucune transformation (p=0)\")\nqqline(penguins$body_mass_g)\nqqnorm(bmsq, main=\"Racine carré (p=0.5)\")\nqqline(bmsq)\nqqnorm(bmfq, main=\"Racine quatrième (p=0.25)\")\nqqline(bmfq)\nqqnorm(bmlog, main=\"Logarithme (p=1)\")\nqqline(bmlog)\n\n\n\n\n\n\n3.1.6 Exercices\n\nExplorez la normalité des variables bill_depth_mm, bill_length_mm et flipper_length_mm à l’aide d’histogrammes et de diagramme en points de Cleveland.\n\n\n\n3.1.7 Nuage de points\nJusqu’à présent, l’accent a été mis sur la détection des valeurs aberrantes, la vérification de la normalité et l’exploration d’ensembles de données associés à des variables explicatives nominales uniques. Les techniques suivantes s’intéressent aux relations entre plusieurs variables. Un nuage de points est un outil permettant de trouver une relation entre deux variables. Il représente une variable sur l’axe horizontal et une seconde variable sur l’axe vertical. Pour aider à visualiser la relation entre les variables, une ligne droite ou une courbe de lissage est souvent ajoutée au graphique.\nLa figure suivante montre le nuage de points entre les variables flipper_length_mm (taille de la nageoire) et body_mass_g (masse du manchot).\n\nplot(penguins$flipper_length_mm, penguins$body_mass_g, xlab=\"Taille des nageoires\", ylab=\"Masse (g)\")\nm1 &lt;- lm(body_mass_g ~ flipper_length_mm, data=penguins)\nabline(m1)\n\n\n\n\n\n\n3.1.8 Pairplots\nSi vous avez plus de deux variables, vous pouvez produire une série de nuage de points : un pour chaque paire de variables. Cependant, le nombre de diagrammes augmente rapidement si vous avez plus de trois variables à explorer. Une meilleure approche, jusqu’à environ 10 variables explicatives, est le diagramme de paires encore appelée matrice de nuage de points. Ces diagrammes présentent plusieurs nuage de points par paire dans un seul graphique et peuvent être utilisés pour détecter les relations entre les variables et pour détecter la colinéarité.\nLa figure suivante montre un pairplot entre les variables body_mass_g, flipper_length_mm, bill_length_mm et bill_depth_mm. Chaque sous-graphique est un nuage de points entre deux variables, avec les etiquettes de chaque variable ajoutée dans la diagonale.\n\n# penguins[3:6] sélectionne les variables bill_length_mm\n# bill_depth_mm, flipper_length_mm, body_mass_g\npairs(penguins[,3:6])\n\n\n\n\nIl est aussi possible d’ajouter une droite ajustée à chaque graphique.\n\npairs(penguins[,3:6], panel=panel.smooth)\n\n\n\n\nOn parle de colinéarité lorsqu’il existe une forte corrélation entre deux (ou plus) variables.\nLa figure ci-dessous montre un autre pairplot du même jeu de donnée sur lequel nous avons, cette fois-ci ajouté les coefficients de corrélation entre les variables dans le triangle bas du pairplot.\nPour cela, nous utiliserons une fonction panel.cor définie comme suit:\n\npanel.cor &lt;- function(x, y, digits=1, prefix=\"\", cex.cor) {\n  usr &lt;- par(\"usr\")\n  on.exit(par(usr))\n  par(usr=c(0, 1, 0, 1))\n  r1 &lt;- cor(x, y, use=\"pairwise.complete.obs\")\n  r &lt;- abs(cor(x, y, use=\"pairwise.complete.obs\"))\n  \n  txt &lt;- format(c(r1, 0.123456789), digits=digits)[1]\n  txt &lt;- paste(prefix, txt, sep=\"\")\n  if (missing(cex.cor)) {\n    cex &lt;- 0.9/strwidth(txt)\n  }\n  text(0.5, 0.5, txt, cex = cex * r)\n}\n\nPar la suite le diagramme est construit en faisant:\n\npairs(penguins[,3:6], lower.panel=panel.cor)\n\n\n\n\nIl faut noter qu’il existe une forte colinéarité entre la taille de la nageoire et la masse du manchot.\nDes diagrammes en paires doivent être établis pour chaque analyse. Ils doivent comprendre (i) un diagramme par paire de toutes les variables de réponse (en supposant que plus d’une variable de réponse soit disponible) ; (ii) un diagramme par paire de toutes les variables explicatives ; et (iii) un diagramme par paire de toutes les variables de réponse et de toutes les variables explicatives.\nLe premier graphique (i) fournit des informations qui aideront à choisir les techniques multivariées les plus appropriées. On espère que les variables de réponse présenteront de fortes relations linéaires (certaines techniques telles que l’ACP dépendent de relations linéaires). Toutefois, si le graphique (ii) montre une relation linéaire claire entre les variables explicatives, indiquant une colinéarité, nous savons que nous avons un problème majeur à résoudre avant de poursuivre l’analyse.\nLe graphique (iii) permet de déterminer si les relations entre les variables de réponse et les variables explicatives sont linéaires. Si ce n’est pas le cas, plusieurs options s’offrent à nous. La plus simple consiste à appliquer une transformation aux variables de réponse et/ou explicatives afin de linéariser les relations.\nD’autres options seront explorée le long du séminaire.\n\n\n3.1.9 Coplot\nUn coplot est un nuage de points conditionnel montrant la relation entre y et x, pour différentes valeurs d’une troisième variable z, voire d’une quatrième variable w. Les variables conditionnelles peuvent être nominales ou continues.\nLa figure suivante présente un coplot entre la masse des manchots et la taille de leur nageoire conditionée par la variable nominale espèce.\nLes panneaux sont classés de la partie inférieure gauche à la partie supérieure droite. Cet ordre correspond à des valeurs croissantes de la variable explicative du conditionnement.\n\ncoplot(body_mass_g ~ flipper_length_mm | species, data = penguins)\n\n\n\n\n\n Missing rows: 4, 272 \n\n\n\n\n3.1.10 Exercices\n\nExplorez la présence de colinéarité entre les variables body_mass_g vs bill_depth_mm et body_mass_g vs bill_length_mm à l’aide de nauge de points et de coplot.\n\n\n\n3.1.11 Diagrammes de conception et d’interaction\nLes diagrammes de conception et d’interaction sont un autre outil précieux pour explorer les ensembles de données avec des variables nominales et sont particulièrement utiles à utiliser avant d’appliquer la régression, la GLM, la modélisation mixte ou l’analyse de variance.\nIls permettent de visualiser (i) les différences entre les valeurs moyennes de la variable réponse pour différents niveaux de variables nominales et (ii) les interactions entre les variables explicatives.\nLa figure suivante montre un diagramme de conception entre la masse des manchots et les trois variables nominale du jeu de données: species, sex et island. Il nous permet de comparer directement les moyennes (ou médianes) de chaque variable nominale en utilisant un seul graphique.\nLe graphique nous montre que la masse moyenne de l’espèce Gentoo est située autour de 5100g et est sensiblement plus élevée que celle de Chinstrap et Adélie. De même on peut aussi remarquer que la masse moyenne des mâles est plus grande que celle des femelles.\n\nplot.design(body_mass_g ~ species + sex + island, data = penguins)\n\n\n\n\nCependant, les diagrammes de conception ne nous permettent pas d’explorer les interactions entre les variables explicatives, d’où la nécessité d’utiliser les diagrammes d’interactions.\n\n\n\n\n\n\nInterprétation d’un diagramme d’interaction\n\nIdentifier les facteurs et les niveaux: observez l’abscisse pour identifier les niveaux du premier facteur et l’ordonnée pour identifier les niveaux du deuxième facteur.\nÉvaluer le parallélisme des lignes: si les lignes sont parallèles, cela signifie qu’il n’y a pas d’interaction entre les facteurs; l’effet d’un facteur est constant pour tous les niveaux de l’autre facteur. Cependant, si les lignes ne sont pas parallèles (c’est-à-dire qu’elles se croisent ou divergent), cela indique une interaction entre les facteurs. L’effet d’un facteur dépend alors du niveau de l’autre facteur.\nAmpleur de l’interaction: le degré de divergence ou de convergence des lignes peut donner une indication de la force de l’interaction. Une divergence ou une convergence plus importante indique un effet d’interaction plus fort.\nDirection des effets: observez si la variable de réponse augmente ou diminue avec les changements de niveau des facteurs. Cela peut aider à déterminer la nature de l’interaction et des effets principaux.\n\nPour résumer\n\nPas d’interaction: les lignes parallèles suggèrent que les deux facteurs n’interagissent pas. Les effets principaux de chaque facteur sont additifs.\nInteraction présente: les lignes non parallèles indiquent une interaction entre les facteurs, ce qui signifie que l’effet d’un facteur dépend du niveau de l’autre facteur.\nForce et direction: le degré de divergence ou de convergence et la direction des lignes donnent des indications sur la force et la nature de l’interaction.\n\n\n\n\nLe premier diagrame d’interaction ci-dessous montre l’interaction entre l’espèce et le sexe. On observe qu’il n’y a une interaction entre le sexe et la masse des manchots, ce qui signifie que la masse dépend du sexe et de l’espèce de manchot.\n\ninteraction.plot(penguins$species, penguins$sex, penguins$body_mass_g)\n\n\n\n\nLe second diagramme montre l’interaction entre l’île (l’habitat du manchot), le sexe et la masse. On observe qu’il existe aussi une interaction entre ces variables.\n\ninteraction.plot(penguins$sex, penguins$island, penguins$body_mass_g)\n\n\n\n\n\n\n3.1.12 Exercices\n\nQuels types d’interaction il existe entre sex, species et flipper_length_mm ? Explorez de même le type d’interaction entre sex, species et bill_length_mm."
  },
  {
    "objectID": "exploration.html#valeurs-aberrantes-transformations-et-standardisation",
    "href": "exploration.html#valeurs-aberrantes-transformations-et-standardisation",
    "title": "3  Exploration des données",
    "section": "3.2 Valeurs aberrantes, transformations et standardisation",
    "text": "3.2 Valeurs aberrantes, transformations et standardisation\n\n3.2.1 Valeurs aberrantes\nUne valeur aberrante est un point de données qui, en raison de sa valeur extrême par rapport au reste de l’ensemble de données, peut influencer incorrectement une analyse. La première question qui se pose est donc la suivante: “Comment identifier une valeur aberrante?” Une approche simple pourrait consister à quantifier tout ce qui est aberrant au-delà d’une certaine distance par rapport au centre des données. On pourrait par exemple considérer les points situés en dehors des charnières d’un diagramme en boîte comme des valeurs aberrantes. Cependant, si la quantité des données n’est pas suffisante, il pourrait être difficile de considérer ces valeurs comme abberantes.\nLes nuages de points peuvent aussi nous permettre d’identifier les valeurs aberrantes. Ainsi, même si une observation n’est pas considérée comme aberrante dans l’espace x ou dans l’espace y (alors possiblement identifiée par une boîte à moustache), elle peut l’être dans l’espace xy. La situation dans laquelle une observation est une valeur aberrante dans l’espace x, ainsi que dans l’espace y, mais pas dans l’espace xy, est également possible.\nPour résumer la détection des valeurs aberrantes peut rapidemment devenir difficile, cependant l’analyse pourrait permettre d’y voir plus clair.\n\n\n3.2.2 Transformation\n\n3.2.2.1 Quelques transformations usuelles\nIl existe de nombreuses raisons de transformer les données, mais c’est généralement parce que les données présentent des valeurs aberrantes et des distributions non normales. La transformation des données (sur les variables de réponse) sera également nécessaire lorsque vous prévoyez d’utiliser l’analyse discriminante et qu’il existe des preuves évidentes (par exemple, en utilisant un diagramme en pointillés de Cleveland) d’hétérogénéité.\nDe plus, le choix de la transformation est influencée par le choix de l’analyse de suivi. Pour certaines techniques, telles que les arbres de classification ou de régression, la transformation des variables explicatives ne change rien aux résultats. Cependant, la plupart des techniques peuvent nécessiter une certaine transformation des données brutes avant l’analyse.\nLe problème le plus facile à résoudre est celui où les observations extrêmes identifiées au cours de l’étape d’exploration des données s’avèrent être des erreurs de frappe. Nous supposerons toutefois que cette solution facile n’existe pas et que nous disposons d’un ensemble de données contenant de véritables observations extrêmes. Si ces observations extrêmes se trouvent dans les variables explicatives, une transformation des variables explicatives (continues) est certainement nécessaire, en particulier si l’on applique des techniques de régression, d’analyse de la covariance, de GLM, de GAM ou des techniques multivariées telles que l’analyse de la redondance et l’analyse canonique des correspondances.\nLorsque les observations extrêmes se trouvent dans la variable réponse, plusieurs approches sont possibles. Vous pouvez soit transformer les données, soit appliquer une technique légèrement plus performante pour traiter les valeurs extrêmes, comme un GLM ou un GAM avec une distribution de Poisson. Cette dernière méthode ne fonctionne que s’il y a une augmentation de la dispersion des données observées pour des valeurs plus élevées. Il est également possible d’utiliser des modèles de quasi-Poisson si les données sont trop dispersées.\n\n\n\n\n\n\nVous ne devez jamais appliquer une racine carrée ou une transformation logarithmique à la variable réponse, puis continuer avec un modèle GLM de Poisson, car cela applique la correction deux fois.\n\n\n\nUne solution plus radicale pour les observations extrêmes consiste à les omettre purement et simplement de l’analyse. Toutefois, si vous adoptez cette approche, vous devez toujours fournir les résultats de l’analyse avec et sans les observations extrêmes. Si les grandes valeurs proviennent toutes d’une région, d’un mois ou d’un sexe, il est possible d’utiliser différentes composantes de variance dans le modèle de régression linéaire, ce qui permet d’obtenir des moindres carrés généralisés (GLS).\nSupossons par exemple que nous souhaitons faire une regression linéaire. Le diagramme de points de Cleveland ou les boîtes à moustaches nous indiquent qu’il n’y a pas de valeurs aberrantes préoccupantes, mais le nuage de points d’une variable réponse et d’une variable explicative montre une relation non linéaire évidente. Dans ce cas, nous devrions envisager de transformer l’une des variables ou les deux. Mais quelle transformation utiliser ? L’éventail des transformations possibles pour les variables réponse et explicative peut être choisi parmi les suivantes \\[ y^{\\frac{1}{4}}, y^{\\frac{1}{3}}, y^{\\frac{1}{2}}, y, log(y), y^2, y^3, y^4, ..\\]\nCes transformations sont des cas particulier de la transformation de puissance de Box-Cox qui est une famille de transformation qui ne peuvent être appliquée que sur les données non-négatives.\nUne autre alternative est d’utiliser la transformation en rang (méthode utilisée par la plupart des méthodes non paramétrique) ou la transformation en données binaires. Par exemple, en supposant que l’on a la série statistique suivante 2, 7, 4, 9, 22, 40, la transformation en rang va donner 1, 3, 2, 4, 5, 6. Si l’on a la série statistique 0, 1, 3, 0, 4, 0, 100 alors la transformation en série binaire va donner 0, 1, 1, 0, 1, 0, 1.\n\n\n3.2.2.2 Stratégies pour le choix d’une transformation\nIl existe plusieurs stratégies pour le choix de la transformation la plus appropriée. Nous présentons ici deux d’entre elles: essai-erreur et la règle de Mosteller-Tukey.\nLa transformation par essai-erreur est le fait de tester différentes transformation et de visualiser les résultats à l’aide des outils graphiques présentés ci-dessus afin d’effectuer la sélection de la meilleure transformation. En utilisant cette méthode, il es important de reporter les résultats incluant aussi bien les transformations réussies que celles qui ont échouées.\nLorsque l’analyse qui suivra nécessite des relations linéaire entre les variables, la règle de Mosteller-Tukey Figure 3.2 qui appartient à la famille des transformations de Box-Cox.\n\n\n\nFigure 3.2: Mosteller-Tukey bulging rule\n\n\nCette approche est basée sur l’identification des motifs non-linéaires par l’inspection des nuages de points. Les transformations requises peuvent alors être inférée du graphique Figure 3.2.\nPar exemple, si la forme du nuage de points est similaire à la forme du quadrant en bas à gauche de la figure Figure 3.2, alors soit if faudra effectuer une transformation \\(x^2\\) ou \\(x^3\\) de la variable explicative ou effectuer une transformation \\(log(y)\\) ou \\(y^{0.5}\\) de la variable réponse.\n\n\n\n3.2.3 Standardisation\nSi les variables comparées proviennent d’échelles très différentes, comme la comparaison des taux de croissance de petites espèces de poissons avec ceux de grandes espèces de poissons, la standardisation (conversion de toutes les variables à la même échelle) peut être une option. Toutefois, cela dépend de la technique statistique utilisée. Il existe plusieurs méthodes pour convertir les données à la même échelle, et une option consiste à centrer toutes les variables autour de zéro par \\[ Y_i^{new} = Y_i - \\bar{Y} \\] où \\(\\bar{Y}\\) est la moyenne de l’échantillon et \\(Y_i\\) est la valeur de la i-ème observation. Cependant la standardisation la plus utilisée est donnée par \\[ Y_i^{new} = (Y_i - \\bar{Y}) / s_y \\] où \\(s_y\\) est l’écart type de l’échanitllon. Les valeurs obtenues sont alors centrées autour de zéro, avec une variance de 1, et sont sans unités. Cette transformation est aussi appelée normalisation.\nComme pour les autres transformations, la décision de standardiser vos données dépend de la technique statistique que vous envisagez d’utiliser. Par exemple, si vous souhaitez comparer des paramètres de régression, vous pouvez juger utile de normaliser les variables explicatives avant l’analyse, en particulier si elles sont exprimées dans des unités différentes ou si elles ont des intervalles différents. Certaines techniques, telles que l’analyse en composantes principales, normalisent ou centrent automatiquement les variables."
  },
  {
    "objectID": "exploration.html#conclusion",
    "href": "exploration.html#conclusion",
    "title": "3  Exploration des données",
    "section": "3.3 Conclusion",
    "text": "3.3 Conclusion\n\n3.3.1 Même si vous ne les voyez pas, elles peuvent être présentes\nMême si les nuages de points suggèrent l’absence de relation entre Y et X, cela ne signifie pas nécessairement qu’il n’en existe pas. Un nuage de points ne montre que la relation entre deux variables, et l’inclusion d’une troisième, d’une quatrième ou même d’une cinquième variable peut conduire à une conclusion différente.\n\n\n3.3.2 Prochaines étapes\nUne fois l’exploration des données terminée, l’étape suivante consiste à vérifier et à étudier les modèles et les relations que cette étape a permis d’identifier. Si le nuage de points indique une relation linéaire entre les variables, la régression linéaire est l’étape suivante évidente. Toutefois, si le nuage de points suggère un modèle non linéaire clair, une approche différente doit être adoptée, qui peut inclure (i) l’utilisation d’interactions et/ou de termes quadratiques dans le modèle de régression linéaire, (ii) la transformation des données, (iii) la poursuite avec un modèle de régression non linéaire, (iii) l’utilisation d’une modélisation linéaire généralisée, (iv) l’application de techniques de modélisation additive généralisée, ou (v) l’application de techniques de modélisation mixtes (additives). Toutes ces approches sont étudiées dans les chapitres suivants. La première option consiste à utiliser le modèle de régression linéaire, mais il faut s’assurer que toutes les hypothèses sont respectées. Pour choisir l’approche la plus appropriée, il faut connaître les hypothèses des méthodes sélectionnées et disposer d’outils permettant de détecter les violations (à l’aide des résidus). Tous ces éléments sont abordés dans les chapitres suivants, mais tout se résume à quelque chose de très fondamental : apprenez de vos erreurs."
  },
  {
    "objectID": "regression.html#introduction",
    "href": "regression.html#introduction",
    "title": "4  Regression linéaire",
    "section": "4.1 Introduction",
    "text": "4.1 Introduction\nAu chapitre précédent, nous avons utilisé divers outils graphiques (diagrammes en points de Cleveland, boîte à moustache, histogrammes) pour explorer la forme de nos données (normalité), rechercher la présence de valeurs aberrantes et évaluer la nécessité de transformer les données.\nNous avons également abordé des méthodes plus complexes (coplot, scatterplot, pairplots) qui ont permis de voir les relations entre une seule variable de réponse et plus d’une variable explicative. Il s’agit de la première étape essentielle de toute analyse qui permet de se faire une idée sur les données avant de passer à des outils statistiques formels tels que la régression linéaire.\nTous les jeux de données ne se prêtent pas à la régression linéaire. Pour les données de comptage ou de présences-absences, la modélisation linéaire généralisée (GLM) est plus appropriée. Et lorsque les modèles paramétriques utilisés par la régression linéaire et la GLM donnent de mauvais résultats, les techniques non paramétriques telles que la modélisation additive et la modélisation additive généralisée (GAM) sont susceptibles de donner de meilleurs résultats.\nLes techniques telles que le GLM et le GAM sont plus difficiles à comprendre et à mettre en œuvre. C’est pourquoi nous commencerons donc par résumer brièvement les principes sous-jacents de la modélisation linéaire."
  },
  {
    "objectID": "regression.html#présentation-de-lanalyse",
    "href": "regression.html#présentation-de-lanalyse",
    "title": "4  Regression linéaire",
    "section": "4.2 Présentation de l’analyse",
    "text": "4.2 Présentation de l’analyse\nAvant de taper avec enthousiasme le code R pour la régression linéaire et de l’exécuter, nous devrions d’abord réfléchir à ce que nous voulons faire. Le but de l’analyse est de trouver une relation entre les densités d’oiseaux (abundance) et les six variables explicatives. Mais il se pourrait bien que les oiseaux perçoivent l’effet area différemment selon que les valeurs de graze sont faibles ou élevées. Si c’est le cas, nous devons inclure un terme d’interaction entre graze et larea. Le problème est qu’il existe un grand nombre d’interactions bidirectionnelles potentielles. Les oiseaux peuvent réagir différemment à area si les valeurs de graze sont faibles en combinaison avec de faibles valeurs d’altitude (altitude). Plus l’ensemble de données est petit (56 observations, c’est petit), plus il est difficile d’inclure de multiples interactions.\nIl peut arriver que vous ne disposiez pas d’un nombre suffisant d’observations par combinaison. Dans ce cas, les observations individuelles peuvent devenir particulièrement influentes. De nombreux groupes de discussion statistiques ont de longs fils de discussion sur le sujet de l’interaction, voici les points que je vous recommande:\n\nCommencer par un modèle sans interactions. Appliquer le modèle, la sélection du modèle et la validation du modèle. Si la validation montre qu’il existe des motifs dans les résidus, cherchez-en la raison. L’ajout d’interactions peut être une option pour améliorer le modèle.\nUtiliser les connaissances biologiques pour décider quelles sont les interactions qu’il est judicieux d’ajouter, le cas échéant.\nAppliquer une bonne exploration des données pour voir quelles interactions peuvent être importantes."
  },
  {
    "objectID": "regression.html#modélisation",
    "href": "regression.html#modélisation",
    "title": "4  Regression linéaire",
    "section": "4.3 Modélisation",
    "text": "4.3 Modélisation\nNous débutons l’analyse avec un modèle sans interactions.\n\nloyn &lt;- read.table(\"Loyn.txt\", header=T)\nloyn$larea &lt;- log(loyn$area)\nloyn$ldist &lt;- log(loyn$distance)\nloyn$lldist &lt;- log(loyn$ldistance)\nloyn$fgraze &lt;- factor(loyn$graze)\nmodel1 &lt;- lm(abundance ~ larea + ldist + lldist + year + altitude + fgraze, data = loyn)\n\nLa question qui se pose maintenant est la suivante : Devons-nous examiner d’abord la sortie numérique ou la sortie graphique ? Il est inutile d’appliquer une validation détaillée du modèle si rien n’est significatif. D’un autre côté, pourquoi examiner les résultats numériques si toutes les hypothèses ne sont pas respectées ? Il est peut-être préférable de commencer par les résultats numériques, car cela prend moins de temps et c’est plus facile. Il existe plusieurs façons d’obtenir le résultat numérique de notre modèle de régression linéaire.\n\n4.3.1 Visualisation des résultats\n\n4.3.1.1 Évaluation de la signification des paramètres de régression\nUn aspect important de la régression linéaire est la répartition de la variabilité totale. La variance totale de \\(Y\\), désignée par \\(SS_{total}\\), peut être divisée en deux parties: la partie expliquée par \\(X\\) (\\(SS_{régression}\\)) et la partie non expliquée par \\(X\\) (\\(SS_{résidual}\\)). \\(SS_{regression}\\) mesure à quel point la partie régression (\\(X\\)) explique \\(Y\\), et \\(SS_{residual}\\) montre la quantité de variabilité dans les valeurs \\(Y\\) qui ne peut être expliquée par le modèle de régression. La plupart des programmes statistiques produisent des tableaux ANOVA de la forme suivante.\n\n\n\n\n\n\n\n\n\n\nSource de variation\nSomme des carrés\nDegré de liberté\nSomme des carrés moyens\nSomme des carrés moyens attendu\n\n\n\n\nRegression\n\\[                                \n                       \\sum^n_{i=1}(\\hat Y_i - \\bar Y)^2  \n                       \\]\n\\[               \n                                                           1                 \n                                                           \\]\n\\[                                          \n                                                                              \\sum^n_{i=1}\\frac{(\\hat Y_i - \\bar Y)^2}{1}  \n                                                                              \\]\n\\[                                                           \n                                                                                                                            \\sigma^2_\\epsilon + \\beta^2\\sum^n_{i=1}(\\hat X_i - \\bar X)^2  \n                                                                                                                            \\]\n\n\nResidus\n\\[                                \n                       \\sum^n_{i=1}(Y_i - \\hat Y_i)^2     \n                       \\]\n\\[               \n                                                           n-2               \n                                                           \\]\n\\[                                          \n                                                                              \\sum^n_{i=1}\\frac{(Y_i - \\hat Y_i)^2}{n-2}   \n                                                                              \\]\n\\[                                                           \n                                                                                                                            \\sigma^2_\\epsilon                                             \n                                                                                                                            \\]\n\n\nTotal\n\\[                                \n                       \\sum^n_{i=1}(Y_i - \\bar Y)^2       \n                       \\]\n\\[               \n                                                           n-1               \n                                                           \\]\n\n\n\n\n\n\n\n\n\n\n\n\nLa somme des carrés dépend de la taille de l’échantillon \\(n\\). Si l’on utilise davantage d’observations, les sommes des carrés sont plus importantes. Par conséquent, les sommes des carrés sont transformées en composantes de variance en les divisant par les degrés de liberté. Les degrés de liberté pour la somme des carrés de régression sont le nombre de paramètres de régression moins 1. Les degrés de liberté pour la somme totale des carrés est \\(n - 1\\). S’il n’y avait pas de variables explicatives, la variance serait estimée à partir du rapport entre la somme totale des carrés et \\(n - 1\\).\nLes degrés de liberté pour la somme des carrés résiduels est \\(n - 2\\); deux paramètres de régression ont été estimés pour calculer cette composante: l’ordonnée à l’origine et la pente. Le rapport des deux composantes de la variance est appelé carré moyen (MS). Les MS sont des variances d’échantillon et, par conséquent, des paramètres d’estimation. \\(MS_{residual}\\) estime \\(\\sigma^2_\\epsilon\\) et \\(MS_{regression}\\) estime \\(\\sigma^2_\\epsilon\\) plus un terme supplémentaire dépendant de \\(\\beta\\) et \\(X\\).\nDans la régression bivariée, le tableau ANOVA est utilisé pour tester l’hypothèse nulle selon laquelle la pente de la ligne de régression est égale à zéro (\\(H_0\\): \\(\\beta\\) = 0). Sous cette hypothèse nulle, le MS attendu pour la composante de régression est égal à un. Ainsi, le rapport des deux composantes de variance \\(MS_{regression}\\) et \\(MS_{residual}\\) est également égal à 1. Si, pour les données de notre échantillon, le rapport est supérieur à un, il y a des raisons de penser que l’hypothèse nulle est fausse. Si les quatre hypothèses de régression se vérifient, le rapport de \\(MS_{regression}\\) et \\(MS_{residual}\\) suivra une distribution \\(F\\) avec \\(ddl_{regression}\\) et \\(ddl_{residual}\\) degrés de liberté.\nLe tableau ANOVA permet donc de vérifier s’il existe une relation linéaire entre \\(Y\\) et \\(X\\).\n\n\n4.3.1.2 Commande summary\nLa commande summary donne le résultat suivant:\n\nsummary(model1)\n\n\nCall:\nlm(formula = abundance ~ larea + ldist + lldist + year + altitude + \n    fgraze, data = loyn)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.8992  -2.7245  -0.2772   2.7052  11.2811 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  36.68025  115.16348   0.319   0.7515    \nlarea         2.96755    0.65287   4.545 3.97e-05 ***\nldist         0.14456    1.19334   0.121   0.9041    \nlldist        0.34641    0.92835   0.373   0.7107    \nyear         -0.01277    0.05803  -0.220   0.8267    \naltitude      0.01070    0.02390   0.448   0.6565    \nfgraze2       0.52851    3.25221   0.163   0.8716    \nfgraze3       0.06601    2.95871   0.022   0.9823    \nfgraze4      -1.24877    3.19838  -0.390   0.6980    \nfgraze5     -12.47309    4.77827  -2.610   0.0122 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.105 on 46 degrees of freedom\nMultiple R-squared:  0.7295,    Adjusted R-squared:  0.6766 \nF-statistic: 13.78 on 9 and 46 DF,  p-value: 2.115e-10\n\n\nLa première partie de la sortie indique le modèle appliqué et quelques informations de base sur les résidus.\nLa partie située sous «Coefficients» donne les paramètres de régression estimés, les erreurs standard, les \\(t\\)-values et les \\(p\\)-values. La seule partie déroutante de ce résultat est peut-être l’absence de graze niveau 1. Il est utilisé comme base de référence. Ainsi, une parcelle ayant le niveau 2 de graze a 0,52 oiseaux (densité) de plus qu’une parcelle ayant le niveau 1, et une parcelle ayant le niveau 5 de graze a 12,4 oiseaux de moins qu’une parcelle ayant le niveau 1. Les \\(p\\)-values correspondantes indiquent si une parcelle est significativement différente du niveau 1. Notez que vous ne devez pas évaluer l’importance d’un facteur en fonction des \\(p\\)-values individuelles. Nous proposerons une meilleure méthode à cet effet dans un instant. Vous ne devez pas laisser de côté les niveaux individuels d’une variable nominale. Ils sont tous pris en compte ou vous laissez tomber la variable entière. La dernière partie du code donne le \\(R^2\\) et le \\(R^2\\) ajusté (pour la sélection du modèle). Le reste de la sortie devrait, je l’espère, vous être familier.\n\n\n4.3.1.3 Commande drop1\nLa fonction drop1 fais exactement ce que son nom indique, elle élimine une variable, chacune à son tour et reévalue le modèle:\n\ndrop1(model1, test=\"F\")\n\nSingle term deletions\n\nModel:\nabundance ~ larea + ldist + lldist + year + altitude + fgraze\n         Df Sum of Sq    RSS    AIC F value   Pr(&gt;F)    \n&lt;none&gt;                1714.4 211.60                     \nlarea     1    770.01 2484.4 230.38 20.6603 3.97e-05 ***\nldist     1      0.55 1715.0 209.62  0.0147  0.90411    \nlldist    1      5.19 1719.6 209.77  0.1392  0.71075    \nyear      1      1.81 1716.2 209.66  0.0485  0.82675    \naltitude  1      7.47 1721.9 209.85  0.2004  0.65650    \nfgraze    4    413.50 2127.9 215.70  2.7736  0.03799 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLe modèle complet a une somme des carrés de 1714,4. Chaque fois, \\(un\\) terme est supprimé à tour de rôle et la somme des carrés résiduelle est calculée à chaque fois. Celles-ci sont ensuite utilisées pour calculer une statistique \\(F\\) et une \\(p\\)-value correspondante. Par exemple, pour obtenir le résultat de la première ligne, R ajuste deux modèles. Le premier modèle contient toutes les variables explicatives et le second modèle toutes, sauf \\(larea\\). Il utilise ensuite les sommes des carrés résiduels de chaque modèle dans la statistique \\(F\\) suivante: \\[ F=\\frac{(SCR_1 - SCR_2)/(p-q)}{SCR_2/(n-p)} \\]\nLes termes \\(SCR_1\\) et \\(SCR_2\\) sont les sommes des carrés résiduels du modèle model1 et du modèle \\(model2\\) , respectivement, et \\(n\\) est le nombre d’observations. Le nombre de paramètres dans les modèles 2 et 1 est respectivement \\(p\\) et \\(q (p &gt; q)\\). Les modèles sont imbriqués dans le sens où un modèle est obtenu à partir d’un autre en fixant certains paramètres à 0. L’hypothèse nulle sous-jacente à cette statistique est que les paramètres omis sont égaux à 0 : \\(H_0: \\beta = 0\\). Plus la valeur de la statistique \\(F\\) est élevée, plus il y a de preuves pour rejeter cette hypothèse. En fait, la statistique \\(F\\) suit une distribution \\(F\\), en supposant l’homogénéité, la normalité, l’indépendance et l’absence de modèles résiduels. Dans ce cas, nous pouvons rejeter l’hypothèse nulle.\nDans la régression linéaire, les \\(p\\)-values de la fonction drop1 sont les mêmes que celles obtenues par la statistique \\(t\\) de la commande summary, mais pour les GLM non gaussiens, ce n’est pas nécessairement le cas. L’hypothèse nulle sous-jacente à la statistique \\(F\\) est que le paramètre de régression du terme qui a été abandonné est égal à 0. Fondamentalement, nous comparons un modèle complet et (à plusieurs reprises) un modèle imbriqué.\nSi le modèle comporte plusieurs variables nominales, la fonction drop1 donne une \\(p\\)-value pour chaque variable, ce qui est pratique.\n\n\n4.3.1.4 Commande anova\nLa commande anova donne le résultat suivant.\n\nanova(model1)\n\nAnalysis of Variance Table\n\nResponse: abundance\n          Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nlarea      1 3471.0  3471.0 93.1303 1.247e-12 ***\nldist      1   65.5    65.5  1.7568  0.191565    \nlldist     1  136.5   136.5  3.6630  0.061868 .  \nyear       1  458.8   458.8 12.3109  0.001019 ** \naltitude   1   78.2    78.2  2.0979  0.154281    \nfgraze     4  413.5   103.4  2.7736  0.037992 *  \nResiduals 46 1714.4    37.3                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nR utilise le carré moyen du modèle complet (37,3) et le carré moyen de chaque ligne dans un test \\(F\\) similaire au précédent. Ainsi, 93,13 est obtenu en divisant 3471,0 par 37,3, et 1,75 est égal à 65,5/37,3. Les carrés moyens sont calculés à partir de la somme des carrés divisée par les degrés de liberté. La somme des carrés sur la première ligne, 3471,0, est la somme des carrés de régression du modèle \\(abundance_i = \\alpha + \\beta \\times larea + \\epsilon_i\\). Les 65,5 de la deuxième ligne représentent la diminution de la somme des carrés résiduels si \\(ldist\\) est ajouté à ce modèle (pour le voir, ajustez un modèle avec seulement l’ordonnée à l’origine et \\(larea\\), et un modèle avec l’ordonnée à l’origine, \\(larea\\), et \\(ldist\\) et comparez les deux sommes des carrés résiduels obtenus à partir des commandes anova ; la différence sera de 65,5). L’avantage de cette approche est que la dernière ligne nous donne une \\(p\\)-value pour la variable nominale graze (puisqu’il s’agit de la dernière variable ajoutée), et nous en avons besoin pour déterminer si graze est significatif. L’inconvénient de cette méthode de test est que les \\(p\\)-value dépendent de l’ordre des variables: Changez l’ordre et vous obtiendrez une conclusion différente. Notez que la dernière ligne de la commande anova et la ligne drop1 sont identiques. Cela s’explique par le fait que les mêmes modèles imbriqués sont comparés. La fonction anova peut également être utilisée pour comparer des modèles imbriqués. Supposons que nous ajustions un modèle linéaire avec toutes les variables explicatives, et un modèle avec toutes les variables explicatives, à l’exception de graze. Ces modèles sont imbriqués car le second modèle est un cas particulier du premier, en supposant que les quatre paramètres de régression pour les niveaux de graze sont égaux à zéro (voir ci-dessous).\n\nmodel2 &lt;- lm(abundance ~ larea + ldist + lldist + year + altitude, data = loyn)\nanova(model1, model2)\n\nAnalysis of Variance Table\n\nModel 1: abundance ~ larea + ldist + lldist + year + altitude + fgraze\nModel 2: abundance ~ larea + ldist + lldist + year + altitude\n  Res.Df    RSS Df Sum of Sq      F  Pr(&gt;F)  \n1     46 1714.4                              \n2     50 2127.9 -4    -413.5 2.7736 0.03799 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nL’hypothèse nulle sous-jacente à la statistique \\(F\\) est que les quatre paramètres de régression pour \\(graze\\) (niveaux 2-5) sont égaux à 0, ce qui est rejeté à 5%. Notez que la \\(p\\)-value est identique à la \\(p\\)-value obtenue pour le même test avec la commande anova(M1). Alors pourquoi faire la comparaison ? L’avantage de la commande anova(M1, M2) est que nous pouvons contrôler quels termes sont éliminés. Ceci est particulièrement utile dans le cas d’interactions multiples.\n\n\n\n4.3.2 Sélection de modèle\nToutes les variables explicatives ne sont pas significativement différentes de 0, comme le montrent les \\(p\\)-values des statistiques \\(t\\) (commande summary) ou les \\(p\\)-value de la statistique \\(F\\) (commande drop1) présentées ci-dessus. Si l’objectif de l’analyse est de comprendre quelles variables explicatives déterminent l’abondance des oiseaux, nous pourrions alors décider d’abandonner les variables explicatives qui ne sont pas significatives. Il s’agit là encore d’un sujet sur lequel les statisticiens ne sont pas d’accord. Il existe essentiellement trois approches principales :\n\nAbandonner les variables explicatives une à une sur la base de procédures de test d’hypothèses.\nAbandonner les variables explicatives une à une (et à chaque fois refaire le modèle) et utiliser un critère de sélection de modèle comme l’AIC ou le BIC pour décider du modèle optimal.\nSpécifier les modèles choisis a priori et comparer ces modèles entre eux.\n\nLa première approche consiste à éliminer le terme le moins significatif, soit sur la base des valeurs \\(t\\) et \\(p\\)-values obtenues par la commande summary, soit par la commande anova pour comparer les modèles imbriqués s’il y a des variables nominales et/ou des interactions. Dans la deuxième approche, nous utilisons un critère de sélection comme le critère d’information d’Akaike (AIC). Il mesure la qualité de l’ajustement et la complexité du modèle. L’avantage de l’AIC est que R dispose d’outils permettant d’appliquer une sélection automatique vers l’avant (forward selection) ou vers l’arrière (backward selection) sur la base de l’AIC, ce qui facilite la vie! L’inconvénient est que l’AIC peut être conservateur et qu’il peut être nécessaire de procéder à un réglage fin (en utilisant les résultats des tests d’hypothèse de la première approche) une fois que l’AIC a sélectionné un modèle optimal. Une sélection en arrière est appliquée par la commande step(model1), et sa sortie est donnée comme suit:\n\nstep(model1)\n\nStart:  AIC=211.6\nabundance ~ larea + ldist + lldist + year + altitude + fgraze\n\n           Df Sum of Sq    RSS    AIC\n- ldist     1      0.55 1715.0 209.62\n- year      1      1.81 1716.2 209.66\n- lldist    1      5.19 1719.6 209.77\n- altitude  1      7.47 1721.9 209.85\n&lt;none&gt;                  1714.4 211.60\n- fgraze    4    413.50 2127.9 215.70\n- larea     1    770.01 2484.4 230.38\n\nStep:  AIC=209.62\nabundance ~ larea + lldist + year + altitude + fgraze\n\n           Df Sum of Sq    RSS    AIC\n- year      1      1.73 1716.7 207.68\n- altitude  1      7.07 1722.0 207.85\n- lldist    1      8.57 1723.5 207.90\n&lt;none&gt;                  1715.0 209.62\n- fgraze    4    413.28 2128.2 213.71\n- larea     1    769.64 2484.6 228.38\n\nStep:  AIC=207.68\nabundance ~ larea + lldist + altitude + fgraze\n\n           Df Sum of Sq    RSS    AIC\n- lldist    1      8.32 1725.0 205.95\n- altitude  1      9.71 1726.4 205.99\n&lt;none&gt;                  1716.7 207.68\n- fgraze    4    848.77 2565.5 222.18\n- larea     1    790.20 2506.9 226.88\n\nStep:  AIC=205.95\nabundance ~ larea + altitude + fgraze\n\n           Df Sum of Sq    RSS    AIC\n- altitude  1      5.37 1730.4 204.12\n&lt;none&gt;                  1725.0 205.95\n- fgraze    4    914.23 2639.3 221.76\n- larea     1   1130.78 2855.8 232.18\n\nStep:  AIC=204.12\nabundance ~ larea + fgraze\n\n         Df Sum of Sq    RSS    AIC\n&lt;none&gt;                1730.4 204.12\n- fgraze  4    1136.5 2866.9 224.40\n- larea   1    1153.8 2884.2 230.73\n\n\n\nCall:\nlm(formula = abundance ~ larea + fgraze, data = loyn)\n\nCoefficients:\n(Intercept)        larea      fgraze2      fgraze3      fgraze4      fgraze5  \n    15.7164       3.1474       0.3826      -0.1893      -1.5916     -11.8938  \n\n\nLa première partie du code montre que le modèle contenant toutes les variables explicatives a un AIC de 211,6 avant l’élimination de chaque terme. Plus l’AIC est faible, meilleur est le modèle. Par conséquent, nous devrions abandonner ldist. La procédure se poursuit en éliminant year, lldist et altitude.\nÀ ce stade, aucun autre terme n’est supprimé; le modèle avec larea et fgraze a un AIC de 204,12, et la suppression de n’importe lequel de ces termes donne un AIC plus élevé. Cela signifie que le modèle optimal basé sur l’AIC contient fgraze et larea. Vous devriez réappliquer ce modèle et voir si les deux termes sont significatifs. Notez que la commande summary et la commande anova sont toutes deux nécessaires pour cela. Les deux termes sont significatifs au niveau de 5%. Vous pouvez également essayer de voir si l’ajout d’une interaction entre larea et fgraze améliore le modèle. Vous devriez pouvoir obtenir une \\(p\\)-value pour ce terme d’interaction.\n\n\n4.3.3 Exercice\n\nRéevaluez le modèle en prenant en compte uniquement fgraze et larea. Introduisez ensuite des interaction dans le modèle.\n\n\n\n4.3.4 Validation de modèle\n\n4.3.4.1 Conditions de validité du modèle linéaire\n\n4.3.4.1.1 Normalité\nL’hypothèse de normalité signifie que si nous répétons l’échantillonnage plusieurs fois dans les mêmes conditions environnementales, les observations seront normalement distribuées pour chaque valeur de \\(X\\). Plusieurs auteurs affirment que la violation de la normalité n’est pas un problème grave en raison de la théorême de la limite centrale. Certains auteurs affirment même que l’hypothèse de normalité n’est pas du tout nécessaire si la taille de l’échantillon est suffisamment grande. La normalité à chaque valeur \\(X\\) doit être vérifiée en réalisant un histogramme de toutes les observations à cette valeur \\(X\\) particulière. Très souvent, nous ne disposons pas de plusieurs observations (sous-échantillons) pour chaque valeur \\(X\\). Dans ce cas, le mieux que nous puissions faire est de regrouper tous les résidus et de réaliser un histogramme des résidus regroupés; la normalité des résidus regroupés est rassurante, mais elle n’implique pas la normalité des données de la population.\nNous expliquons également comment ne pas vérifier la normalité, car le concept sous-jacent de normalité est très mal compris par de nombreuses personnes. Le modèle de régression linéaire exige la normalité des données, et donc des résidus pour chaque valeur \\(X\\). Les résidus représentent l’information qui reste après avoir éliminé l’effet des variables explicatives. Cependant, les données brutes \\(Y\\) (\\(Y\\) représente la variable réponse) contiennent les effets des variables explicatives. Pour évaluer la normalité des données \\(Y\\), il est donc trompeur de fonder son jugement uniquement sur un histogramme de toutes les données \\(Y\\). La situation est différente si vous disposez d’un grand nombre de réplicats pour chaque valeur \\(X\\).\nEn résumé, à moins que vous ne disposiez d’observations répétées pour chaque valeur \\(X\\), vous ne devez pas fonder votre jugement de normalité des données sur un histogramme des données brutes. Il faudrait plutôt appliquer un modèle et inspecter les résidus.\n\n\n4.3.4.1.2 Hétérogénéité\nIl est possible de s’en sortir avec une petite quantité de non-normalité. Cependant, l’hétérogénéité (violation de l’homogénéité), également appelée hétéroscédasticité, se produit si la dispersion des données n’est pas la même pour chaque valeur \\(X\\). Ceci peut être vérifié en comparant la dispersion des résidus pour les différentes valeurs \\(X\\). Comme dans la sous-section précédente, nous pouvons affirmer que la plupart du temps, nous n’avons pas d’observations multiples pour chaque valeur \\(X\\), du moins pas dans la plupart des études sur le terrain. La seule chose que nous puissions faire est de regrouper tous les résidus et de les comparer aux valeurs ajustées. L’écart devrait être à peu près le même dans toute la gamme des valeurs ajustées.\nIl est possible d’utiliser des tests pour vérifier l’homogénéité des données. Cependant la plupart d’entre eux se basent sur la normalité de ces valeurs (par exemple le test de Barlett). Mon avis personnel est de prioriser les outils graphiques de visualisation des résidus. Il est aussi possible d’effectuer une regression linéaire, même en présence d’une faible hétérogénéité cependant cela n’est pas le cas lorsqu’on observe une forte hétérogénéité.\n\n\n4.3.4.1.3 X fixe\nLe \\(X\\) fixe est une hypothèse impliquant que les variables explicatives sont déterministes. Vous connaissez à l’avance les valeurs de chaque échantillon. C’est par exemple le cas si l’on sélectionne a priori des sites avec une valeur de température prédéfinie ou si l’on choisit la quantité de toxine dans un bassin. Mais si vous allez sur le terrain, que vous prélevez un échantillon au hasard et que vous mesurez ensuite la température ou la concentration de toxines, il s’agit alors d’un processus aléatoire.\nLa violation grave de cette hypothèse entraîne des paramètres de régression biaisés. L’expression «biaisé» signifie que la valeur attendue du paramètre estimé n’est pas égale à la valeur de la population. Heureusement, nous pouvons ignorer le problème si l’erreur dans la détermination de la variable explicative est faible par rapport à l’étendue de la variable explicative. Ainsi, si vous avez 20 échantillons où la température varie entre 15 et 20 degrés Celsius, et que l’erreur de votre thermomètre est de 0,1, tout va bien. Mais, par exemple, dans le cas de la détermination de l’âge des baleines, celà peut être une autre histoire, car l’âge varie de 0 à 40 ans, mais l’erreur sur la lecture de l’âge peut (ou non) être de quelques années. Il existe des solutions élégantes à ce problème telles que le bootstrapping que nous n’aborderons pas ici.\n\n\n4.3.4.1.4 Indépendance\nLa violation de l’indépendance est le problème le plus grave car elle invalide des tests importants tels que le test \\(F\\) et le test \\(t\\). La question clé est donc de savoir comment identifier un manque d’indépendance et comment y remédier. Il y a violation de l’indépendance si la valeur \\(Y\\) à \\(X_i\\) est influencée par d’autres \\(X_i\\) (Quinn et Keough, 2002).\nEn fait, cela peut se produire de deux manières: soit un modèle inapproprié, soit une structure de dépendance due à la nature des données elles-mêmes. Supposons que vous ajustiez une ligne droite à un ensemble de données qui présente un schéma non linéaire clair entre \\(Y\\) et \\(X\\) dans un nuage de points. Si vous tracez les résidus en fonction de \\(X\\), vous verrez un schéma clair dans les résidus: les résidus des échantillons ayant des valeurs \\(X\\) similaires sont tous positifs ou négatifs. Une mauvaise formulation du modèle peut donc entraîner une violation de l’indépendance. La solution consiste à améliorer le modèle ou à le transformer pour «linéariser la relation».\nD’autres causes de violation de l’indépendance sont dues à la nature des données elles-mêmes. Ce que vous mangez maintenant dépend de ce que vous mangiez il y a une minute. S’il pleut à 100 m dans l’air, il pleuvra aussi à 200 m dans l’air. Si nous avons un grand nombre d’oiseaux à l’instant \\(t\\), il est probable qu’il y avait également un grand nombre d’oiseaux à l’instant \\(t - 1\\). Il en va de même pour des emplacements spatiaux proches les uns des autres. Ce type de violation de l’indépendance peut être résolu en incorporant une structure de dépendance temporelle ou spatiale entre les observations (ou les résidus) dans le modèle.\n\n\n\n4.3.4.2 Evaluation du modèle précédent\nUne fois que le modèle optimal a été trouvé, il est temps d’appliquer une validation du modèle. Ce processus comprend (au minimum) les étapes suivantes:\n\nTracer les résidus (standardisés) par rapport aux valeurs ajustées pour évaluer l’homogénéité.\nFaites un histogramme des résidus pour vérifier la normalité. Vous pouvez également utiliser un QQ-plot.\nTracez les résidus en fonction de chaque variable explicative utilisée dans le modèle. Si vous observez un schema (structure non aléatoire), vous ne respectez pas l’hypothèse d’indépendance.\nReprésentez les résidus par rapport à chaque variable explicative non utilisée dans le modèle. Si vous observez un schema, incluez la variable explicative omise et relancez le modèle. Si les motifs de résidus disparaissent, incluez le terme, même s’il n’est pas significatif.\nÉvaluer le modèle en fonction des observations influentes. La fonction de distance de Cook est un outil utile.\n\nVoici quelques problèmes courants et leurs solutions:\n1. Il y a une violation de l’homogénéité indiquée par les résidus par rapport aux valeurs ajustées. Cependant, les résidus tracés en fonction des variables explicatives ne présentent pas de schema aléatoire clair. Les solutions possibles sont une transformation de la variable réponse, l’ajout d’interactions ou l’utilisation d’une modélisation linéaire généralisée avec une distribution de Poisson (si les données sont des comptages).\n2. Il y a une violation de l’homogénéité et les résidus tracés en fonction des variables explicatives présentent un schéma aléatoire clair. Les solutions possibles sont les suivantes. Ajoutez des interactions ou des termes non linéaires de la variable explicative (par exemple, des termes quadratiques). Vous pouvez également envisager une modélisation additive généralisée.\n3. Il n’y a pas de violation de l’homogénéité, mais il existe des schémas clairs dans les résidus tracés en fonction des variables explicatives. Solutions possibles : Envisager une transformation des variables explicatives ou appliquer une modélisation additive.\n\nmodel3 &lt;- lm(abundance ~ larea + fgraze, data = loyn)\nop &lt;- par(mfrow = c(2,2))\nE &lt;- rstandard(model3)\nhist(E)\nqqnorm(E)\nqqline(E)\nplot(y = E, x = loyn$larea, xlab = \"area\", ylab=\"residus\")\nabline(0,0)\nplot(E ~ loyn$fgraze, xlab=\"graze\", ylab=\"residus\")\nabline(0,0)\n\n\n\n\nil existe des preuves d’hétérogénéité (comme le montre le graphique des résidus par rapport aux valeurs ajustées) et de non-normalité. Il semble qu’il y ait moins de dispersion sur les sites dont le niveau de graze est de 5. Sur la base du graphique qui montre les résidus par rapport à larea, il semble que nous ayons une certaine violation de l’indépendance.\n\n\n\n4.3.5 Visualisation du modèle\nIl est parfois utile d’inclure une représentation graphique de votre modèle; un graphique peut en dire plus que plusieurs lignes de texte. C’est pourquoi nous avons donné un graphique qui montre ce que l’on peut attendre d’un modèle. Les abondances observées sont représentées en fonction de larea. Chaque ligne représente un niveau de pâturage et la ligne la plus basse correspond au niveau 5. Il est également possible d’utiliser des couleurs différentes pour les lignes ou des symboles différents pour les points.\n\nd1 &lt;- data.frame(larea=loyn$larea[loyn$graze==1], fgraze=\"1\")\nd2 &lt;- data.frame(larea=loyn$larea[loyn$graze==2], fgraze=\"2\")\nd3 &lt;- data.frame(larea=loyn$larea[loyn$graze==3], fgraze=\"3\")\nd4 &lt;- data.frame(larea=loyn$larea[loyn$graze==4], fgraze=\"4\")\nd5 &lt;- data.frame(larea=loyn$larea[loyn$graze==5], fgraze=\"5\")\np1 &lt;- predict(model3, newdata=d1)\np2 &lt;- predict(model3, newdata=d2)\np3 &lt;- predict(model3, newdata=d3)\np4 &lt;- predict(model3, newdata=d4)\np5 &lt;- predict(model3, newdata=d5)\nplot(loyn$larea, loyn$abundance)\nlines(d1$larea, p1, lty=1)\nlines(d2$larea, p2, lty=2)\nlines(d3$larea, p3, lty=3)\nlines(d4$larea, p4, lty=4)\nlines(d5$larea, p5, lty=5)\n\n\n\n\nLes lignes de la figure sont parallèles parce qu’il n’y a pas de terme d’interaction entre larea et fgraze dans le modèle. Si un terme d’interaction était significatif, les lignes auraient des pentes différentes."
  },
  {
    "objectID": "additive.html",
    "href": "additive.html",
    "title": "5  Modélisation additive",
    "section": "",
    "text": "Le nuage de points de abundance en fonction de larea est présenté ci-dessous:\n\nplot(loyn$larea, loyn$abundance)\n\n\n\n\nLes résidus en fonction de larea (voir chapitre précédent) et l’ajustement des lignes dans la suggèrent tous que l’imposition d’un effet linéaire de larea peut être incorrecte. D’un point de vue biologique, il est également plus logique de supposer que plus les parcelles forestières sont grandes, plus le nombre d’oiseaux est élevé, mais seulement jusqu’à un certain niveau. Un modèle additif généralisé (GAM) est une méthode qui peut être utilisée pour vérifier le type de modèle requis. Si le GAM indique que la fonction de lissage est une ligne droite, nous savons que le modèle de régression linéaire est correct.\nNous utiliserons un GAM avec une distribution gaussienne et appliquerons le modèle suivant: \\(abundance_i = \\alpha +f_1(larea_i) + f_2(ldist_i) + f_3(lldist_i) + f_4(year_i) + f_5(altitude_i) + factor(graze_i) + \\epsilon_i\\)\nPar défaut, les fonctions de lissage \\(f_j\\) sont estimées par une spline de régression à plaques minces, mais il existe diverses alternatives comme les splines de régression cubiques ; voir le fichier d’aide ?s. Il n’est pas essentiel de connaître la différence entre tous ces lisseurs, mais cela devient un problème pour les très grands ensembles de données.\n\nlibrary(mgcv)\namodel1 &lt;- gam(abundance ~ s(larea) + s(ldist) + s(lldist) + s(year) + s(altitude) + fgraze, data = loyn)\n\nNous avons délibérément commencé par un modèle qui contient toutes les variables explicatives et non le sous-ensemble de variables explicatives (larea et graze) qui ont été sélectionnées dans le modèle de régression linéaire optimal. La raison en est que certaines variables peuvent avoir un effet non linéaire, ce qui peut les rendre non significatives dans un modèle de régression linéaire. Toutefois, si notre question est : «L’effet de larea dans le modèle de régression linéaire optimal est-il vraiment linéaire ?» par rapport à «Quel est le modèle optimal ?», nous pourrions comparer le modèle de régression linéaire optimal contenant uniquement larea et graze avec un modèle GAM qui contient uniquement une fonction de lissage de larea et graze (en tant que variable nominale).\nLa commande anova n’applique pas un test F séquentiel comme elle l’a fait pour le modèle de régression linéaire. Au lieu de cela, elle donne le test de Wald (approximatif !) qui montre la signification de chaque terme dans le modèle. Son résultat est le suivant:\n\nanova(amodel1)\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\nabundance ~ s(larea) + s(ldist) + s(lldist) + s(year) + s(altitude) + \n    fgraze\n\nParametric Terms:\n       df     F p-value\nfgraze  4 3.281  0.0202\n\nApproximate significance of smooth terms:\n              edf Ref.df     F  p-value\ns(larea)    3.030  3.650 8.709 7.16e-05\ns(ldist)    2.524  3.160 0.541    0.645\ns(lldist)   1.000  1.000 0.219    0.642\ns(year)     2.842  3.376 1.384    0.281\ns(altitude) 1.000  1.000 0.720    0.401\n\n\nLa commande summary donne les valeurs estimées des paramètres de régression pour chaque niveau. Notez que les différents lisseurs ne sont pas significatifs au niveau de 5%. Cela signifie que nous revenons au processus de sélection des données. Là encore, il existe plusieurs approches, voir également la section sur la régression linéaire ci-dessus. Nous pouvons soit comparer des modèles sélectionnés a priori (non abordés ici), soit utiliser des procédures de test d’hypothèse ou un outil de sélection de modèle tel que l’AIC. Dans ce cas, il existe une autre option, que nous mentionnons à la fin de cette section.\nL’approche par test d’hypothèse est la plus simple: il suffit d’éliminer du modèle le terme le moins significatif, de réajuster le modèle et de répéter ce processus jusqu’à ce que tous les termes soient significatifs. Il s’agit d’une approche un peu rapide et sale, mais elle est utile si le temps de calcul est long. Vous pouvez également utiliser l’AIC obtenu par la commande AIC(AM1), mais dans gam il n’y a pas d’étape de fonction qui fera le travail pour vous ; vous devez supprimer chaque terme à tour de rôle, écrire l’AIC, et choisir la variable à supprimer du modèle, et répéter ce processus un certain nombre de fois. Ce processus peut prendre beaucoup de temps.\nIl existe une autre option. Le degré optimal de lissage est estimé à l’aide d’une méthode appelée validation croisée (Wood, 2006), où un degré de liberté produit une ligne droite et 10 degrés de liberté une courbe fortement non linéaire. Dans la régression linéaire, un terme non significatif consomme encore un degré de liberté. La fonction gam est capable de produire des lisseurs avec 0 degré de liberté, ce qui élimine la nécessité de réajuster le modèle sans les termes. Elle ne fonctionne qu’avec les splines de régression à plaques minces et les splines de régression cubiques. Le code est le suivant:\n\namodel2 &lt;- gam(abundance ~ s(larea, bs=\"cs\") + s(ldist, bs=\"cs\") + s(lldist, bs=\"cs\") + s(year, bs=\"cs\") + s(altitude, bs=\"cs\") + fgraze, data=loyn)\n\nLa nouveauté est la partie bs=\"cs\". Elle indique à R d’utiliser la spline de régression cubique avec rétrécissement. Encore une fois, il n’est pas très important pour vous de comprendre les différences entre ces différents types de lisseurs. Dans la pratique, ils se ressemblent. Les lisseurs de plaques minces ont tendance à être légèrement plus linéaires.\n\nanova(amodel2)\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\nabundance ~ s(larea, bs = \"cs\") + s(ldist, bs = \"cs\") + s(lldist, \n    bs = \"cs\") + s(year, bs = \"cs\") + s(altitude, bs = \"cs\") + \n    fgraze\n\nParametric Terms:\n       df     F p-value\nfgraze  4 4.091 0.00675\n\nApproximate significance of smooth terms:\n                  edf    Ref.df     F  p-value\ns(larea)    2.369e+00 9.000e+00 4.033 1.41e-06\ns(ldist)    2.993e+00 9.000e+00 0.432    0.270\ns(lldist)   1.759e-07 9.000e+00 0.000    0.762\ns(year)     2.668e+00 9.000e+00 0.450    0.199\ns(altitude) 9.151e-08 9.000e+00 0.000    0.594\n\n\nNotez que les lisseurs pour lldist et altitude ont 0 degré de liberté. Cependant, il y a encore du travail à faire car les lisseurs de ldist et year ne sont pas significatifs au niveau de 5%. Si l’on supprime ces deux variables (une à une), on constate que le modèle optimal ne contient plus qu’un effet larea et un effet graze.\nLe lisseur pour larea de ce modèle est présenté dans la figure suivante:\n\namodel3 &lt;- gam(abundance ~ s(larea, bs=\"cs\") + fgraze, data=loyn)\nplot(amodel3)\n\n\n\n\nLe processus de validation du modèle devrait suivre pratiquement les mêmes étapes que pour la régression linéaire. Les seules différences sont que les résidus sont obtenus par la commande resid (amodel3) et qu’il n’y a pas de fonction qui trace les résidus par rapport aux valeurs ajustées. Vous devez le faire manuellement en utilisant le code suivant:\n\ne.am3 &lt;- resid(amodel3)\nfit.am3 &lt;- fitted(amodel3)\nplot(x=fit.am3, y=e.am3, xlab=\"Valeurs ajustées\", ylab=\"Residus\")\n\n\n\n\nLà encore, il est important de tracer les résidus en fonction de chaque variable explicative! Si l’un de ces graphiques montre un schema, vous devez trouver une solution. La dernière question que nous devons nous poser est de savoir si le GAM était nécessaire. Nous nous sommes retrouvés avec le même ensemble de variables explicatives, et l’on peut imaginer une ligne droite à l’intérieur des intervalles de confiance à 95 % de la figure précédente. Les degrés de liberté estimés à 2,79 indiquent également un effet larea presque linéaire. En fait, nous pouvons tester si le GAM est meilleur que le modèle de régression linéaire car les deux modèles contiennent le même ensemble de variables explicatives.\n\nmodel3 &lt;- lm(abundance ~ larea + fgraze, data=loyn)\namodel3 &lt;- gam(abundance ~ s(larea, bs=\"cs\") + fgraze, data=loyn)\nanova(model3, amodel3, test=\"F\")\n\nAnalysis of Variance Table\n\nModel 1: abundance ~ larea + fgraze\nModel 2: abundance ~ s(larea, bs = \"cs\") + fgraze\n  Res.Df    RSS     Df Sum of Sq      F  Pr(&gt;F)  \n1 50.000 1730.4                                  \n2 48.058 1520.0 1.9415    210.37 3.4258 0.04196 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nL’hypothèse nulle sous-jacente est que les deux modèles sont identiques ou, plus mathématiquement, que le lisseur est une ligne droite (1 df). Dans ce cas, nous pouvons rejeter cette hypothèse nulle car le modèle le plus compliqué est le GAM; il est significativement meilleur au seuil de 5%, même s’il a une \\(p\\)-value peu convaincante de 0,04. Mais nous préférons également le GAM car il ne présente pas de modèles résiduels. Cependant, l’effet non linéaire de larea est principalement dû à deux grands patchs. Il serait utile d’échantillonner davantage de parcelles de ce type à l’avenir."
  },
  {
    "objectID": "glm.html#glm-poisson",
    "href": "glm.html#glm-poisson",
    "title": "7  Modèles linéaires généralisés",
    "section": "7.1 GLM Poisson",
    "text": "7.1 GLM Poisson"
  },
  {
    "objectID": "glm.html#glm-logistique",
    "href": "glm.html#glm-logistique",
    "title": "7  Modèles linéaires généralisés",
    "section": "7.2 GLM logistique",
    "text": "7.2 GLM logistique"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Zuur, Alain F, Elena N Ieno, Graham M Smith, et al. 2007. Analysing\nEcological Data. Vol. 680. Springer."
  },
  {
    "objectID": "exponential.html#introduction",
    "href": "exponential.html#introduction",
    "title": "6  La famille des lois exponentielles",
    "section": "6.1 Introduction",
    "text": "6.1 Introduction\nDans les chapitres précédents, la régression linéaire et la modélisation additive ont été abordées. Dans les chapitres suivants traitent des techniques de modélisation linéaire généralisée (GLM) et de modélisation additive généralisée (GAM). Dans la régression linéaire et la modélisation additive, nous utilisons la distribution normale (ou gaussienne). Il est important de comprendre que cette distribution s’applique à la variable réponse. La GLM et la GAM sont des extensions de la modélisation linéaire et additive en ce sens qu’une distribution non gaussienne est utilisée pour la variable réponse et que la relation (ou le lien) entre la variable réponse et les variables explicatives peut être différente. Dans ce chapitre, nous nous concentrons sur le premier point, la distribution.\nDe nombreuses raisons justifient l’utilisation de la GLM et de la GAM au lieu de la régression linéaire et de la modélisation additive. Les données d’absence-présence sont (généralement) codées comme 1 et 0, les données proportionnelles sont toujours comprises entre 0 et 100 % et les données de comptage sont toujours non négatives. Les modèles GLM et GAM utilisés pour les données 0-1 et proportionnelles sont généralement basés sur les distributions de Bernoulli et binomiale et, pour les données de comptage, les distributions de Poisson et binomiale négative sont des options courantes. Pour les données continues, la distribution gaussienne est la plus utilisée, mais vous pouvez également utiliser la distribution gamma. Ainsi, avant d’utiliser les GLM et les GAM, nous devons nous concentrer sur les questions suivantes: Que sont ces distributions, à quoi ressemblent-elles et quand les utiliser ? Ces trois questions constituent la base de ce chapitre. Nous consacrons un chapitre entier à ce sujet car, d’après notre expérience, peu de nos étudiants sont familiarisés avec les distributions de Poisson, binomiale négative ou gamma, et un certain niveau de familiarité est nécessaire avant d’entrer dans le monde des GLM et des GAM dans le chapitre suivant.\nComme nous le verrons dans le chapitre suivant, un GLM (ou GAM) se compose de trois étapes : (i) le choix d’une distribution pour la variable réponse, (ii) la définition de la partie systématique en termes de covariables, et (iii) la spécification de la relation (ou : lien) entre la valeur attendue de la variable réponse et la partie systématique. Cela signifie que nous devons nous arrêter un instant et réfléchir à la nature de la variable réponse."
  },
  {
    "objectID": "exponential.html#la-loi-normale",
    "href": "exponential.html#la-loi-normale",
    "title": "6  La famille des lois exponentielles",
    "section": "6.2 La loi normale",
    "text": "6.2 La loi normale"
  },
  {
    "objectID": "exponential.html#la-loi-de-poisson",
    "href": "exponential.html#la-loi-de-poisson",
    "title": "6  La famille des lois exponentielles",
    "section": "6.3 La loi de Poisson",
    "text": "6.3 La loi de Poisson\nLa loi de Poisson (Poisson, 1837) est une loi de probabilité discrète qui décrit le comportement du nombre d’événements se produisant dans un intervalle de temps fixé, si ces événements se produisent avec une fréquence moyenne ou espérance connue, et indépendamment du temps écoulé depuis l’événement précédent.\nSoit \\(\\lambda &gt; 0\\), on dit qu’une variable aléatoire \\(X\\) suit la loi de Poisson de paramètre \\(\\lambda\\), ce que l’on note \\(X \\hookrightarrow \\mathcal{P}(\\lambda)\\) si \\(X(\\Omega) = \\mathbb{N}\\)"
  },
  {
    "objectID": "exponential.html#la-loi-binomial-négative",
    "href": "exponential.html#la-loi-binomial-négative",
    "title": "6  La famille des lois exponentielles",
    "section": "6.4 La loi binomial négative",
    "text": "6.4 La loi binomial négative"
  },
  {
    "objectID": "exponential.html#la-loi-de-bernoulli-et-la-loi-binomiale",
    "href": "exponential.html#la-loi-de-bernoulli-et-la-loi-binomiale",
    "title": "6  La famille des lois exponentielles",
    "section": "6.5 La loi de Bernoulli et la loi binomiale",
    "text": "6.5 La loi de Bernoulli et la loi binomiale\nLes deux dernières distributions que nous examinons sont la distribution de Bernoulli et la distribution binomiale, et nous commençons par cette dernière. Dans un cours de statistique de première année, elle est souvent présentée comme la distribution utilisée pour étudier le lancer d’une pièce de monnaie. Supposons que vous sachiez qu’une pièce est équilibrée (personne ne l’a manipulée et la probabilité d’obtenir un face est la même que celle d’obtenir la pile), et que vous la lanciez 20 fois. La question est de savoir combien de faces vous attendez? Les valeurs possibles sont comprises entre 0 et 20. Évidemment, la valeur la plus probable est 10 faces. En utilisant la distribution binomiale, nous pouvons dire quelle est la probabilité que vous obteniez 0, 1, 2, . . ., 19 ou 20 faces.\nUne distribution binomiale est définie comme suit. Nous disposons de N essais indépendants et identiques, chacun ayant une probabilité \\(P(Y_i = 1) = \\pi\\) de succès et une probabilité \\(P(Y_i = 0) = 1 - \\pi\\) d’échec. Les étiquettes «succès» et «échec» sont utilisées pour les résultats 1 et 0 de l’expérience. Le terme «succès» peut être assimilé à \\(P(Y_i = face)\\) et le terme «échec» à \\(P(Y_i = pile)\\). Le terme «indépendant» signifie que tous les lancers ne sont pas liés. Le terme identique signifie que chaque lancer a la même probabilité de réussite. Sous ces hypothèses, la fonction de densité est donnée par \\[f(y; \\pi) = \\binom{N}{y} \\times \\pi^y \\times (1 - \\pi)^{N-y}\\]\nLa probabilité pour chaque valeur de \\(y\\) entre 0 et 20 pour l’exemple du lancer peut être calculée avec cette fonction de probabilité. Par exemple, si \\(N = 20\\) et \\(π = 0,5\\), la probabilité de mesurer 9 faces est de \\((20!/(9! × 11!)) × 0.5^9 × (1 - 0.5)^{11}\\). Comme prévu, la valeur \\(y = 10\\) a la probabilité la plus élevée, mais 9 et 11 ont des probabilités très similaires.\nL’espérance et la variance de distribution binomiale est donnée par: \\[E(Y) = N \\times \\pi\\] et \\[var(Y) = N \\times \\pi \\times (1 - \\pi)\\]\nEn biologie, on pourrait prendre l’exemple d’un un élevage de poulets et prélevons des échantillons de \\(N\\) animaux pour détecter la présence ou l’absence d’une maladie particulière. Dans ce type de recherche, on veut connaître la probabilité \\(π\\) qu’un animal donné soit infecté par la maladie."
  }
]