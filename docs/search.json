[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Modélisation des données biologiques avec R",
    "section": "",
    "text": "Bienvenue\nCe livre est le manuel compagnon de la formation modélisation de données biologiques organisée par le service innovation et transfert de technologie des fablab agritech à destination des élèves ingénieurs agronomes ou biologiques de l’Institut National Polytechnique Félix Houphouët-Boigny.\nLa formation se déroule typiquement sur cinq jours et réuni autour de 10 étudiants.\nCe manuel est et sera toujours gratuit, sous licence CC BY-NC-ND 3.0."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "Ce que vous allez apprendre\nAu cours de cette formation nous aborderons les concepts suivants:\nCe livre est un manuel de formation à destination des ingénieurs en sciences agronomiques ou biologiques en fin de cycle (3e année).\nIl se veut pratique et facile à aborder tout en n’hésitant pas à revenir sur les concepts mathématiques avancés pour solidifier les fondements mathématiques des participants.\nCe manuel est utilisé lors de la formation organisée par le biais du service innovation et transfert de technologie des fablabs agritech de l’Institut National Polytechnique Félix Houphouët-Boigny de Côte d’Ivoire.\nCette formation n’est pas une formation à l’utilisation de R. Nous n’aborderons donc pas les notions basiques de son utilisation. Nous ferons cependant l’effort d’apporter de l’aide ou des informations pour chaque fonction utilisée. Nous n’aborderont pas aussi les thèmes des tests d’hypothèses, de la conception d’expérience, de la manipulation des données, de la visualisation des données ou de la programmation littérale avec R.\nCes thèmes sont abordés dans d’autres formations que nous organisons.\nNous avons choisi de faire cette formation en utilisant les fonction R basiques et non sous l’aspect de la philosophie tidyverse (pour laquelle nous consacrons une autre formation). Ce choix est fait pour garder l’attention des étudiants dirigées exclusivement sur la compréhension des concepts statistiques présentés et à leur mise en pratique avec R.\nNous avons fait quelques suppositions sur ce que vous savez déjà pour tirer le meilleur parti de cette formation. Vous devez avoir des connaissances générales en calcul et il est utile que vous ayez déjà une certaine expérience de la programmation de base. Si vous n’avez jamais programmé auparavant, Hands on Programming with R pourrait être un outil précieux.\nVous avez besoin de quatre éléments pour exécuter le code de ce livre : R, RStudio, un certains nombre de jeu de données. Les packages sont les unités fondamentales du code R reproductible. Ils comprennent des fonctions réutilisables, de la documentation décrivant comment les utiliser et des exemples de données."
  },
  {
    "objectID": "intro.html#r",
    "href": "intro.html#r",
    "title": "Introduction",
    "section": "R",
    "text": "R\nPour télécharger R, rendez-vous sur CRAN, le réseau complet d’archives R, https://cloud.r-project.org. Une nouvelle version majeure de R est publiée une fois par an, et il y a 2 à 3 versions mineures par an. C’est une bonne idée de faire des mises à jour régulièrement. La mise à jour peut être un peu fastidieuse, en particulier pour les versions majeures qui vous obligent à réinstaller tous vos paquets, mais la remettre à plus tard ne fait qu’empirer les choses. Nous recommandons R 4.2.0 ou une version ultérieure pour cette formation."
  },
  {
    "objectID": "intro.html#rstudio",
    "href": "intro.html#rstudio",
    "title": "Introduction",
    "section": "RStudio",
    "text": "RStudio\nRStudio est un environnement de développement intégré (IDE) pour la programmation R, que vous pouvez télécharger à partir de https://posit.co/download/rstudio-desktop/. RStudio est mis à jour plusieurs fois par an et vous informe automatiquement de la sortie d’une nouvelle version. C’est une bonne idée de faire des mises à jour régulières pour profiter des dernières et meilleures fonctionnalités. Pour ce livre, assurez-vous d’avoir au moins RStudio 2022.02.0.\nLorsque vous démarrez RStudio , vous voyez deux zones clés de l’interface : le volet de console et le volet de sortie. Pour une exécution du code ligne par ligne, il faut taper le code R dans le volet de la console et appuyer sur la touche Entrée pour l’exécuter. Cependant si l’on veut créer un fichier pour y saisir le code il est possible d’utiliser l’éditeur de texte incorporé."
  },
  {
    "objectID": "datasets.html#le-jeu-de-données-penguins",
    "href": "datasets.html#le-jeu-de-données-penguins",
    "title": "1  Présentation des jeux de données",
    "section": "1.1 Le jeu de données penguins",
    "text": "1.1 Le jeu de données penguins\n\n1.1.1 A propos\n\n\n\nFigure 1.1: Palmer penguins hex sticker (Artwork by allison_horst)\n\n\nLe jeu de données Penguins est un jeu de données collectées et mises à disposition par le Dr. Kristen Gorman et la station Palmer, Antarctica LTER, membre du Long Term Ecological Research Network (réseau de recherche écologique à long terme) et mise à disposition de la communauté R au travers du package palmerpenguins.\nLe jeu de données s’appelle penguins, mais fait références en français à des manchots et non à des pingouins. Pour rappel, il y a deux différences fondamentales entre les pingouins et les manchots: leur répartition géographique et leur (in)capacité à voler. Les pingouins vivent dans l’hémisphère nord et ils peuvent voler! Quant aux manchots, ils ne peuvent pas voler et ils vivent dans l’hémisphère sud. Cependant, lors de ce atelier nous allons faire reference a ce jeu de données en utilisant le terme penguins pour garder le nom original du jeu de données.\nLe jeu de données contient des données de 344 manchots. Il y a 3 espèces différentes de manchots dans ce jeu de données Figure 1.2, collectées sur 3 îles de l’archipel de Palmer, en Antarctique.\n\n\n\nFigure 1.2: Les espèces de manchots dans palmerpenguins\n\n\n\n\n1.1.2 Installation et description courte\nLe package est disponible sur le CRAN et peut être installé à partir de la console R en utilisant la commande ci-dessous:\n\ninstall.packages(\"palmerpenguins\")\n\nLe jeu de donnée est composé de 344 observations et de 8 variables:\n\nstr(penguins)\n\ntibble [344 × 8] (S3: tbl_df/tbl/data.frame)\n $ species          : Factor w/ 3 levels \"Adelie\",\"Chinstrap\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ island           : Factor w/ 3 levels \"Biscoe\",\"Dream\",..: 3 3 3 3 3 3 3 3 3 3 ...\n $ bill_length_mm   : num [1:344] 39.1 39.5 40.3 NA 36.7 39.3 38.9 39.2 34.1 42 ...\n $ bill_depth_mm    : num [1:344] 18.7 17.4 18 NA 19.3 20.6 17.8 19.6 18.1 20.2 ...\n $ flipper_length_mm: int [1:344] 181 186 195 NA 193 190 181 195 193 190 ...\n $ body_mass_g      : int [1:344] 3750 3800 3250 NA 3450 3650 3625 4675 3475 4250 ...\n $ sex              : Factor w/ 2 levels \"female\",\"male\": 2 1 1 NA 1 2 1 2 NA NA ...\n $ year             : int [1:344] 2007 2007 2007 2007 2007 2007 2007 2007 2007 2007 ...\n\n\nLes différentes variables sont l’espèce, l’île (lieu de collecte des données), la longueur du culmen (mm), la profondeur du culmen (mm), la longueur de la nageoire (mm), le poids (g), le sexe et l’année de l’étude. Le culmen est appelé bill dans le jeu de donnée. En zoologie, le culmen est l’arête dorsale de la mandibule supérieure des oiseaux Figure 1.3.\n\n\n\nFigure 1.3: Description du culmen des manchots\n\n\nPour rappel, la description complète du format du jeu de donnée est disponible directement dans R en utilisant la commande ?penguins."
  },
  {
    "objectID": "datasets.html#le-jeu-de-donnée-rikz",
    "href": "datasets.html#le-jeu-de-donnée-rikz",
    "title": "1  Présentation des jeux de données",
    "section": "1.2 Le jeu de donnée rikz",
    "text": "1.2 Le jeu de donnée rikz\n\n1.2.1 A propos\nLe changement climatique est, sans aucun doute, la menace la plus importante à laquelle est confronté le littoral mondial et a fait l’objet d’un débat intense. Les écosystèmes marins côtiers sont extrêmement vulnérables, car ils constituent les communautés les plus productives et les plus diversifiées de la planète. Les zones côtières ne sont toutefois pas seulement soumises au changement climatique, mais aussi à de nombreuses autres formes d’activités humaines. Parmi celles-ci, les revendications territoriales, la pollution, les activités de loisirs et de dragage ont menacé la plupart des côtes européennes, entraînant dans de nombreux cas la fragmentation et/ou la dégradation de l’habitat intertidal (Raffaelli et Hawkins 1996). Les conséquences de ces changements ont été bien documentées dans un nombre considérable d’études qui se sont penchées sur l’impact et ont fait état d’une diminution de la performance de l’écosystème.\nÀ une échelle plus locale, les Néerlandais ont livré de grandes batailles avec la mer du Nord afin d’étendre leur masse continentale, comme en témoigne la présence de digues et de systèmes sophistiqués de défense côtière. L’effet de l’élévation du niveau de la mer sur l’écologie du système côtier néerlandais constitue un problème sérieux qui ne doit pas être ignoré à court terme.\nL’institut gouvernemental néerlandais RIKZ a donc lancé un projet de recherche sur la relation entre certains aspects abiotiques (par exemple, la composition des sédiments, la pente de la plage) dans la mesure où ils peuvent affecter la faune benthique. Mulder (2000) a décrit les résultats d’une étude pilote portant sur les effets des différences de pente et de granulométrie sur la faune de la zone côtière. En utilisant les données de cette étude pilote et les techniques de conception expérimentale statistique, un plan d’échantillonnage a été développé dans lequel neuf plages ont été choisies en stratifiant les niveaux d’exposition : trois plages avec une exposition élevée, trois plages avec une exposition moyenne et trois plages avec une exposition faible. L’échantillonnage a été réalisé en juin 2002 et cinq stations ont été sélectionnées pour chaque plage. L’effort à chaque station était faible (Van der Meer 1997).\nL’objectif du projet était de trouver des relations entre la macrofaune de la zone intertidale et les variables abiotiques.\n\n\n1.2.2 Acquisition et description\n\n1.2.2.1 Acquisition\nLes données sont téléchargeable en utilisant ce lien.\n\n\n1.2.2.2 Description\nLa variable NAP est la hauteur de la station d’échantillonnage par rapport au niveau moyen de la marée. L’exposition est un indice composé des éléments suivants : l’action des vagues, la longueur de la zone de surf, la pente, la granulométrie et la profondeur de la couche anaérobie. L’humus est la quantité de matière organique. L’échantillonnage a eu lieu en juin 2002. Une variable nominale «week» a été introduite pour chaque échantillon, avec les valeurs 1, 2, 3 et 4, indiquant la semaine de juin au cours de laquelle la plage a été surveillée. Les règles suivantes ont été utilisées. Échantillonnage entre le 1er et le 7 juin : Semaine,= 1. Échantillonnage entre le 8 et le 14 juin : Semainez =2. Échantillonnage entre le 15 et le 22 juin : Semaine = 3 et échantillonnage entre le 23 et le 29 juin : semaine = 4. L’indice i est l’indice de la station et va de 1 à 45. Il y avait neuf plages, et sur chaque plage, cinq stations ont été échantillonnées (d’où 45 observations). Dix sous-échantillons ont été prélevés par station. L’angle1 représente l’angle de chaque station, tandis que l’angle2 est l’angle de l’ensemble de la zone d’échantillonnage sur la plage."
  },
  {
    "objectID": "organigram.html",
    "href": "organigram.html",
    "title": "2  Organigramme de relation entre les outils d’analyse",
    "section": "",
    "text": "Nous présentons ici un organigramme montrant comment la régression linéaire, la modélisation additive, la modélisation linéaire généralisée (utilisant la distribution de Poisson et la fonction log-link) et la modélisation additive généralisée (utilisant la distribution de Poisson) sont liées les unes aux autres.\n\n\n\nOrganigramme des outils d’analyse\n\n\nDans la régression linéaire, la violation de l’homogénéité signifie que le GLM avec une distribution de Poisson peut être utilisé. La normalité mais les relations non linéaires (détectées par exemple par un graphique des résidus en fonction de chaque variable explicative) signifient que la modélisation additive peut être appliquée. Des relations non linéaires et une violation de l’hypothèse de normalité signifient qu’une GAM avec une distribution de Poisson peut être utilisée. Le graphique changera si une autre fonction de liaison ou une autre distribution est utilisée."
  },
  {
    "objectID": "exploration.html#outils-graphiques",
    "href": "exploration.html#outils-graphiques",
    "title": "3  Exploration des données",
    "section": "3.1 Outils graphiques",
    "text": "3.1 Outils graphiques\n\n3.1.1 Boîte à moustache\nUn diagramme en boîte, ou boîte à moustaches, permet de visualiser la moyenne et la dispersion d’une variable univariée. Normalement, le point central d’un diagramme en boîte est la médiane, mais il peut également s’agir de la moyenne. Les quartiles 25% et 75% (Q25 et Q75) définissent les charnières (extrémités des boîtes), et la différence entre les charnières est appelée l’écart. Des lignes (ou moustaches) sont tracées à partir de chaque charnière jusqu’à 1,5 fois l’écart ou jusqu’à la valeur la plus extrême de l’écart, la plus petite étant retenue. Tous les points situés en dehors de ces valeurs sont normalement identifiés comme des valeurs aberrantes Figure 3.1.\n\n\n\nFigure 3.1: Boîte à moustache\n\n\n\n\n\n\n\n\nLa syntaxe générale pour créer la boîte à moustache d’une variable en utilisant base R est: boxplot(variable).\nL’aide complète de la fonction boxplot() est obtenue par la commande ?boxplot().\n\n\n\nEtudions la variable body_mass_g en représentant sa boîte à moustache.\n\nboxplot(penguins$body_mass_g, xlab=\"Masse (g)\")\n\n\n\n\nOn remarque qu’aucune valeur aberrante n’apparait de façon visible. De plus on observe que la médiane du poids des manchots se situent autour de 4000 g (on peut facilement confirmer cela en calculant la médiane median(penguins$body_mass_g)).\nOn peut alors être amenée à continuer l’exploration en étudiant le poids, cette fois-ci en étudiant la variable par espèce de manchot.\n\nboxplot(body_mass_g ~ species, data=penguins, ylab=\"Masse (g)\", xlab=NULL)\n\n\n\n\nLes boîte à moustaches representées nous permettent de nous rendre compte que l’espèce Gentoo a sensiblement une masse plus élevée que les deux autres. On pourra confirmer si cette différence est significative avec une analyse de variance. De plus, on observe la présence de quelques valeurs aberrantes chez l’espèce Chinstrap.\n\n\n3.1.2 Diagramme de points de Cleveland\nLes diagrammes de points de Cleveland sont utiles pour identifier les valeurs aberrantes et l’homogénéité.\nL’homogénéité signifie que la variance des données ne change pas le long des gradients. La violation de cette condition est appelée hétérogénéité et l’homogénéité est une hypothèse cruciale pour de nombreuses méthodes statistiques.\nLa valeur est présentée sur l’axe horizontal et l’ordre des points (tel qu’il est organisé par le programme) est présenté sur l’axe vertical.\n\n\n\n\n\n\nLa syntaxe pour générer un nuage de points avec base R est dotchart(variable). L’aide est disponible avec la commande ?dotchart().\n\n\n\nRepresentons le nuage de point de Cleveland avec pour but d’identifier une possible violation de l’homogénéité ou la présence de valeurs aberrantes. Le diagrammes à points est réalisé en utilisant différents symboles conditionnels à une variable explicative nominale qui est ici l’espèce.\n\ndotchart(penguins$body_mass_g, main=\"Masse (g)\", pch=as.numeric(penguins$species))\n\n\n\n\nTout point isolé à droite ou à gauche indique des valeurs aberrantes, mais dans cet jeu de données, en considérant l’ensemble du graphique aucun point n’est considéré comme aberrant, ce qui confirme notre observation de la boîte à moustache. Cependant comme observé plus haut, en prenant les points par groupe (chaque symbole représentant une espèce différente) on observe bien un groupe présentant des valeurs aberrantes.\n\n\n3.1.3 Histogramme\nUn histogramme montre le centre et la distribution des données et donne une indication de la normalité. Toutefois, l’application d’une transformation des données pour les faire correspondre à une distribution normale nécessite des précautions.\n\n# Subdivision du panel graphique\nlayout(matrix(c(1, 2, 1, 3), nrow = 2, byrow = TRUE))\n\n# Histrogramme de la masse pour l'ensemble des manchots\nhist(penguins$body_mass_g, main=\"Masse (g)\", xlab=NULL)\n\n# Histrogramme de la masse pour les males\nhist(penguins$body_mass_g[penguins$sex==\"male\"], main=\"Males\", xlab=NULL)\n# Histrogramme de la masse pour les femelles\nhist(penguins$body_mass_g[penguins$sex==\"female\"], main=\"Females\", xlab=NULL)\n\n\n\n\nLa forme de l’histogramme montre une certaine asymétrie et l’on pourrait être tenté d’appliquer une transformation. Cependant, un histogramme conditionnel donne une image assez différente. Dans un histogramme conditionnel, les données sont divisées en fonction d’une variable nominale et les histogrammes des sous-ensembles sont tracés côte-à-côte. A ce moment on obtient une figure tout autre montrant une bimodalité aussi bien chez les males que chez les femelles manchots. On a une différence claire du centre de la distribution et la pic initial de données observé sur le premier histogramme est grandement du aux femelles. Il faudrait donc explorer les effets du sexe sur le poids des manchots ainsi que les interactions avant d’envisager une transformation des données.\n\n\n3.1.4 QQ-plots\nUn graphique Quantile-Quantile (QQ-plots) est un outil graphique utilisé pour déterminer si les données suivent une distribution particulière. Le graphique QQ pour une distribution normale compare la distribution d’une variable donnée à la distribution gaussienne. Si les points obtenus se situent approximativement sur une ligne droite, on considère que la distribution des données est la même que celle d’une variable normalement distribuée.\nLe \\(p\\)-ème quantile \\(q\\) d’une variable aléatoire \\(y\\) est donnée par \\(F(q) = P(y \\leq q) = p\\). Si l’on souhaire savoir quelle valeur de \\(q\\) appartient à \\(p\\), on inverse la formule précédente pour obtenir \\(q = F^{-1}(p)\\). Supposons que nous avons cinq observations \\(Y_i\\) avec les valeurs 1, 2, 3, 4, 5. Par définition, le premier chiffre est le 0% percentile, le milieu est le 50% percentile et 5 est le 100% percentile. La différence entre un quantile et un percentile est un seulement le facteur 100. Les QQ-plots sont soit basés sur les percentiles ou typiquement sur les points quantiles de l’échantillon définis par \\((i-0.5)/n\\) où \\(i\\) varie de 1 à 5 et \\(n=5\\) dans notre example. Ainsi pour notre exemple, les points de quantiles de l’échantillon sont 0,1, 0,3, 0,5, 0,7 et 0,9. Ce sont les valeurs de \\(p\\) pour l’échantillon. Dans la séconde étape, ces quantiles de l’échantillon sont comparés à une distribution normale. Celà signifie que la fonction de densité \\(P(y\\leq q)\\) est désormais une fonction de densité normale et \\(F()\\) est désormais la fonction de répartition de la loi normale.\nLe QQ-plot est donc un graphique des valeurs de \\(Y_i\\) de l’échantillon comparés aux \\(q_i\\). On peut ajouter à ce graphique dans R, une ligne qui connecte les 25e et le 75e quartiles.\nNous appliquons dans le même temps une transformation des données pour visualiser laquelle produit le meilleur ajustement.\nIl est très souvent utilse de combiner les QQ-plots avec des transformations de puissance, qui est donnée par \\[ \\frac{Y^p - 1}{p}, \\forall p \\neq 0; log(Y) , p = 0\\]\nVeuillez noter que le \\(p\\) utilisé ici n’est pas le \\(p\\) utilisé pour décrire les quantiles. Il est aussi utile de comparer plusieurs QQ-plots pour différentes valeurs de \\(p\\).\n\n# Transformation racine carrée\nbmsq &lt;- sqrt(penguins$body_mass_g)\n# Transformation racine quatrième\nbmfq &lt;- penguins$body_mass_g^(0.25)\n# Transformation logarithmique\nbmlog &lt;- log(penguins$body_mass_g)\n\nDans le graphique ci-dessous, aucune transormation semble prendre le dessus sur l’autre.\n\nlayout(matrix(c(1, 2, 3, 4), nrow = 2, byrow = TRUE))\nqqnorm(penguins$body_mass_g, main=\"Aucune transformation (p=0)\")\nqqline(penguins$body_mass_g)\nqqnorm(bmsq, main=\"Racine carré (p=0.5)\")\nqqline(bmsq)\nqqnorm(bmfq, main=\"Racine quatrième (p=0.25)\")\nqqline(bmfq)\nqqnorm(bmlog, main=\"Logarithme (p=1)\")\nqqline(bmlog)\n\n\n\n\n\n\n3.1.5 Nuage de points\nJusqu’à présent, l’accent a été mis sur la détection des valeurs aberrantes, la vérification de la normalité et l’exploration d’ensembles de données associés à des variables explicatives nominales uniques. Les techniques suivantes s’intéressent aux relations entre plusieurs variables. Un nuage de points est un outil permettant de trouver une relation entre deux variables. Il représente une variable sur l’axe horizontal et une seconde variable sur l’axe vertical. Pour aider à visualiser la relation entre les variables, une ligne droite ou une courbe de lissage est souvent ajoutée au graphique.\nLa figure suivante montre le nuage de points entre les variables flipper_length_mm (taille de la nageoire) et body_mass_g (masse du manchot).\n\nplot(penguins$flipper_length_mm, penguins$body_mass_g, xlab=\"Taille des nageoires\", ylab=\"Masse (g)\")\nm1 &lt;- lm(body_mass_g ~ flipper_length_mm, data=penguins)\nabline(m1)\n\n\n\n\n\n\n3.1.6 Pairplots\nSi vous avez plus de deux variables, vous pouvez produire une série de nuage de points : un pour chaque paire de variables. Cependant, le nombre de diagrammes augmente rapidement si vous avez plus de trois variables à explorer. Une meilleure approche, jusqu’à environ 10 variables explicatives, est le diagramme de paires encore appelée matrice de nuage de points. Ces diagrammes présentent plusieurs nuage de points par paire dans un seul graphique et peuvent être utilisés pour détecter les relations entre les variables et pour détecter la colinéarité.\nLa figure suivante montre un pairplot entre les variables body_mass_g, flipper_length_mm, bill_length_mm et bill_depth_mm. Chaque sous-graphique est un nuage de points entre deux variables, avec les etiquettes de chaque variable ajoutée dans la diagonale.\n\n# penguins[3:6] sélectionne les variables bill_length_mm\n# bill_depth_mm, flipper_length_mm, body_mass_g\npairs(penguins[,3:6])\n\n\n\n\nIl est aussi possible d’ajouter une droite ajustée à chaque graphique.\n\npairs(penguins[,3:6], panel=panel.smooth)\n\n\n\n\nOn parle de colinéarité lorsqu’il existe une forte corrélation entre deux (ou plus) variables.\nLa figure ci-dessous montre un autre pairplot du même jeu de donnée sur lequel nous avons, cette fois-ci ajouté les coefficients de corrélation entre les variables dans le triangle bas du pairplot.\nPour cela, nous utiliserons une fonction panel.cor définie comme suit:\n\npanel.cor &lt;- function(x, y, digits=1, prefix=\"\", cex.cor) {\n  usr &lt;- par(\"usr\")\n  on.exit(par(usr))\n  par(usr=c(0, 1, 0, 1))\n  r1 &lt;- cor(x, y, use=\"pairwise.complete.obs\")\n  r &lt;- abs(cor(x, y, use=\"pairwise.complete.obs\"))\n  \n  txt &lt;- format(c(r1, 0.123456789), digits=digits)[1]\n  txt &lt;- paste(prefix, txt, sep=\"\")\n  if (missing(cex.cor)) {\n    cex &lt;- 0.9/strwidth(txt)\n  }\n  text(0.5, 0.5, txt, cex = cex * r)\n}\n\nPar la suite le diagramme est construit en faisant:\n\npairs(penguins[,3:6], lower.panel=panel.cor)\n\n\n\n\nIl faut noter qu’il existe une forte colinéarité entre la taille de la nageoire et la masse du manchot.\nDes diagrammes en paires doivent être établis pour chaque analyse. Ils doivent comprendre (i) un diagramme par paire de toutes les variables de réponse (en supposant que plus d’une variable de réponse soit disponible) ; (ii) un diagramme par paire de toutes les variables explicatives ; et (iii) un diagramme par paire de toutes les variables de réponse et de toutes les variables explicatives.\nLe premier graphique (i) fournit des informations qui aideront à choisir les techniques multivariées les plus appropriées. On espère que les variables de réponse présenteront de fortes relations linéaires (certaines techniques telles que l’ACP dépendent de relations linéaires). Toutefois, si le graphique (ii) montre une relation linéaire claire entre les variables explicatives, indiquant une colinéarité, nous savons que nous avons un problème majeur à résoudre avant de poursuivre l’analyse.\nLe graphique (iii) permet de déterminer si les relations entre les variables de réponse et les variables explicatives sont linéaires. Si ce n’est pas le cas, plusieurs options s’offrent à nous. La plus simple consiste à appliquer une transformation aux variables de réponse et/ou explicatives afin de linéariser les relations.\nD’autres options seront explorée le long du séminaire.\n\n\n3.1.7 Coplot\nUn coplot est un nuage de points conditionnel montrant la relation entre y et x, pour différentes valeurs d’une troisième variable z, voire d’une quatrième variable w. Les variables conditionnelles peuvent être nominales ou continues.\nLa figure suivante présente un coplot entre la masse des manchots et la taille de leur nageoire conditionée par la variable nominale espèce.\nLes panneaux sont classés de la partie inférieure gauche à la partie supérieure droite. Cet ordre correspond à des valeurs croissantes de la variable explicative du conditionnement.\n\ncoplot(body_mass_g ~ flipper_length_mm | species, data = penguins)\n\n\n\n\n\n Missing rows: 4, 272 \n\n\n\n\n3.1.8 Diagrammes de conception et d’interaction\nLes diagrammes de conception et d’interaction sont un autre outil précieux pour explorer les ensembles de données avec des variables nominales et sont particulièrement utiles à utiliser avant d’appliquer la régression, la GLM, la modélisation mixte ou l’analyse de variance.\nIls permettent de visualiser (i) les différences entre les valeurs moyennes de la variable réponse pour différents niveaux de variables nominales et (ii) les interactions entre les variables explicatives.\nLa figure suivante montre un diagramme de conception entre la masse des manchots et les trois variables nominale du jeu de données: species, sex et island. Il nous permet de comparer directement les moyennes (ou médianes) de chaque variable nominale en utilisant un seul graphique.\nLe graphique nous montre que la masse moyenne de l’espèce Gentoo est située autour de 5100g et est sensiblement plus élevée que celle de Chinstrap et Adélie. De même on peut aussi remarquer que la masse moyenne des mâles est plus grande que celle des femelles.\n\nplot.design(body_mass_g ~ species + sex + island, data = penguins)\n\n\n\n\nCependant, les diagrammes de conception ne nous permettent pas d’explorer les interactions entre les variables explicatives, d’où la nécessité d’utiliser les diagrammes d’interactions.\n\n\n\n\n\n\nInterprétation d’un diagramme d’interaction\n\nIdentifier les facteurs et les niveaux: observez l’abscisse pour identifier les niveaux du premier facteur et l’ordonnée pour identifier les niveaux du deuxième facteur.\nÉvaluer le parallélisme des lignes: si les lignes sont parallèles, cela signifie qu’il n’y a pas d’interaction entre les facteurs; l’effet d’un facteur est constant pour tous les niveaux de l’autre facteur. Cependant, si les lignes ne sont pas parallèles (c’est-à-dire qu’elles se croisent ou divergent), cela indique une interaction entre les facteurs. L’effet d’un facteur dépend alors du niveau de l’autre facteur.\nAmpleur de l’interaction: le degré de divergence ou de convergence des lignes peut donner une indication de la force de l’interaction. Une divergence ou une convergence plus importante indique un effet d’interaction plus fort.\nDirection des effets: observez si la variable de réponse augmente ou diminue avec les changements de niveau des facteurs. Cela peut aider à déterminer la nature de l’interaction et des effets principaux.\n\nPour résumer\n\nPas d’interaction: les lignes parallèles suggèrent que les deux facteurs n’interagissent pas. Les effets principaux de chaque facteur sont additifs.\nInteraction présente: les lignes non parallèles indiquent une interaction entre les facteurs, ce qui signifie que l’effet d’un facteur dépend du niveau de l’autre facteur.\nForce et direction: le degré de divergence ou de convergence et la direction des lignes donnent des indications sur la force et la nature de l’interaction.\n\n\n\n\nLe premier diagrame d’interaction ci-dessous montre l’interaction entre l’espèce et le sexe. On observe qu’il n’y a une interaction entre le sexe et la masse des manchots, ce qui signifie que la masse dépend du sexe et de l’espèce de manchot.\n\ninteraction.plot(penguins$species, penguins$sex, penguins$body_mass_g)\n\n\n\n\nLe second diagramme montre l’interaction entre l’île (l’habitat du manchot), le sexe et la masse. On observe qu’il existe aussi une interaction entre ces variables.\n\ninteraction.plot(penguins$sex, penguins$island, penguins$body_mass_g)"
  },
  {
    "objectID": "exploration.html#valeurs-aberrantes-transformations-et-standardisation",
    "href": "exploration.html#valeurs-aberrantes-transformations-et-standardisation",
    "title": "3  Exploration des données",
    "section": "3.2 Valeurs aberrantes, transformations et standardisation",
    "text": "3.2 Valeurs aberrantes, transformations et standardisation\n\n3.2.1 Valeurs aberrantes\nUne valeur aberrante est un point de données qui, en raison de sa valeur extrême par rapport au reste de l’ensemble de données, peut influencer incorrectement une analyse. La première question qui se pose est donc la suivante: “Comment identifier une valeur aberrante?” Une approche simple pourrait consister à quantifier tout ce qui est aberrant au-delà d’une certaine distance par rapport au centre des données. On pourrait par exemple considérer les points situés en dehors des charnières d’un diagramme en boîte comme des valeurs aberrantes. Cependant, si la quantité des données n’est pas suffisante, il pourrait être difficile de considérer ces valeurs comme abberantes.\nLes nuages de points peuvent aussi nous permettre d’identifier les valeurs aberrantes. Ainsi, même si une observation n’est pas considérée comme aberrante dans l’espace x ou dans l’espace y (alors possiblement identifiée par une boîte à moustache), elle peut l’être dans l’espace xy. La situation dans laquelle une observation est une valeur aberrante dans l’espace x, ainsi que dans l’espace y, mais pas dans l’espace xy, est également possible.\nPour résumer la détection des valeurs aberrantes peut rapidemment devenir difficile, cependant l’analyse pourrait permettre d’y voir plus clair.\n\n\n3.2.2 Transformation\n\n3.2.2.1 Quelques transformations usuelles\nIl existe de nombreuses raisons de transformer les données, mais c’est généralement parce que les données présentent des valeurs aberrantes et des distributions non normales. La transformation des données (sur les variables de réponse) sera également nécessaire lorsque vous prévoyez d’utiliser l’analyse discriminante et qu’il existe des preuves évidentes (par exemple, en utilisant un diagramme en pointillés de Cleveland) d’hétérogénéité.\nDe plus, le choix de la transformation est influencée par le choix de l’analyse de suivi. Pour certaines techniques, telles que les arbres de classification ou de régression, la transformation des variables explicatives ne change rien aux résultats. Cependant, la plupart des techniques peuvent nécessiter une certaine transformation des données brutes avant l’analyse.\nLe problème le plus facile à résoudre est celui où les observations extrêmes identifiées au cours de l’étape d’exploration des données s’avèrent être des erreurs de frappe. Nous supposerons toutefois que cette solution facile n’existe pas et que nous disposons d’un ensemble de données contenant de véritables observations extrêmes. Si ces observations extrêmes se trouvent dans les variables explicatives, une transformation des variables explicatives (continues) est certainement nécessaire, en particulier si l’on applique des techniques de régression, d’analyse de la covariance, de GLM, de GAM ou des techniques multivariées telles que l’analyse de la redondance et l’analyse canonique des correspondances.\nLorsque les observations extrêmes se trouvent dans la variable réponse, plusieurs approches sont possibles. Vous pouvez soit transformer les données, soit appliquer une technique légèrement plus performante pour traiter les valeurs extrêmes, comme un GLM ou un GAM avec une distribution de Poisson. Cette dernière méthode ne fonctionne que s’il y a une augmentation de la dispersion des données observées pour des valeurs plus élevées. Il est également possible d’utiliser des modèles de quasi-Poisson si les données sont trop dispersées.\n\n\n\n\n\n\nVous ne devez jamais appliquer une racine carrée ou une transformation logarithmique à la variable réponse, puis continuer avec un modèle GLM de Poisson, car cela applique la correction deux fois.\n\n\n\nUne solution plus radicale pour les observations extrêmes consiste à les omettre purement et simplement de l’analyse. Toutefois, si vous adoptez cette approche, vous devez toujours fournir les résultats de l’analyse avec et sans les observations extrêmes. Si les grandes valeurs proviennent toutes d’une région, d’un mois ou d’un sexe, il est possible d’utiliser différentes composantes de variance dans le modèle de régression linéaire, ce qui permet d’obtenir des moindres carrés généralisés (GLS).\nSupossons par exemple que nous souhaitons faire une regression linéaire. Le diagramme de points de Cleveland ou les boîtes à moustaches nous indiquent qu’il n’y a pas de valeurs aberrantes préoccupantes, mais le nuage de points d’une variable réponse et d’une variable explicative montre une relation non linéaire évidente. Dans ce cas, nous devrions envisager de transformer l’une des variables ou les deux. Mais quelle transformation utiliser ? L’éventail des transformations possibles pour les variables réponse et explicative peut être choisi parmi les suivantes \\[ y^{\\frac{1}{4}}, y^{\\frac{1}{3}}, y^{\\frac{1}{2}}, y, log(y), y^2, y^3, y^4, ..\\]\nCes transformations sont des cas particulier de la transformation de puissance de Box-Cox qui est une famille de transformation qui ne peuvent être appliquée que sur les données non-négatives.\nUne autre alternative est d’utiliser la transformation en rang (méthode utilisée par la plupart des méthodes non paramétrique) ou la transformation en données binaires. Par exemple, en supposant que l’on a la série statistique suivante 2, 7, 4, 9, 22, 40, la transformation en rang va donner 1, 3, 2, 4, 5, 6. Si l’on a la série statistique 0, 1, 3, 0, 4, 0, 100 alors la transformation en série binaire va donner 0, 1, 1, 0, 1, 0, 1.\n\n\n3.2.2.2 Stratégies pour le choix d’une transformation\nIl existe plusieurs stratégies pour le choix de la transformation la plus appropriée. Nous présentons ici deux d’entre elles: essai-erreur et la règle de Mosteller-Tukey.\nLa transformation par essai-erreur est le fait de tester différentes transformation et de visualiser les résultats à l’aide des outils graphiques présentés ci-dessus afin d’effectuer la sélection de la meilleure transformation. En utilisant cette méthode, il es important de reporter les résultats incluant aussi bien les transformations réussies que celles qui ont échouées.\nLorsque l’analyse qui suivra nécessite des relations linéaire entre les variables, la règle de Mosteller-Tukey Figure 3.2 qui appartient à la famille des transformations de Box-Cox.\n\n\n\nFigure 3.2: Mosteller-Tukey bulging rule\n\n\nCette approche est basée sur l’identification des motifs non-linéaires par l’inspection des nuages de points. Les transformations requises peuvent alors être inférée du graphique Figure 3.2.\nPar exemple, si la forme du nuage de points est similaire à la forme du quadrant en bas à gauche de la figure Figure 3.2, alors soit if faudra effectuer une transformation \\(x^2\\) ou \\(x^3\\) de la variable explicative ou effectuer une transformation \\(log(y)\\) ou \\(y^{0.5}\\) de la variable réponse.\n\n\n\n3.2.3 Standardisation\nSi les variables comparées proviennent d’échelles très différentes, comme la comparaison des taux de croissance de petites espèces de poissons avec ceux de grandes espèces de poissons, la standardisation (conversion de toutes les variables à la même échelle) peut être une option. Toutefois, cela dépend de la technique statistique utilisée. Il existe plusieurs méthodes pour convertir les données à la même échelle, et une option consiste à centrer toutes les variables autour de zéro par \\[ Y_i^{new} = Y_i - \\bar{Y} \\] où \\(\\bar{Y}\\) est la moyenne de l’échantillon et \\(Y_i\\) est la valeur de la i-ème observation. Cependant la standardisation la plus utilisée est donnée par \\[ Y_i^{new} = (Y_i - \\bar{Y}) / s_y \\] où \\(s_y\\) est l’écart type de l’échanitllon. Les valeurs obtenues sont alors centrées autour de zéro, avec une variance de 1, et sont sans unités. Cette transformation est aussi appelée normalisation.\nComme pour les autres transformations, la décision de standardiser vos données dépend de la technique statistique que vous envisagez d’utiliser. Par exemple, si vous souhaitez comparer des paramètres de régression, vous pouvez juger utile de normaliser les variables explicatives avant l’analyse, en particulier si elles sont exprimées dans des unités différentes ou si elles ont des intervalles différents. Certaines techniques, telles que l’analyse en composantes principales, normalisent ou centrent automatiquement les variables."
  },
  {
    "objectID": "exploration.html#conclusion",
    "href": "exploration.html#conclusion",
    "title": "3  Exploration des données",
    "section": "3.3 Conclusion",
    "text": "3.3 Conclusion\n\n3.3.1 Même si vous ne les voyez pas, elles peuvent être présentes\nMême si les nuages de points suggèrent l’absence de relation entre Y et X, cela ne signifie pas nécessairement qu’il n’en existe pas. Un nuage de points ne montre que la relation entre deux variables, et l’inclusion d’une troisième, d’une quatrième ou même d’une cinquième variable peut conduire à une conclusion différente.\n\n\n3.3.2 Prochaines étapes\nUne fois l’exploration des données terminée, l’étape suivante consiste à vérifier et à étudier les modèles et les relations que cette étape a permis d’identifier. Si le nuage de points indique une relation linéaire entre les variables, la régression linéaire est l’étape suivante évidente. Toutefois, si le nuage de points suggère un modèle non linéaire clair, une approche différente doit être adoptée, qui peut inclure (i) l’utilisation d’interactions et/ou de termes quadratiques dans le modèle de régression linéaire, (ii) la transformation des données, (iii) la poursuite avec un modèle de régression non linéaire, (iii) l’utilisation d’une modélisation linéaire généralisée, (iv) l’application de techniques de modélisation additive généralisée, ou (v) l’application de techniques de modélisation mixtes (additives). Toutes ces approches sont étudiées dans les chapitres suivants. La première option consiste à utiliser le modèle de régression linéaire, mais il faut s’assurer que toutes les hypothèses sont respectées (par exemple, pas de modèles résiduels). Pour choisir l’approche la plus appropriée, il faut connaître les hypothèses des méthodes sélectionnées et disposer d’outils permettant de détecter les violations (à l’aide des résidus). Tous ces éléments sont abordés dans les chapitres suivants, mais tout se résume à quelque chose de très fondamental : apprenez de vos erreurs."
  },
  {
    "objectID": "regression.html#regression-linéaire-bivariée",
    "href": "regression.html#regression-linéaire-bivariée",
    "title": "4  Regression linéaire",
    "section": "4.1 Regression linéaire bivariée",
    "text": "4.1 Regression linéaire bivariée\nLe jeu de données RIKZ présente l’abondance d’environ 75 espèces d’invertébrés provenant de 45 sites et qui a été mesurée sur différentes plages le long de la côte néerlandaise. Dans cette étude, la variable «NAP» mesure la hauteur du site d’échantillonnage par rapport au niveau moyen de la mer et indique le temps pendant lequel un site est sous l’eau. Un site ayant une faible valeur NAP passera plus de temps sous l’eau qu’un site ayant une valeur NAP élevée, et les sites ayant des valeurs NAP élevées se trouvent normalement plus haut sur la plage. Les marées créent un environnement difficile pour les animaux qui y vivent, et il est raisonnable de supposer que des espèces et des abondances d’espèces différentes seront trouvées sur des plages ayant des valeurs de PNA différentes.\nUn point de départ simple consiste donc à comparer la diversité des espèces (richesse des espèces) avec les valeurs de NAP dans différentes zones de la plage. Bien que les connaissances écologiques suggèrent que la relation entre la richesse en espèces et les valeurs de NAP n’est probablement pas linéaire, nous commençons par un modèle de régression linéaire bivarié utilisant une seule variable explicative. Il est toujours préférable de commencer par un modèle simple et de ne passer à des modèles plus avancés que lorsque cette approche s’avère inadéquate.\nLe modèle de régression linéaire bivarié (c’est-à-dire à deux variables) est donné par la formule suivante:\n\\[ Y_i = \\alpha + X_i\\beta + \\epsilon_i\\]\noù α est l’ordonnée à l’origine, β est la pente et ε est le résidu, ou l’information qui n’est pas expliquée par le modèle. Ce modèle est basé sur l’ensemble de la population, mais comme expliqué ci-dessus, nous ne disposons que d’un échantillon de la population, et nous devons d’une manière ou d’une autre utiliser les données de cet échantillon pour estimer les valeurs de α et de β pour l’ensemble de la population. Pour ce faire, nous devons faire quatre hypothèses sur nos données qui permettront à une procédure mathématique de produire des valeurs estimées pour α et β. Ces estimateurs, appelés α et β, basés sur les données de l’échantillon agissent alors comme des estimateurs pour leurs paramètres de population équivalents, α et β respectivement. Les quatre hypothèses qui permettent d’utiliser les données de l’échantillon pour estimer les données de la population sont (i) la normalité, (ii) l’homogénéité, (iii) l’indépendance et (iv) la fixité de X.\n\n4.1.1 Normalité\nL’hypothèse de normalité signifie que si nous répétons l’échantillonnage plusieurs fois dans les mêmes conditions environnementales, les observations seront normalement distribuées pour chaque valeur de X."
  },
  {
    "objectID": "glm.html#glm-poisson",
    "href": "glm.html#glm-poisson",
    "title": "5  Modèles linéaires généralisés",
    "section": "5.1 GLM Poisson",
    "text": "5.1 GLM Poisson"
  },
  {
    "objectID": "glm.html#glm-logistique",
    "href": "glm.html#glm-logistique",
    "title": "5  Modèles linéaires généralisés",
    "section": "5.2 GLM logistique",
    "text": "5.2 GLM logistique"
  }
]